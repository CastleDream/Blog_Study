{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**概述**：\n",
    "\n",
    "+ 使用IMDB评论数据集，其中包含来自互联网电影数据库的5w条影评文本，分为了2.5w训练集和2.5w测试集。\n",
    "+ 训练集和测试机是平衡的，即其中包含相同数量的正面影评和负面影评\n",
    "+ 二分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data,train_labels),(test_data,test_labels)=keras.datasets.imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keras.datasets.imdb.load_data(path='imdb.npz', num_words=None, skip_top=0, maxlen=None, seed=113, start_char=1, oov_char=2, index_from=3, **kwargs)\n",
    "\n",
    "可以看到这个函数是有默认路径的，可以去这个imdb的load_data去看看，因为这里还有num_words的限制，注意这个数据集是英语的，所以不需要分词，统计个数就好了.\n",
    "\n",
    "参数num_words=10000,保留训练数据中出现频次在前1w的词，为确保数据规模处于可管理的水平，罕见字词将会被舍弃"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000,), (25000,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape,train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(189, array([1, 0, 0], dtype=int64), 141)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data[1]),train_labels[:3],len(train_data[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_data已经被处理成了数字序号列表，但是列表长度并不一致，下面会对此进行处理。二分类是 0（负面）,1（正面）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将整数转回字词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index=keras.datasets.imdb.get_word_index()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "直接执行上面那句，会显示\n",
    "\n",
    "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
    "\n",
    "然后报错，时间超时，无法下载。。如法炮制昨天的做法，迅雷下载，然后把东西放到那个文件夹下。\n",
    "\n",
    "keras.datasets.imdb.get_word_index(path='imdb_word_index.json')\n",
    "\n",
    "可以看到都有一个默认的path值，就是相对~/.keras/datasets这个路径的路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index={k:(v+3) for k,v in word_index.items()} #空出三个是为了填充一些别的东西\n",
    "word_index['<PAD>']=0\n",
    "word_index['<START>']=1\n",
    "word_index['<UNK>']=2 #unknown\n",
    "word_index['<UNUSED>']=3\n",
    "#上面是 词 到 num\n",
    "reverse_word_index=dict([(value,key) for (key,value) in word_index.items()])\n",
    "# reverse_word_index={value:key for key,value in word_index.items()}  \n",
    "#这种写法和上面效果差不多，但是比上面简单，虽然不明白为什么没采用这个"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    print(reverse_word_index[1])\n",
    "    reverse21_word_index={value:key for key,value in word_index.items()}\n",
    "    print(reverse21_word_index[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_review(text):\n",
    "    return(' '.join([reverse_word_index.get(i,'?') for i in text]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_review(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "影评数据必须转换为张量，然后才能馈送到神经网络中，可以通过以下两种方式实现：（所以使用独热码或者数字序号表示都只是数据处理的一种方式，二者是相斥的）\n",
    "\n",
    "+ 1. 使用独热码编码。比如，序列[3,5]会变成一个1w维的向量，（上面num_words是1w），除了索引3和5转为1之外，其余全转换为0。然后，将它作为网络的第一层，一个可以处理浮点向量数据的密集层。不过这种方法会占据大量内存，需要一个大小为num_words\\*num_reviews的矩阵\n",
    "+ 2. 或者，我们可以填充数组，使它们具有相同的长度，然后创建一个形状为max_length\\*num_reviews的整数张量，可以使用一个能够处理这种形状的嵌入层作为网络中的第一层\n",
    "\n",
    "+ 这个教程中，使用第二种方式。要保证影评数据长度相同，所以使用pad_sequences函数将长度标准化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    sequences: List of lists, where each element is a sequence.\n",
    "    maxlen: Int, maximum length of all sequences.\n",
    "    dtype: Type of the output sequences.\n",
    "        To pad sequences with variable length strings, you can use `object`.\n",
    "    padding: String, 'pre' or 'post':\n",
    "        pad either before or after each sequence.\n",
    "    truncating: String, 'pre' or 'post':\n",
    "        remove values from sequences larger than\n",
    "        `maxlen`, either at the beginning or at the end of the sequences.\n",
    "    value: Float or String, padding value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen=256\n",
    "train_data=keras.preprocessing.sequence.pad_sequences(train_data,\n",
    "                                                      maxlen=maxlen,\n",
    "                                                      padding='post',  #在后面填充\n",
    "                                                      value=word_index['<PAD>']\n",
    "                                                     )\n",
    "test_data=keras.preprocessing.sequence.pad_sequences(test_data,\n",
    "                                                      maxlen=maxlen,\n",
    "                                                      padding='post',  \n",
    "                                                      value=word_index['<PAD>']\n",
    "                                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 256)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data[0]),len(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1   14   22   16   43  530  973 1622 1385   65  458 4468   66 3941\n",
      "    4  173   36  256    5   25  100   43  838  112   50  670    2    9\n",
      "   35  480  284    5  150    4  172  112  167    2  336  385   39    4\n",
      "  172 4536 1111   17  546   38   13  447    4  192   50   16    6  147\n",
      " 2025   19   14   22    4 1920 4613  469    4   22   71   87   12   16\n",
      "   43  530   38   76   15   13 1247    4   22   17  515   17   12   16\n",
      "  626   18    2    5   62  386   12    8  316    8  106    5    4 2223\n",
      " 5244   16  480   66 3785   33    4  130   12   16   38  619    5   25\n",
      "  124   51   36  135   48   25 1415   33    6   22   12  215   28   77\n",
      "   52    5   14  407   16   82    2    8    4  107  117 5952   15  256\n",
      "    4    2    7 3766    5  723   36   71   43  530  476   26  400  317\n",
      "   46    7    4    2 1029   13  104   88    4  381   15  297   98   32\n",
      " 2071   56   26  141    6  194 7486   18    4  226   22   21  134  476\n",
      "   26  480    5  144   30 5535   18   51   36   28  224   92   25  104\n",
      "    4  226   65   16   38 1334   88   12   16  283    5   16 4472  113\n",
      "  103   32   15   16 5345   19  178   32    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])  #可以看到确实是在后面填充0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络通过堆叠层创建而成，主要是要决定两个方面的架构：\n",
    "\n",
    "+ 要在模型中使用多少层\n",
    "+ 要针对每个层使用多少个隐藏单元\n",
    "\n",
    "本实例中，输入数据由字词-索引数组构成，要预测的标签是0或1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 第一层是Embedding层，它会在 整数编码的词汇表（字词-整数id） 中查找每个 字词-索引的嵌入向量，模型在接受训练时会学习这些向量，这些向量会像输出数组添加一个维度。生成的维度为：（batch,sequence,embedding）\n",
    "    + 每次输入模型的是一个batch大小的矩阵，其实衣服的10分类也是有batch_size的，只是直接用了默认值32\n",
    "    + 所以这里输入到下一层的维度是 （16，256，embedding）\n",
    "    +  input_dim: int > 0. Size of the vocabulary, i.e. maximum integer index + 1.\n",
    "    +  output_dim: int >= 0. Dimension of the dense embedding.\n",
    " \n",
    "+ 接下来，使用GlobalAveragePooling1D层对序列维度求平均值，针对每个样本返回一个长度固定的输出向量，这样，模型便能够以尽可能简单的方式处理各种长度的输入。这里没有看到什么默认的steps值。每个词经过嵌入层，都变成了一个16d表示的向量，一共1w个词，所以训练参数就是1w6k\n",
    "    \n",
    "    Global average pooling operation for temporal data.\n",
    "    Arguments:\n",
    "      data_format: A string,\n",
    "          one of `channels_last` (default) or `channels_first`.\n",
    "          The ordering of the dimensions in the inputs.\n",
    "          `channels_last` corresponds to inputs with shape\n",
    "          `(batch, steps, features)` while `channels_first`\n",
    "          corresponds to inputs with shape\n",
    "          `(batch, features, steps)`.\n",
    "\n",
    "    Input shape:\n",
    "        - If `data_format='channels_last'`:\n",
    "            3D tensor with shape:\n",
    "            `(batch_size, steps, features)`\n",
    "        - If `data_format='channels_first'`:\n",
    "            3D tensor with shape:\n",
    "            `(batch_size, features, steps)`\n",
    "\n",
    "    Output shape:\n",
    "        2D tensor with shape:\n",
    "        `(batch_size, features)`\n",
    "+ 最后一层与单个输出节点全连接，使用sigmoid激活函数后，结果是介于0-1之间的浮点数，表示概率或者置信水平"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 16)          160000    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 160,289\n",
      "Trainable params: 160,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size=10000\n",
    "\n",
    "model=keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size,16)) \n",
    "model.add(keras.layers.GlobalAveragePooling1D())\n",
    "model.add(keras.layers.Dense(16,activation=tf.nn.relu))\n",
    "model.add(keras.layers.Dense(1,activation=tf.nn.sigmoid))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**隐藏单元**：\n",
    "\n",
    "+ 输入输出之间的层就是隐藏层，如果模型有更多的隐藏单元（更高的表示维度空间）或者更多层，说明网络可以学习更复杂的表示法。不过这将耗费网络大量的计算资源，同时可能导致学习到不需要的模式（能优化训练集上的表现，但不会优化测试集的表现），这称为过拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**损失函数和优化器**：\n",
    "\n",
    "+ 模型在训练时需要一个损失函数和一个优化器，由于这是一个二分类问题，同时模型会输出一个概率（应用S型激活函数的单个单元层），因此将使用binary_crossentropy损失函数\n",
    "+ 该函数并不是唯一的损失函数，也可以使用mean_squared_error，但一般来说，binary_crossentropy更适合处理概率问题，它可以测量概率分布之间的‘差距’，在本例中则为实际分布和预测之间的差距\n",
    "+ 在探索回归问题时（例如房价预测），一般使用另一个称为均方误差的损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.train.AdamOptimizer(),\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建验证集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在训练时，需要检查 模型处理从未见过的数据 的准确率。所以需要创建验证集（从训练集中选一部分作为验证集，只打算使用训练数据开发和调整模型）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val=train_data[10000:]\n",
    "partial_x_train=train_data[:10000]\n",
    "\n",
    "y_val=train_labels[:10000]\n",
    "partial_y_train=train_labels[10000:]  \n",
    "\n",
    "#取前1w的记录作为"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
