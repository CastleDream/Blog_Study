{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2019-4-4无法在UCI上搜索到皮马人糖尿病数据集，最后是在[Kaggle](https://www.kaggle.com/uciml/pima-indians-diabetes-database/version/1)上找到的，已经放入dataset文件夹中"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第7章"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath='datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['diabetes.csv', 'iris.data', 'pima-indians-diabetes-database.zip']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file=os.listdir(filepath)\n",
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(filepath+'diabetes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 进行实验"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 划分特征和标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=7\n",
    "np.random.seed(seed) #固定随机数种子，以便之后使用梯度下降时可以复现结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=np.loadtxt(filepath+'diabetes.csv',delimiter=',',skiprows=1)  #跳过第一行的列名，不然报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=dataset[:,0:8]\n",
    "Y=dataset[:,8]  #划分特征和标签，便于训练  特征的维度为8"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from  keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()  #序列模型，最简单的keras模型，可以使用add来添加层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(12,input_dim=8,kernel_initializer='uniform',activation='relu'))\n",
    "# 此处注意，原文的代码 使用的参数是 'init' 但是随着keras参数更新，这里写为'kernel_initializer' 意思是权重矩阵初始化方式 采用0-1之间的均匀分布\n",
    "model.add(Dense(8,kernel_initializer='uniform',activation='relu'))\n",
    "# 第二层没有写输入维度，因为第一次规定的输入维度12就是第二层的输入维度，所以这层只显示说明输出维度即可\n",
    "model.add(Dense(1,kernel_initializer='uniform',activation='sigmoid'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 函数说明：\n",
    "+ Dense参数说明 https://keras-cn.readthedocs.io/en/latest/layers/core_layer/#dense\n",
    "+ kernel_initializer' 意思是权重矩阵初始化方式 采用0-0.05之间的均匀分布(0-0.05这个值的范围没有找到出处，暂存疑虑)\n",
    "+ Dense(12,input_dim=8)也就是这层接受的输入为8d，输出12d（也就是本层神经元的个数）\n",
    "+ 前两层的函数都是relu，（可以把值控制在0-1之间），最后一层使用S形函数，这样映射到0.5的阈值也很容易。\n",
    "+ 前两个隐藏层分别有12和8个神经元（units），最后一层是1个神经元（是否有糖尿病）\n",
    "+ PS注意，当时用jupyter编写sequential模型的时候，不要重复对model.add这个cell进行操作，不然会加很多重复层上去\n",
    "\n",
    "![img](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1554989732&di=51ca7a643a806fe07bf40962c003cc6c&imgtype=jpg&er=1&src=http%3A%2F%2Fpic4.zhimg.com%2Fv2-a0b0227110882928d7aa980b1e1e9363_b.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 12)                108       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 104       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 221\n",
      "Trainable params: 221\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Dense层所做的操作就是 dot(W(权重),X(输入向量))+b(偏置) ,\n",
    "+ 所以Dense1 8\\*12+12=9\\*12=108 个参数，同理，Dense2 12\\*8+8=13\\*8=104,Dense3 1\\*8+1=9"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 编译模型"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义好的模型可以编译:keras自动寻找相应的后端（我的电脑是TensorFlow），后端再去选择表示网络的最佳方法，配合你电脑的硬件。\n",
    "\n",
    "训练神经网络的意义是：找到可以解决问题的最好的一组权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "768/768 [==============================] - 0s 530us/step - loss: 0.6771 - acc: 0.6510\n",
      "Epoch 2/150\n",
      "768/768 [==============================] - 0s 125us/step - loss: 0.6584 - acc: 0.6510\n",
      "Epoch 3/150\n",
      "768/768 [==============================] - 0s 119us/step - loss: 0.6468 - acc: 0.6510\n",
      "Epoch 4/150\n",
      "768/768 [==============================] - 0s 125us/step - loss: 0.6391 - acc: 0.6510\n",
      "Epoch 5/150\n",
      "768/768 [==============================] - 0s 121us/step - loss: 0.6321 - acc: 0.6510\n",
      "Epoch 6/150\n",
      "768/768 [==============================] - 0s 122us/step - loss: 0.6181 - acc: 0.6510\n",
      "Epoch 7/150\n",
      "768/768 [==============================] - 0s 117us/step - loss: 0.6196 - acc: 0.6510\n",
      "Epoch 8/150\n",
      "768/768 [==============================] - 0s 126us/step - loss: 0.6150 - acc: 0.6510\n",
      "Epoch 9/150\n",
      "768/768 [==============================] - 0s 121us/step - loss: 0.6094 - acc: 0.6510\n",
      "Epoch 10/150\n",
      "768/768 [==============================] - 0s 125us/step - loss: 0.6158 - acc: 0.6510\n",
      "Epoch 11/150\n",
      "768/768 [==============================] - 0s 115us/step - loss: 0.6048 - acc: 0.6510\n",
      "Epoch 12/150\n",
      "768/768 [==============================] - 0s 115us/step - loss: 0.6040 - acc: 0.6510\n",
      "Epoch 13/150\n",
      "768/768 [==============================] - 0s 126us/step - loss: 0.6003 - acc: 0.6510\n",
      "Epoch 14/150\n",
      "768/768 [==============================] - 0s 122us/step - loss: 0.6036 - acc: 0.6510\n",
      "Epoch 15/150\n",
      "768/768 [==============================] - 0s 147us/step - loss: 0.5998 - acc: 0.6510\n",
      "Epoch 16/150\n",
      "768/768 [==============================] - 0s 130us/step - loss: 0.5996 - acc: 0.6510\n",
      "Epoch 17/150\n",
      "768/768 [==============================] - 0s 116us/step - loss: 0.5993 - acc: 0.6510\n",
      "Epoch 18/150\n",
      "768/768 [==============================] - 0s 116us/step - loss: 0.6007 - acc: 0.6510\n",
      "Epoch 19/150\n",
      "768/768 [==============================] - 0s 125us/step - loss: 0.5974 - acc: 0.6510\n",
      "Epoch 20/150\n",
      "768/768 [==============================] - 0s 120us/step - loss: 0.5982 - acc: 0.6510\n",
      "Epoch 21/150\n",
      "768/768 [==============================] - 0s 117us/step - loss: 0.5973 - acc: 0.6510\n",
      "Epoch 22/150\n",
      "768/768 [==============================] - 0s 121us/step - loss: 0.5940 - acc: 0.6510\n",
      "Epoch 23/150\n",
      "768/768 [==============================] - 0s 120us/step - loss: 0.5932 - acc: 0.6510\n",
      "Epoch 24/150\n",
      "768/768 [==============================] - 0s 119us/step - loss: 0.5992 - acc: 0.6510\n",
      "Epoch 25/150\n",
      "768/768 [==============================] - 0s 115us/step - loss: 0.5939 - acc: 0.6510\n",
      "Epoch 26/150\n",
      "768/768 [==============================] - 0s 122us/step - loss: 0.5968 - acc: 0.6510\n",
      "Epoch 27/150\n",
      "768/768 [==============================] - 0s 121us/step - loss: 0.5951 - acc: 0.6510\n",
      "Epoch 28/150\n",
      "768/768 [==============================] - 0s 124us/step - loss: 0.5895 - acc: 0.6510\n",
      "Epoch 29/150\n",
      "768/768 [==============================] - 0s 121us/step - loss: 0.5932 - acc: 0.6510\n",
      "Epoch 30/150\n",
      "768/768 [==============================] - 0s 119us/step - loss: 0.5915 - acc: 0.6510\n",
      "Epoch 31/150\n",
      "768/768 [==============================] - 0s 117us/step - loss: 0.5899 - acc: 0.6510\n",
      "Epoch 32/150\n",
      "768/768 [==============================] - 0s 126us/step - loss: 0.5900 - acc: 0.6510\n",
      "Epoch 33/150\n",
      "768/768 [==============================] - 0s 121us/step - loss: 0.5865 - acc: 0.6510\n",
      "Epoch 34/150\n",
      "768/768 [==============================] - 0s 122us/step - loss: 0.5906 - acc: 0.6510\n",
      "Epoch 35/150\n",
      "768/768 [==============================] - 0s 119us/step - loss: 0.5893 - acc: 0.6510\n",
      "Epoch 36/150\n",
      "768/768 [==============================] - 0s 126us/step - loss: 0.5828 - acc: 0.6510\n",
      "Epoch 37/150\n",
      "768/768 [==============================] - 0s 132us/step - loss: 0.5819 - acc: 0.6510\n",
      "Epoch 38/150\n",
      "768/768 [==============================] - 0s 130us/step - loss: 0.5929 - acc: 0.6510\n",
      "Epoch 39/150\n",
      "768/768 [==============================] - 0s 133us/step - loss: 0.5833 - acc: 0.6810\n",
      "Epoch 40/150\n",
      "768/768 [==============================] - 0s 125us/step - loss: 0.5875 - acc: 0.7031\n",
      "Epoch 41/150\n",
      "768/768 [==============================] - 0s 122us/step - loss: 0.5828 - acc: 0.7044\n",
      "Epoch 42/150\n",
      "768/768 [==============================] - 0s 128us/step - loss: 0.5805 - acc: 0.7031\n",
      "Epoch 43/150\n",
      "768/768 [==============================] - 0s 128us/step - loss: 0.5784 - acc: 0.7122\n",
      "Epoch 44/150\n",
      "768/768 [==============================] - 0s 119us/step - loss: 0.5866 - acc: 0.7096\n",
      "Epoch 45/150\n",
      "768/768 [==============================] - 0s 121us/step - loss: 0.5793 - acc: 0.7135\n",
      "Epoch 46/150\n",
      "768/768 [==============================] - 0s 122us/step - loss: 0.5774 - acc: 0.6953\n",
      "Epoch 47/150\n",
      "768/768 [==============================] - 0s 122us/step - loss: 0.5780 - acc: 0.7083\n",
      "Epoch 48/150\n",
      "768/768 [==============================] - 0s 119us/step - loss: 0.5759 - acc: 0.6940\n",
      "Epoch 49/150\n",
      "768/768 [==============================] - 0s 125us/step - loss: 0.5740 - acc: 0.7122\n",
      "Epoch 50/150\n",
      "768/768 [==============================] - 0s 128us/step - loss: 0.5749 - acc: 0.7135\n",
      "Epoch 51/150\n",
      "768/768 [==============================] - 0s 121us/step - loss: 0.5730 - acc: 0.7148\n",
      "Epoch 52/150\n",
      "768/768 [==============================] - 0s 122us/step - loss: 0.5720 - acc: 0.7122\n",
      "Epoch 53/150\n",
      "768/768 [==============================] - 0s 125us/step - loss: 0.5740 - acc: 0.7109\n",
      "Epoch 54/150\n",
      "768/768 [==============================] - 0s 124us/step - loss: 0.5741 - acc: 0.7135\n",
      "Epoch 55/150\n",
      "768/768 [==============================] - 0s 122us/step - loss: 0.5730 - acc: 0.7096\n",
      "Epoch 56/150\n",
      "768/768 [==============================] - 0s 120us/step - loss: 0.5730 - acc: 0.7057\n",
      "Epoch 57/150\n",
      "768/768 [==============================] - 0s 119us/step - loss: 0.5702 - acc: 0.7031\n",
      "Epoch 58/150\n",
      "768/768 [==============================] - 0s 125us/step - loss: 0.5756 - acc: 0.7044\n",
      "Epoch 59/150\n",
      "768/768 [==============================] - 0s 113us/step - loss: 0.5692 - acc: 0.7161\n",
      "Epoch 60/150\n",
      "768/768 [==============================] - 0s 116us/step - loss: 0.5709 - acc: 0.7070\n",
      "Epoch 61/150\n",
      "768/768 [==============================] - 0s 115us/step - loss: 0.5668 - acc: 0.7083\n",
      "Epoch 62/150\n",
      "768/768 [==============================] - 0s 115us/step - loss: 0.5685 - acc: 0.7174\n",
      "Epoch 63/150\n",
      "768/768 [==============================] - 0s 112us/step - loss: 0.5686 - acc: 0.7083\n",
      "Epoch 64/150\n",
      "768/768 [==============================] - 0s 115us/step - loss: 0.5686 - acc: 0.7135\n",
      "Epoch 65/150\n",
      "768/768 [==============================] - 0s 113us/step - loss: 0.5645 - acc: 0.7031\n",
      "Epoch 66/150\n",
      "768/768 [==============================] - 0s 113us/step - loss: 0.5621 - acc: 0.7109\n",
      "Epoch 67/150\n",
      "768/768 [==============================] - 0s 115us/step - loss: 0.5602 - acc: 0.7122\n",
      "Epoch 68/150\n",
      "768/768 [==============================] - 0s 117us/step - loss: 0.5625 - acc: 0.7031\n",
      "Epoch 69/150\n",
      "768/768 [==============================] - 0s 115us/step - loss: 0.5610 - acc: 0.7253\n",
      "Epoch 70/150\n",
      "768/768 [==============================] - 0s 129us/step - loss: 0.5621 - acc: 0.7057\n",
      "Epoch 71/150\n",
      "768/768 [==============================] - 0s 116us/step - loss: 0.5573 - acc: 0.7161\n",
      "Epoch 72/150\n",
      "768/768 [==============================] - 0s 136us/step - loss: 0.5575 - acc: 0.7044\n",
      "Epoch 73/150\n",
      "768/768 [==============================] - 0s 111us/step - loss: 0.5535 - acc: 0.7214\n",
      "Epoch 74/150\n",
      "768/768 [==============================] - 0s 112us/step - loss: 0.5565 - acc: 0.7057\n",
      "Epoch 75/150\n",
      "768/768 [==============================] - 0s 124us/step - loss: 0.5537 - acc: 0.7201\n",
      "Epoch 76/150\n",
      "768/768 [==============================] - 0s 163us/step - loss: 0.5529 - acc: 0.7214\n",
      "Epoch 77/150\n",
      "768/768 [==============================] - 0s 124us/step - loss: 0.5581 - acc: 0.7161\n",
      "Epoch 78/150\n",
      "768/768 [==============================] - 0s 160us/step - loss: 0.5470 - acc: 0.7187\n",
      "Epoch 79/150\n",
      "768/768 [==============================] - 0s 162us/step - loss: 0.5548 - acc: 0.7109\n",
      "Epoch 80/150\n",
      "768/768 [==============================] - 0s 173us/step - loss: 0.5481 - acc: 0.7187 0s - loss: 0.5493 - acc: 0.721\n",
      "Epoch 81/150\n",
      "768/768 [==============================] - 0s 168us/step - loss: 0.5459 - acc: 0.7318\n",
      "Epoch 82/150\n",
      "768/768 [==============================] - 0s 176us/step - loss: 0.5506 - acc: 0.7109\n",
      "Epoch 83/150\n",
      "768/768 [==============================] - 0s 176us/step - loss: 0.5489 - acc: 0.7253\n",
      "Epoch 84/150\n",
      "768/768 [==============================] - 0s 164us/step - loss: 0.5411 - acc: 0.7266\n",
      "Epoch 85/150\n",
      "768/768 [==============================] - 0s 172us/step - loss: 0.5458 - acc: 0.7096\n",
      "Epoch 86/150\n",
      "768/768 [==============================] - 0s 160us/step - loss: 0.5494 - acc: 0.7148\n",
      "Epoch 87/150\n",
      "768/768 [==============================] - 0s 165us/step - loss: 0.5429 - acc: 0.7135\n",
      "Epoch 88/150\n",
      "768/768 [==============================] - 0s 132us/step - loss: 0.5373 - acc: 0.7279\n",
      "Epoch 89/150\n",
      "768/768 [==============================] - 0s 124us/step - loss: 0.5566 - acc: 0.7266\n",
      "Epoch 90/150\n",
      "768/768 [==============================] - 0s 126us/step - loss: 0.5408 - acc: 0.7266\n",
      "Epoch 91/150\n",
      "768/768 [==============================] - 0s 124us/step - loss: 0.5372 - acc: 0.7292\n",
      "Epoch 92/150\n",
      "768/768 [==============================] - 0s 116us/step - loss: 0.5390 - acc: 0.7214\n",
      "Epoch 93/150\n",
      "768/768 [==============================] - 0s 160us/step - loss: 0.5356 - acc: 0.7148\n",
      "Epoch 94/150\n",
      "768/768 [==============================] - 0s 124us/step - loss: 0.5385 - acc: 0.7422\n",
      "Epoch 95/150\n",
      "768/768 [==============================] - 0s 112us/step - loss: 0.5324 - acc: 0.7253\n",
      "Epoch 96/150\n",
      "768/768 [==============================] - 0s 121us/step - loss: 0.5378 - acc: 0.7344\n",
      "Epoch 97/150\n",
      "768/768 [==============================] - 0s 125us/step - loss: 0.5332 - acc: 0.7305\n",
      "Epoch 98/150\n",
      "768/768 [==============================] - 0s 126us/step - loss: 0.5279 - acc: 0.7383\n",
      "Epoch 99/150\n",
      "768/768 [==============================] - 0s 124us/step - loss: 0.5294 - acc: 0.7383\n",
      "Epoch 100/150\n",
      "768/768 [==============================] - 0s 115us/step - loss: 0.5291 - acc: 0.7305\n",
      "Epoch 101/150\n",
      "768/768 [==============================] - 0s 120us/step - loss: 0.5334 - acc: 0.7161\n",
      "Epoch 102/150\n",
      "768/768 [==============================] - 0s 119us/step - loss: 0.5298 - acc: 0.7396\n",
      "Epoch 103/150\n",
      "768/768 [==============================] - 0s 124us/step - loss: 0.5326 - acc: 0.7253\n",
      "Epoch 104/150\n",
      "768/768 [==============================] - 0s 124us/step - loss: 0.5343 - acc: 0.7344 0s - loss: 0.5501 - acc: 0.724\n",
      "Epoch 105/150\n",
      "768/768 [==============================] - 0s 120us/step - loss: 0.5266 - acc: 0.7357\n",
      "Epoch 106/150\n",
      "768/768 [==============================] - 0s 151us/step - loss: 0.5280 - acc: 0.7383\n",
      "Epoch 107/150\n",
      "768/768 [==============================] - 0s 151us/step - loss: 0.5311 - acc: 0.7461\n",
      "Epoch 108/150\n",
      "768/768 [==============================] - 0s 158us/step - loss: 0.5264 - acc: 0.7305\n",
      "Epoch 109/150\n",
      "768/768 [==============================] - 0s 163us/step - loss: 0.5235 - acc: 0.7370\n",
      "Epoch 110/150\n",
      "768/768 [==============================] - 0s 126us/step - loss: 0.5213 - acc: 0.7357\n",
      "Epoch 111/150\n",
      "768/768 [==============================] - 0s 117us/step - loss: 0.5224 - acc: 0.7435\n",
      "Epoch 112/150\n",
      "768/768 [==============================] - 0s 121us/step - loss: 0.5188 - acc: 0.7370\n",
      "Epoch 113/150\n",
      "768/768 [==============================] - 0s 120us/step - loss: 0.5154 - acc: 0.7383\n",
      "Epoch 114/150\n",
      "768/768 [==============================] - 0s 116us/step - loss: 0.5233 - acc: 0.7461\n",
      "Epoch 115/150\n",
      "768/768 [==============================] - 0s 119us/step - loss: 0.5148 - acc: 0.7500\n",
      "Epoch 116/150\n",
      "768/768 [==============================] - 0s 117us/step - loss: 0.5177 - acc: 0.7474\n",
      "Epoch 117/150\n",
      "768/768 [==============================] - 0s 129us/step - loss: 0.5171 - acc: 0.7292\n",
      "Epoch 118/150\n",
      "768/768 [==============================] - 0s 159us/step - loss: 0.5216 - acc: 0.7487\n",
      "Epoch 119/150\n",
      "768/768 [==============================] - 0s 143us/step - loss: 0.5106 - acc: 0.7409\n",
      "Epoch 120/150\n",
      "768/768 [==============================] - 0s 155us/step - loss: 0.5112 - acc: 0.7591\n",
      "Epoch 121/150\n",
      "768/768 [==============================] - 0s 132us/step - loss: 0.5128 - acc: 0.7539\n",
      "Epoch 122/150\n",
      "768/768 [==============================] - 0s 128us/step - loss: 0.5179 - acc: 0.7435\n",
      "Epoch 123/150\n",
      "768/768 [==============================] - 0s 119us/step - loss: 0.5093 - acc: 0.7487\n",
      "Epoch 124/150\n",
      "768/768 [==============================] - 0s 121us/step - loss: 0.5045 - acc: 0.7643\n",
      "Epoch 125/150\n",
      "768/768 [==============================] - 0s 119us/step - loss: 0.5050 - acc: 0.7604\n",
      "Epoch 126/150\n",
      "768/768 [==============================] - 0s 121us/step - loss: 0.5016 - acc: 0.7526\n",
      "Epoch 127/150\n",
      "768/768 [==============================] - 0s 116us/step - loss: 0.5063 - acc: 0.7682\n",
      "Epoch 128/150\n",
      "768/768 [==============================] - 0s 125us/step - loss: 0.4965 - acc: 0.7630\n",
      "Epoch 129/150\n",
      "768/768 [==============================] - 0s 126us/step - loss: 0.5140 - acc: 0.7578\n",
      "Epoch 130/150\n",
      "768/768 [==============================] - 0s 119us/step - loss: 0.4976 - acc: 0.7695\n",
      "Epoch 131/150\n",
      "768/768 [==============================] - 0s 126us/step - loss: 0.5020 - acc: 0.7617\n",
      "Epoch 132/150\n",
      "768/768 [==============================] - 0s 119us/step - loss: 0.4983 - acc: 0.7643\n",
      "Epoch 133/150\n",
      "768/768 [==============================] - 0s 124us/step - loss: 0.5030 - acc: 0.7617\n",
      "Epoch 134/150\n",
      "768/768 [==============================] - 0s 121us/step - loss: 0.4939 - acc: 0.7682\n",
      "Epoch 135/150\n",
      "768/768 [==============================] - 0s 171us/step - loss: 0.4984 - acc: 0.7604\n",
      "Epoch 136/150\n",
      "768/768 [==============================] - 0s 152us/step - loss: 0.4943 - acc: 0.7539\n",
      "Epoch 137/150\n",
      "768/768 [==============================] - 0s 132us/step - loss: 0.4928 - acc: 0.7565\n",
      "Epoch 138/150\n",
      "768/768 [==============================] - 0s 155us/step - loss: 0.4971 - acc: 0.7721\n",
      "Epoch 139/150\n",
      "768/768 [==============================] - 0s 120us/step - loss: 0.4892 - acc: 0.7812\n",
      "Epoch 140/150\n",
      "768/768 [==============================] - 0s 167us/step - loss: 0.4899 - acc: 0.7617\n",
      "Epoch 141/150\n",
      "768/768 [==============================] - 0s 159us/step - loss: 0.4845 - acc: 0.7708\n",
      "Epoch 142/150\n",
      "768/768 [==============================] - 0s 154us/step - loss: 0.4909 - acc: 0.7552 0s - loss: 0.4922 - acc: 0.752\n",
      "Epoch 143/150\n",
      "768/768 [==============================] - 0s 154us/step - loss: 0.4832 - acc: 0.7708\n",
      "Epoch 144/150\n",
      "768/768 [==============================] - 0s 126us/step - loss: 0.4893 - acc: 0.7643\n",
      "Epoch 145/150\n",
      "768/768 [==============================] - 0s 124us/step - loss: 0.4911 - acc: 0.7591\n",
      "Epoch 146/150\n",
      "768/768 [==============================] - 0s 129us/step - loss: 0.4887 - acc: 0.7734\n",
      "Epoch 147/150\n",
      "768/768 [==============================] - 0s 141us/step - loss: 0.4895 - acc: 0.7708\n",
      "Epoch 148/150\n",
      "768/768 [==============================] - 0s 130us/step - loss: 0.4865 - acc: 0.7747\n",
      "Epoch 149/150\n",
      "768/768 [==============================] - 0s 163us/step - loss: 0.4798 - acc: 0.7695\n",
      "Epoch 150/150\n",
      "768/768 [==============================] - 0s 130us/step - loss: 0.4765 - acc: 0.7695\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(X,Y,epochs=150,batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768/768 [==============================] - 0s 106us/step\n"
     ]
    }
   ],
   "source": [
    "scores=model.evaluate(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4826677590608597, 0.7721354166666666)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[0],scores[1] #这两个值分别是loss和acc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第8章-测试神经网络"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  自动验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 514 samples, validate on 254 samples\n",
      "Epoch 1/150\n",
      "514/514 [==============================] - 0s 183us/step - loss: 0.5035 - acc: 0.7510 - val_loss: 0.4445 - val_acc: 0.8110\n",
      "Epoch 2/150\n",
      "514/514 [==============================] - 0s 142us/step - loss: 0.5144 - acc: 0.7451 - val_loss: 0.4336 - val_acc: 0.7953\n",
      "Epoch 3/150\n",
      "514/514 [==============================] - 0s 164us/step - loss: 0.4941 - acc: 0.7704 - val_loss: 0.4268 - val_acc: 0.8110\n",
      "Epoch 4/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.4861 - acc: 0.7840 - val_loss: 0.4357 - val_acc: 0.7913\n",
      "Epoch 5/150\n",
      "514/514 [==============================] - 0s 154us/step - loss: 0.4964 - acc: 0.7704 - val_loss: 0.4514 - val_acc: 0.7953\n",
      "Epoch 6/150\n",
      "514/514 [==============================] - 0s 167us/step - loss: 0.5014 - acc: 0.7646 - val_loss: 0.4506 - val_acc: 0.7835\n",
      "Epoch 7/150\n",
      "514/514 [==============================] - 0s 142us/step - loss: 0.4913 - acc: 0.7782 - val_loss: 0.4522 - val_acc: 0.7874\n",
      "Epoch 8/150\n",
      "514/514 [==============================] - 0s 154us/step - loss: 0.5036 - acc: 0.7549 - val_loss: 0.4305 - val_acc: 0.8110\n",
      "Epoch 9/150\n",
      "514/514 [==============================] - 0s 150us/step - loss: 0.4878 - acc: 0.7782 - val_loss: 0.4376 - val_acc: 0.8071\n",
      "Epoch 10/150\n",
      "514/514 [==============================] - 0s 154us/step - loss: 0.4875 - acc: 0.7860 - val_loss: 0.5007 - val_acc: 0.7598\n",
      "Epoch 11/150\n",
      "514/514 [==============================] - 0s 164us/step - loss: 0.4990 - acc: 0.7607 - val_loss: 0.4488 - val_acc: 0.8110\n",
      "Epoch 12/150\n",
      "514/514 [==============================] - ETA: 0s - loss: 0.4882 - acc: 0.773 - 0s 146us/step - loss: 0.4863 - acc: 0.7724 - val_loss: 0.4672 - val_acc: 0.8031\n",
      "Epoch 13/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.4872 - acc: 0.7646 - val_loss: 0.4460 - val_acc: 0.8228\n",
      "Epoch 14/150\n",
      "514/514 [==============================] - 0s 156us/step - loss: 0.4828 - acc: 0.7724 - val_loss: 0.4443 - val_acc: 0.7913\n",
      "Epoch 15/150\n",
      "514/514 [==============================] - 0s 148us/step - loss: 0.4892 - acc: 0.7704 - val_loss: 0.4383 - val_acc: 0.8110\n",
      "Epoch 16/150\n",
      "514/514 [==============================] - 0s 150us/step - loss: 0.4830 - acc: 0.7724 - val_loss: 0.4358 - val_acc: 0.8071\n",
      "Epoch 17/150\n",
      "514/514 [==============================] - 0s 146us/step - loss: 0.4772 - acc: 0.7763 - val_loss: 0.4450 - val_acc: 0.7992\n",
      "Epoch 18/150\n",
      "514/514 [==============================] - 0s 150us/step - loss: 0.4903 - acc: 0.7704 - val_loss: 0.4390 - val_acc: 0.7913\n",
      "Epoch 19/150\n",
      "514/514 [==============================] - 0s 150us/step - loss: 0.4912 - acc: 0.7646 - val_loss: 0.4527 - val_acc: 0.7953\n",
      "Epoch 20/150\n",
      "514/514 [==============================] - 0s 150us/step - loss: 0.4792 - acc: 0.7879 - val_loss: 0.4514 - val_acc: 0.7835\n",
      "Epoch 21/150\n",
      "514/514 [==============================] - 0s 158us/step - loss: 0.4861 - acc: 0.7743 - val_loss: 0.4496 - val_acc: 0.7756\n",
      "Epoch 22/150\n",
      "514/514 [==============================] - 0s 156us/step - loss: 0.4879 - acc: 0.7802 - val_loss: 0.4374 - val_acc: 0.8071\n",
      "Epoch 23/150\n",
      "514/514 [==============================] - 0s 185us/step - loss: 0.4830 - acc: 0.7743 - val_loss: 0.4469 - val_acc: 0.7992\n",
      "Epoch 24/150\n",
      "514/514 [==============================] - 0s 179us/step - loss: 0.4820 - acc: 0.7685 - val_loss: 0.4467 - val_acc: 0.7795\n",
      "Epoch 25/150\n",
      "514/514 [==============================] - 0s 173us/step - loss: 0.4815 - acc: 0.7646 - val_loss: 0.4442 - val_acc: 0.8189\n",
      "Epoch 26/150\n",
      "514/514 [==============================] - 0s 177us/step - loss: 0.4809 - acc: 0.7763 - val_loss: 0.4604 - val_acc: 0.7953\n",
      "Epoch 27/150\n",
      "514/514 [==============================] - 0s 183us/step - loss: 0.4759 - acc: 0.7626 - val_loss: 0.4660 - val_acc: 0.7953\n",
      "Epoch 28/150\n",
      "514/514 [==============================] - 0s 175us/step - loss: 0.5139 - acc: 0.7490 - val_loss: 0.4495 - val_acc: 0.7874\n",
      "Epoch 29/150\n",
      "514/514 [==============================] - ETA: 0s - loss: 0.4866 - acc: 0.759 - 0s 160us/step - loss: 0.4798 - acc: 0.7704 - val_loss: 0.4747 - val_acc: 0.7835\n",
      "Epoch 30/150\n",
      "514/514 [==============================] - 0s 154us/step - loss: 0.4807 - acc: 0.7704 - val_loss: 0.4611 - val_acc: 0.7835\n",
      "Epoch 31/150\n",
      "514/514 [==============================] - 0s 154us/step - loss: 0.4882 - acc: 0.7743 - val_loss: 0.4707 - val_acc: 0.7874\n",
      "Epoch 32/150\n",
      "514/514 [==============================] - 0s 253us/step - loss: 0.4948 - acc: 0.7607 - val_loss: 0.4448 - val_acc: 0.7992\n",
      "Epoch 33/150\n",
      "514/514 [==============================] - 0s 150us/step - loss: 0.4805 - acc: 0.7646 - val_loss: 0.4499 - val_acc: 0.7795\n",
      "Epoch 34/150\n",
      "514/514 [==============================] - 0s 187us/step - loss: 0.4772 - acc: 0.7724 - val_loss: 0.4374 - val_acc: 0.8110\n",
      "Epoch 35/150\n",
      "514/514 [==============================] - 0s 167us/step - loss: 0.4733 - acc: 0.7899 - val_loss: 0.4584 - val_acc: 0.7992\n",
      "Epoch 36/150\n",
      "514/514 [==============================] - 0s 171us/step - loss: 0.4916 - acc: 0.7782 - val_loss: 0.4486 - val_acc: 0.7913\n",
      "Epoch 37/150\n",
      "514/514 [==============================] - 0s 171us/step - loss: 0.4771 - acc: 0.7860 - val_loss: 0.4429 - val_acc: 0.8071\n",
      "Epoch 38/150\n",
      "514/514 [==============================] - 0s 173us/step - loss: 0.4711 - acc: 0.7840 - val_loss: 0.4533 - val_acc: 0.8071\n",
      "Epoch 39/150\n",
      "514/514 [==============================] - 0s 173us/step - loss: 0.4783 - acc: 0.7821 - val_loss: 0.4663 - val_acc: 0.7913\n",
      "Epoch 40/150\n",
      "514/514 [==============================] - 0s 156us/step - loss: 0.4642 - acc: 0.7821 - val_loss: 0.5825 - val_acc: 0.6850\n",
      "Epoch 41/150\n",
      "514/514 [==============================] - 0s 148us/step - loss: 0.5205 - acc: 0.7412 - val_loss: 0.4600 - val_acc: 0.7953\n",
      "Epoch 42/150\n",
      "514/514 [==============================] - 0s 150us/step - loss: 0.4787 - acc: 0.7646 - val_loss: 0.4530 - val_acc: 0.8031\n",
      "Epoch 43/150\n",
      "514/514 [==============================] - 0s 175us/step - loss: 0.4681 - acc: 0.7879 - val_loss: 0.4560 - val_acc: 0.7913\n",
      "Epoch 44/150\n",
      "514/514 [==============================] - 0s 158us/step - loss: 0.4745 - acc: 0.7724 - val_loss: 0.4549 - val_acc: 0.7953\n",
      "Epoch 45/150\n",
      "514/514 [==============================] - 0s 162us/step - loss: 0.4686 - acc: 0.7782 - val_loss: 0.4666 - val_acc: 0.7874\n",
      "Epoch 46/150\n",
      "514/514 [==============================] - 0s 156us/step - loss: 0.4685 - acc: 0.7802 - val_loss: 0.4500 - val_acc: 0.7992\n",
      "Epoch 47/150\n",
      "514/514 [==============================] - 0s 152us/step - loss: 0.4724 - acc: 0.7802 - val_loss: 0.4583 - val_acc: 0.7992\n",
      "Epoch 48/150\n",
      "514/514 [==============================] - 0s 158us/step - loss: 0.4659 - acc: 0.7821 - val_loss: 0.4575 - val_acc: 0.7874\n",
      "Epoch 49/150\n",
      "514/514 [==============================] - 0s 154us/step - loss: 0.4742 - acc: 0.7899 - val_loss: 0.4550 - val_acc: 0.8031\n",
      "Epoch 50/150\n",
      "514/514 [==============================] - 0s 164us/step - loss: 0.4666 - acc: 0.7665 - val_loss: 0.4646 - val_acc: 0.7835\n",
      "Epoch 51/150\n",
      "514/514 [==============================] - 0s 150us/step - loss: 0.4696 - acc: 0.7821 - val_loss: 0.4657 - val_acc: 0.8031\n",
      "Epoch 52/150\n",
      "514/514 [==============================] - ETA: 0s - loss: 0.4725 - acc: 0.769 - 0s 165us/step - loss: 0.4676 - acc: 0.7763 - val_loss: 0.4604 - val_acc: 0.7992\n",
      "Epoch 53/150\n",
      "514/514 [==============================] - 0s 150us/step - loss: 0.4677 - acc: 0.7879 - val_loss: 0.4556 - val_acc: 0.7913\n",
      "Epoch 54/150\n",
      "514/514 [==============================] - 0s 162us/step - loss: 0.4651 - acc: 0.7860 - val_loss: 0.4858 - val_acc: 0.7638\n",
      "Epoch 55/150\n",
      "514/514 [==============================] - 0s 197us/step - loss: 0.4759 - acc: 0.7588 - val_loss: 0.4869 - val_acc: 0.7756\n",
      "Epoch 56/150\n",
      "514/514 [==============================] - 0s 150us/step - loss: 0.4569 - acc: 0.8016 - val_loss: 0.4690 - val_acc: 0.7953\n",
      "Epoch 57/150\n",
      "514/514 [==============================] - 0s 156us/step - loss: 0.4639 - acc: 0.7899 - val_loss: 0.4882 - val_acc: 0.7520\n",
      "Epoch 58/150\n",
      "514/514 [==============================] - 0s 177us/step - loss: 0.4767 - acc: 0.7918 - val_loss: 0.4620 - val_acc: 0.8031\n",
      "Epoch 59/150\n",
      "514/514 [==============================] - 0s 156us/step - loss: 0.4729 - acc: 0.7763 - val_loss: 0.4628 - val_acc: 0.7953\n",
      "Epoch 60/150\n",
      "514/514 [==============================] - 0s 162us/step - loss: 0.4760 - acc: 0.7607 - val_loss: 0.4692 - val_acc: 0.7795\n",
      "Epoch 61/150\n",
      "514/514 [==============================] - 0s 175us/step - loss: 0.4664 - acc: 0.7782 - val_loss: 0.4493 - val_acc: 0.8110\n",
      "Epoch 62/150\n",
      "514/514 [==============================] - 0s 173us/step - loss: 0.4647 - acc: 0.7821 - val_loss: 0.4735 - val_acc: 0.7638\n",
      "Epoch 63/150\n",
      "514/514 [==============================] - 0s 171us/step - loss: 0.4805 - acc: 0.7685 - val_loss: 0.4705 - val_acc: 0.7835\n",
      "Epoch 64/150\n",
      "514/514 [==============================] - 0s 175us/step - loss: 0.4681 - acc: 0.7782 - val_loss: 0.4588 - val_acc: 0.7953\n",
      "Epoch 65/150\n",
      "514/514 [==============================] - 0s 169us/step - loss: 0.4723 - acc: 0.7879 - val_loss: 0.5244 - val_acc: 0.7323\n",
      "Epoch 66/150\n",
      "514/514 [==============================] - 0s 169us/step - loss: 0.5079 - acc: 0.7510 - val_loss: 0.4766 - val_acc: 0.7795\n",
      "Epoch 67/150\n",
      "514/514 [==============================] - 0s 156us/step - loss: 0.4779 - acc: 0.7646 - val_loss: 0.5020 - val_acc: 0.7362\n",
      "Epoch 68/150\n",
      "514/514 [==============================] - 0s 148us/step - loss: 0.4816 - acc: 0.7588 - val_loss: 0.4735 - val_acc: 0.7795\n",
      "Epoch 69/150\n",
      "514/514 [==============================] - 0s 164us/step - loss: 0.4588 - acc: 0.7899 - val_loss: 0.4546 - val_acc: 0.8071\n",
      "Epoch 70/150\n",
      "514/514 [==============================] - 0s 156us/step - loss: 0.4642 - acc: 0.7763 - val_loss: 0.4581 - val_acc: 0.8031\n",
      "Epoch 71/150\n",
      "514/514 [==============================] - 0s 146us/step - loss: 0.4549 - acc: 0.7977 - val_loss: 0.4584 - val_acc: 0.7953\n",
      "Epoch 72/150\n",
      "514/514 [==============================] - 0s 154us/step - loss: 0.4645 - acc: 0.7724 - val_loss: 0.4738 - val_acc: 0.7559\n",
      "Epoch 73/150\n",
      "514/514 [==============================] - 0s 156us/step - loss: 0.4640 - acc: 0.7860 - val_loss: 0.4643 - val_acc: 0.7795\n",
      "Epoch 74/150\n",
      "514/514 [==============================] - 0s 158us/step - loss: 0.4666 - acc: 0.7899 - val_loss: 0.4769 - val_acc: 0.7953\n",
      "Epoch 75/150\n",
      "514/514 [==============================] - 0s 152us/step - loss: 0.4561 - acc: 0.7899 - val_loss: 0.4529 - val_acc: 0.8031\n",
      "Epoch 76/150\n",
      "514/514 [==============================] - 0s 150us/step - loss: 0.4523 - acc: 0.7918 - val_loss: 0.4536 - val_acc: 0.7795\n",
      "Epoch 77/150\n",
      "514/514 [==============================] - 0s 156us/step - loss: 0.4628 - acc: 0.7860 - val_loss: 0.4669 - val_acc: 0.7913\n",
      "Epoch 78/150\n",
      "514/514 [==============================] - 0s 164us/step - loss: 0.4591 - acc: 0.7879 - val_loss: 0.4674 - val_acc: 0.8071\n",
      "Epoch 79/150\n",
      "514/514 [==============================] - 0s 156us/step - loss: 0.4571 - acc: 0.7860 - val_loss: 0.4751 - val_acc: 0.7835\n",
      "Epoch 80/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.4543 - acc: 0.7957 - val_loss: 0.4837 - val_acc: 0.7795\n",
      "Epoch 81/150\n",
      "514/514 [==============================] - 0s 158us/step - loss: 0.4551 - acc: 0.7938 - val_loss: 0.4618 - val_acc: 0.7992\n",
      "Epoch 82/150\n",
      "514/514 [==============================] - 0s 158us/step - loss: 0.4612 - acc: 0.7802 - val_loss: 0.4568 - val_acc: 0.8031\n",
      "Epoch 83/150\n",
      "514/514 [==============================] - 0s 156us/step - loss: 0.4538 - acc: 0.7840 - val_loss: 0.4550 - val_acc: 0.7992\n",
      "Epoch 84/150\n",
      "514/514 [==============================] - 0s 148us/step - loss: 0.4540 - acc: 0.7996 - val_loss: 0.4581 - val_acc: 0.7953\n",
      "Epoch 85/150\n",
      "514/514 [==============================] - 0s 154us/step - loss: 0.4537 - acc: 0.7802 - val_loss: 0.4916 - val_acc: 0.7795\n",
      "Epoch 86/150\n",
      "514/514 [==============================] - 0s 158us/step - loss: 0.4576 - acc: 0.7782 - val_loss: 0.4634 - val_acc: 0.7913\n",
      "Epoch 87/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.4635 - acc: 0.7743 - val_loss: 0.4695 - val_acc: 0.7953\n",
      "Epoch 88/150\n",
      "514/514 [==============================] - 0s 150us/step - loss: 0.4518 - acc: 0.7782 - val_loss: 0.4717 - val_acc: 0.7992\n",
      "Epoch 89/150\n",
      "514/514 [==============================] - 0s 158us/step - loss: 0.4494 - acc: 0.7938 - val_loss: 0.4652 - val_acc: 0.7992\n",
      "Epoch 90/150\n",
      "514/514 [==============================] - 0s 152us/step - loss: 0.4470 - acc: 0.7977 - val_loss: 0.4725 - val_acc: 0.7717\n",
      "Epoch 91/150\n",
      "514/514 [==============================] - 0s 150us/step - loss: 0.4491 - acc: 0.7996 - val_loss: 0.5285 - val_acc: 0.7165\n",
      "Epoch 92/150\n",
      "514/514 [==============================] - 0s 162us/step - loss: 0.4504 - acc: 0.7977 - val_loss: 0.4654 - val_acc: 0.7913\n",
      "Epoch 93/150\n",
      "514/514 [==============================] - 0s 165us/step - loss: 0.4439 - acc: 0.7938 - val_loss: 0.4732 - val_acc: 0.7913\n",
      "Epoch 94/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.4507 - acc: 0.7996 - val_loss: 0.4805 - val_acc: 0.7756\n",
      "Epoch 95/150\n",
      "514/514 [==============================] - 0s 152us/step - loss: 0.4513 - acc: 0.7821 - val_loss: 0.4945 - val_acc: 0.7362\n",
      "Epoch 96/150\n",
      "514/514 [==============================] - 0s 148us/step - loss: 0.4570 - acc: 0.7899 - val_loss: 0.5035 - val_acc: 0.7402\n",
      "Epoch 97/150\n",
      "514/514 [==============================] - 0s 150us/step - loss: 0.4469 - acc: 0.7802 - val_loss: 0.4651 - val_acc: 0.7795\n",
      "Epoch 98/150\n",
      "514/514 [==============================] - 0s 156us/step - loss: 0.4538 - acc: 0.7840 - val_loss: 0.4897 - val_acc: 0.7835\n",
      "Epoch 99/150\n",
      "514/514 [==============================] - 0s 154us/step - loss: 0.4529 - acc: 0.7918 - val_loss: 0.4687 - val_acc: 0.7756\n",
      "Epoch 100/150\n",
      "514/514 [==============================] - 0s 181us/step - loss: 0.4680 - acc: 0.7743 - val_loss: 0.4678 - val_acc: 0.7874\n",
      "Epoch 101/150\n",
      "514/514 [==============================] - 0s 152us/step - loss: 0.4561 - acc: 0.8016 - val_loss: 0.4723 - val_acc: 0.7795\n",
      "Epoch 102/150\n",
      "514/514 [==============================] - 0s 150us/step - loss: 0.4491 - acc: 0.7879 - val_loss: 0.4770 - val_acc: 0.8031\n",
      "Epoch 103/150\n",
      "514/514 [==============================] - 0s 162us/step - loss: 0.4412 - acc: 0.8016 - val_loss: 0.4833 - val_acc: 0.7913\n",
      "Epoch 104/150\n",
      "514/514 [==============================] - 0s 146us/step - loss: 0.4480 - acc: 0.7938 - val_loss: 0.4757 - val_acc: 0.7913\n",
      "Epoch 105/150\n",
      "514/514 [==============================] - 0s 154us/step - loss: 0.4460 - acc: 0.8074 - val_loss: 0.4659 - val_acc: 0.7835\n",
      "Epoch 106/150\n",
      "514/514 [==============================] - 0s 158us/step - loss: 0.4456 - acc: 0.8035 - val_loss: 0.4883 - val_acc: 0.7795\n",
      "Epoch 107/150\n",
      "514/514 [==============================] - 0s 154us/step - loss: 0.4520 - acc: 0.7879 - val_loss: 0.4746 - val_acc: 0.7835\n",
      "Epoch 108/150\n",
      "514/514 [==============================] - 0s 150us/step - loss: 0.4425 - acc: 0.7996 - val_loss: 0.4767 - val_acc: 0.7835\n",
      "Epoch 109/150\n",
      "514/514 [==============================] - 0s 158us/step - loss: 0.4436 - acc: 0.7957 - val_loss: 0.4878 - val_acc: 0.7677\n",
      "Epoch 110/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.4397 - acc: 0.8093 - val_loss: 0.5248 - val_acc: 0.7441\n",
      "Epoch 111/150\n",
      "514/514 [==============================] - 0s 156us/step - loss: 0.4511 - acc: 0.8074 - val_loss: 0.4725 - val_acc: 0.7874\n",
      "Epoch 112/150\n",
      "514/514 [==============================] - 0s 156us/step - loss: 0.4387 - acc: 0.7938 - val_loss: 0.4954 - val_acc: 0.7835\n",
      "Epoch 113/150\n",
      "514/514 [==============================] - 0s 154us/step - loss: 0.4423 - acc: 0.8152 - val_loss: 0.4786 - val_acc: 0.7677\n",
      "Epoch 114/150\n",
      "514/514 [==============================] - 0s 146us/step - loss: 0.4600 - acc: 0.7879 - val_loss: 0.4913 - val_acc: 0.7835\n",
      "Epoch 115/150\n",
      "514/514 [==============================] - 0s 150us/step - loss: 0.4491 - acc: 0.7802 - val_loss: 0.4929 - val_acc: 0.7874\n",
      "Epoch 116/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.4568 - acc: 0.7899 - val_loss: 0.4945 - val_acc: 0.7677\n",
      "Epoch 117/150\n",
      "514/514 [==============================] - 0s 152us/step - loss: 0.4324 - acc: 0.8074 - val_loss: 0.4796 - val_acc: 0.7795\n",
      "Epoch 118/150\n",
      "514/514 [==============================] - 0s 164us/step - loss: 0.4533 - acc: 0.7860 - val_loss: 0.4701 - val_acc: 0.7913\n",
      "Epoch 119/150\n",
      "514/514 [==============================] - 0s 154us/step - loss: 0.4545 - acc: 0.7782 - val_loss: 0.4977 - val_acc: 0.7638\n",
      "Epoch 120/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "514/514 [==============================] - 0s 146us/step - loss: 0.4456 - acc: 0.7840 - val_loss: 0.4872 - val_acc: 0.7677\n",
      "Epoch 121/150\n",
      "514/514 [==============================] - 0s 148us/step - loss: 0.4347 - acc: 0.7977 - val_loss: 0.5041 - val_acc: 0.7520\n",
      "Epoch 122/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.4418 - acc: 0.8093 - val_loss: 0.4749 - val_acc: 0.7835\n",
      "Epoch 123/150\n",
      "514/514 [==============================] - 0s 156us/step - loss: 0.4371 - acc: 0.7938 - val_loss: 0.4816 - val_acc: 0.7992\n",
      "Epoch 124/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.4353 - acc: 0.8074 - val_loss: 0.4799 - val_acc: 0.7835\n",
      "Epoch 125/150\n",
      "514/514 [==============================] - 0s 146us/step - loss: 0.4428 - acc: 0.8035 - val_loss: 0.4872 - val_acc: 0.7717\n",
      "Epoch 126/150\n",
      "514/514 [==============================] - 0s 150us/step - loss: 0.4496 - acc: 0.7918 - val_loss: 0.5175 - val_acc: 0.7598\n",
      "Epoch 127/150\n",
      "514/514 [==============================] - 0s 164us/step - loss: 0.4388 - acc: 0.7977 - val_loss: 0.4977 - val_acc: 0.7874\n",
      "Epoch 128/150\n",
      "514/514 [==============================] - 0s 152us/step - loss: 0.4412 - acc: 0.8035 - val_loss: 0.4640 - val_acc: 0.7835\n",
      "Epoch 129/150\n",
      "514/514 [==============================] - 0s 150us/step - loss: 0.4472 - acc: 0.8016 - val_loss: 0.4865 - val_acc: 0.7598\n",
      "Epoch 130/150\n",
      "514/514 [==============================] - 0s 158us/step - loss: 0.4362 - acc: 0.7840 - val_loss: 0.4870 - val_acc: 0.7598\n",
      "Epoch 131/150\n",
      "514/514 [==============================] - 0s 152us/step - loss: 0.4495 - acc: 0.8016 - val_loss: 0.5402 - val_acc: 0.7441\n",
      "Epoch 132/150\n",
      "514/514 [==============================] - 0s 154us/step - loss: 0.4427 - acc: 0.8035 - val_loss: 0.4817 - val_acc: 0.7835\n",
      "Epoch 133/150\n",
      "514/514 [==============================] - 0s 146us/step - loss: 0.4234 - acc: 0.8113 - val_loss: 0.5005 - val_acc: 0.7835\n",
      "Epoch 134/150\n",
      "514/514 [==============================] - 0s 169us/step - loss: 0.4657 - acc: 0.7782 - val_loss: 0.4922 - val_acc: 0.7717\n",
      "Epoch 135/150\n",
      "514/514 [==============================] - 0s 148us/step - loss: 0.4310 - acc: 0.8230 - val_loss: 0.4973 - val_acc: 0.7559\n",
      "Epoch 136/150\n",
      "514/514 [==============================] - 0s 156us/step - loss: 0.4387 - acc: 0.7938 - val_loss: 0.5052 - val_acc: 0.7756\n",
      "Epoch 137/150\n",
      "514/514 [==============================] - 0s 144us/step - loss: 0.4456 - acc: 0.7996 - val_loss: 0.4907 - val_acc: 0.7559\n",
      "Epoch 138/150\n",
      "514/514 [==============================] - 0s 150us/step - loss: 0.4373 - acc: 0.7879 - val_loss: 0.4947 - val_acc: 0.7756\n",
      "Epoch 139/150\n",
      "514/514 [==============================] - 0s 164us/step - loss: 0.4450 - acc: 0.7938 - val_loss: 0.4908 - val_acc: 0.7677\n",
      "Epoch 140/150\n",
      "514/514 [==============================] - 0s 158us/step - loss: 0.4344 - acc: 0.8093 - val_loss: 0.4867 - val_acc: 0.7874\n",
      "Epoch 141/150\n",
      "514/514 [==============================] - 0s 156us/step - loss: 0.4541 - acc: 0.7957 - val_loss: 0.5408 - val_acc: 0.7283\n",
      "Epoch 142/150\n",
      "514/514 [==============================] - 0s 156us/step - loss: 0.4560 - acc: 0.7743 - val_loss: 0.4890 - val_acc: 0.7638\n",
      "Epoch 143/150\n",
      "514/514 [==============================] - 0s 144us/step - loss: 0.4292 - acc: 0.8054 - val_loss: 0.5083 - val_acc: 0.7756\n",
      "Epoch 144/150\n",
      "514/514 [==============================] - 0s 158us/step - loss: 0.4542 - acc: 0.7840 - val_loss: 0.4987 - val_acc: 0.7638\n",
      "Epoch 145/150\n",
      "514/514 [==============================] - 0s 183us/step - loss: 0.4277 - acc: 0.7996 - val_loss: 0.4997 - val_acc: 0.7992\n",
      "Epoch 146/150\n",
      "514/514 [==============================] - 0s 169us/step - loss: 0.4335 - acc: 0.8132 - val_loss: 0.4831 - val_acc: 0.7835\n",
      "Epoch 147/150\n",
      "514/514 [==============================] - 0s 177us/step - loss: 0.4286 - acc: 0.8093 - val_loss: 0.4977 - val_acc: 0.7874\n",
      "Epoch 148/150\n",
      "514/514 [==============================] - 0s 187us/step - loss: 0.4350 - acc: 0.7840 - val_loss: 0.4757 - val_acc: 0.7913\n",
      "Epoch 149/150\n",
      "514/514 [==============================] - 0s 171us/step - loss: 0.4369 - acc: 0.8054 - val_loss: 0.4839 - val_acc: 0.7953\n",
      "Epoch 150/150\n",
      "514/514 [==============================] - 0s 162us/step - loss: 0.4346 - acc: 0.8035 - val_loss: 0.4998 - val_acc: 0.7953\n"
     ]
    }
   ],
   "source": [
    "history2=model.fit(X,Y,epochs=150,batch_size=10,validation_split=0.33)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 手动验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 514 samples, validate on 254 samples\n",
      "Epoch 1/150\n",
      "514/514 [==============================] - 0s 173us/step - loss: 0.4695 - acc: 0.7899 - val_loss: 0.4277 - val_acc: 0.8150\n",
      "Epoch 2/150\n",
      "514/514 [==============================] - 0s 167us/step - loss: 0.4603 - acc: 0.7957 - val_loss: 0.4310 - val_acc: 0.8071\n",
      "Epoch 3/150\n",
      "514/514 [==============================] - 0s 154us/step - loss: 0.4410 - acc: 0.8074 - val_loss: 0.4127 - val_acc: 0.8268\n",
      "Epoch 4/150\n",
      "514/514 [==============================] - ETA: 0s - loss: 0.4059 - acc: 0.800 - ETA: 0s - loss: 0.4482 - acc: 0.791 - 0s 165us/step - loss: 0.4577 - acc: 0.7879 - val_loss: 0.4172 - val_acc: 0.8110\n",
      "Epoch 5/150\n",
      "514/514 [==============================] - 0s 158us/step - loss: 0.4454 - acc: 0.7957 - val_loss: 0.4459 - val_acc: 0.7795\n",
      "Epoch 6/150\n",
      "514/514 [==============================] - 0s 202us/step - loss: 0.4676 - acc: 0.7626 - val_loss: 0.4295 - val_acc: 0.7992\n",
      "Epoch 7/150\n",
      "514/514 [==============================] - 0s 164us/step - loss: 0.4716 - acc: 0.7821 - val_loss: 0.4229 - val_acc: 0.8031\n",
      "Epoch 8/150\n",
      "514/514 [==============================] - 0s 177us/step - loss: 0.4466 - acc: 0.7977 - val_loss: 0.4466 - val_acc: 0.7835\n",
      "Epoch 9/150\n",
      "514/514 [==============================] - 0s 169us/step - loss: 0.4482 - acc: 0.8054 - val_loss: 0.4473 - val_acc: 0.8150\n",
      "Epoch 10/150\n",
      "514/514 [==============================] - 0s 162us/step - loss: 0.4446 - acc: 0.8054 - val_loss: 0.4270 - val_acc: 0.7953\n",
      "Epoch 11/150\n",
      "514/514 [==============================] - 0s 173us/step - loss: 0.4513 - acc: 0.7879 - val_loss: 0.4364 - val_acc: 0.8031\n",
      "Epoch 12/150\n",
      "514/514 [==============================] - 0s 158us/step - loss: 0.4500 - acc: 0.7879 - val_loss: 0.4376 - val_acc: 0.7953\n",
      "Epoch 13/150\n",
      "514/514 [==============================] - 0s 142us/step - loss: 0.4448 - acc: 0.8016 - val_loss: 0.4590 - val_acc: 0.7756\n",
      "Epoch 14/150\n",
      "514/514 [==============================] - 0s 150us/step - loss: 0.4498 - acc: 0.7918 - val_loss: 0.4245 - val_acc: 0.7953\n",
      "Epoch 15/150\n",
      "514/514 [==============================] - 0s 154us/step - loss: 0.4441 - acc: 0.7938 - val_loss: 0.4290 - val_acc: 0.8071\n",
      "Epoch 16/150\n",
      "514/514 [==============================] - 0s 152us/step - loss: 0.4401 - acc: 0.7938 - val_loss: 0.4369 - val_acc: 0.7913\n",
      "Epoch 17/150\n",
      "514/514 [==============================] - 0s 167us/step - loss: 0.4343 - acc: 0.8035 - val_loss: 0.4311 - val_acc: 0.8031\n",
      "Epoch 18/150\n",
      "514/514 [==============================] - 0s 152us/step - loss: 0.4512 - acc: 0.7977 - val_loss: 0.4339 - val_acc: 0.8071\n",
      "Epoch 19/150\n",
      "514/514 [==============================] - 0s 146us/step - loss: 0.4548 - acc: 0.7840 - val_loss: 0.4322 - val_acc: 0.7953\n",
      "Epoch 20/150\n",
      "514/514 [==============================] - 0s 150us/step - loss: 0.4489 - acc: 0.7957 - val_loss: 0.4452 - val_acc: 0.7992\n",
      "Epoch 21/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.4396 - acc: 0.7977 - val_loss: 0.4482 - val_acc: 0.7913\n",
      "Epoch 22/150\n",
      "514/514 [==============================] - 0s 156us/step - loss: 0.4302 - acc: 0.8035 - val_loss: 0.4350 - val_acc: 0.7953\n",
      "Epoch 23/150\n",
      "514/514 [==============================] - 0s 158us/step - loss: 0.4469 - acc: 0.7938 - val_loss: 0.4368 - val_acc: 0.7992\n",
      "Epoch 24/150\n",
      "514/514 [==============================] - 0s 152us/step - loss: 0.4428 - acc: 0.7840 - val_loss: 0.4428 - val_acc: 0.8031\n",
      "Epoch 25/150\n",
      "514/514 [==============================] - 0s 152us/step - loss: 0.4366 - acc: 0.7996 - val_loss: 0.4292 - val_acc: 0.8031\n",
      "Epoch 26/150\n",
      "514/514 [==============================] - 0s 169us/step - loss: 0.4394 - acc: 0.7938 - val_loss: 0.4340 - val_acc: 0.8031\n",
      "Epoch 27/150\n",
      "514/514 [==============================] - 0s 150us/step - loss: 0.4492 - acc: 0.8016 - val_loss: 0.4375 - val_acc: 0.7913\n",
      "Epoch 28/150\n",
      "514/514 [==============================] - 0s 144us/step - loss: 0.4315 - acc: 0.8171 - val_loss: 0.4395 - val_acc: 0.7953\n",
      "Epoch 29/150\n",
      "514/514 [==============================] - ETA: 0s - loss: 0.4241 - acc: 0.797 - 0s 169us/step - loss: 0.4333 - acc: 0.7899 - val_loss: 0.4430 - val_acc: 0.7992\n",
      "Epoch 30/150\n",
      "514/514 [==============================] - 0s 154us/step - loss: 0.4324 - acc: 0.7957 - val_loss: 0.4733 - val_acc: 0.7835\n",
      "Epoch 31/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.4523 - acc: 0.7860 - val_loss: 0.4551 - val_acc: 0.7913\n",
      "Epoch 32/150\n",
      "514/514 [==============================] - 0s 175us/step - loss: 0.4471 - acc: 0.7860 - val_loss: 0.4528 - val_acc: 0.7795\n",
      "Epoch 33/150\n",
      "514/514 [==============================] - 0s 156us/step - loss: 0.4417 - acc: 0.7802 - val_loss: 0.4353 - val_acc: 0.7953\n",
      "Epoch 34/150\n",
      "514/514 [==============================] - 0s 146us/step - loss: 0.4356 - acc: 0.8035 - val_loss: 0.4390 - val_acc: 0.7953\n",
      "Epoch 35/150\n",
      "514/514 [==============================] - 0s 165us/step - loss: 0.4387 - acc: 0.7840 - val_loss: 0.4428 - val_acc: 0.8071\n",
      "Epoch 36/150\n",
      "514/514 [==============================] - 0s 164us/step - loss: 0.4236 - acc: 0.8152 - val_loss: 0.4468 - val_acc: 0.7835\n",
      "Epoch 37/150\n",
      "514/514 [==============================] - 0s 140us/step - loss: 0.4309 - acc: 0.7957 - val_loss: 0.4353 - val_acc: 0.7913\n",
      "Epoch 38/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.4343 - acc: 0.7821 - val_loss: 0.4732 - val_acc: 0.7992\n",
      "Epoch 39/150\n",
      "514/514 [==============================] - 0s 167us/step - loss: 0.4301 - acc: 0.7918 - val_loss: 0.4309 - val_acc: 0.8071\n",
      "Epoch 40/150\n",
      "514/514 [==============================] - 0s 158us/step - loss: 0.4302 - acc: 0.7996 - val_loss: 0.4614 - val_acc: 0.7638\n",
      "Epoch 41/150\n",
      "514/514 [==============================] - 0s 152us/step - loss: 0.4358 - acc: 0.7977 - val_loss: 0.4442 - val_acc: 0.7835\n",
      "Epoch 42/150\n",
      "514/514 [==============================] - 0s 148us/step - loss: 0.4264 - acc: 0.7996 - val_loss: 0.4651 - val_acc: 0.7953\n",
      "Epoch 43/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.4197 - acc: 0.7918 - val_loss: 0.4498 - val_acc: 0.7913\n",
      "Epoch 44/150\n",
      "514/514 [==============================] - 0s 169us/step - loss: 0.4296 - acc: 0.8093 - val_loss: 0.4785 - val_acc: 0.7953\n",
      "Epoch 45/150\n",
      "514/514 [==============================] - 0s 146us/step - loss: 0.4258 - acc: 0.8016 - val_loss: 0.4654 - val_acc: 0.7638\n",
      "Epoch 46/150\n",
      "514/514 [==============================] - 0s 158us/step - loss: 0.4273 - acc: 0.8035 - val_loss: 0.4508 - val_acc: 0.7913\n",
      "Epoch 47/150\n",
      "514/514 [==============================] - 0s 152us/step - loss: 0.4279 - acc: 0.8093 - val_loss: 0.4517 - val_acc: 0.7953\n",
      "Epoch 48/150\n",
      "514/514 [==============================] - 0s 146us/step - loss: 0.4355 - acc: 0.7840 - val_loss: 0.4712 - val_acc: 0.7677\n",
      "Epoch 49/150\n",
      "514/514 [==============================] - 0s 156us/step - loss: 0.4352 - acc: 0.7879 - val_loss: 0.4507 - val_acc: 0.7795\n",
      "Epoch 50/150\n",
      "514/514 [==============================] - 0s 158us/step - loss: 0.4267 - acc: 0.7977 - val_loss: 0.4558 - val_acc: 0.7717\n",
      "Epoch 51/150\n",
      "514/514 [==============================] - ETA: 0s - loss: 0.4335 - acc: 0.783 - 0s 160us/step - loss: 0.4236 - acc: 0.7938 - val_loss: 0.4573 - val_acc: 0.7874\n",
      "Epoch 52/150\n",
      "514/514 [==============================] - 0s 183us/step - loss: 0.4391 - acc: 0.7957 - val_loss: 0.5038 - val_acc: 0.7638\n",
      "Epoch 53/150\n",
      "514/514 [==============================] - 0s 156us/step - loss: 0.4364 - acc: 0.7957 - val_loss: 0.4752 - val_acc: 0.7953\n",
      "Epoch 54/150\n",
      "514/514 [==============================] - 0s 173us/step - loss: 0.4275 - acc: 0.7938 - val_loss: 0.4607 - val_acc: 0.7756\n",
      "Epoch 55/150\n",
      "514/514 [==============================] - 0s 164us/step - loss: 0.4214 - acc: 0.8054 - val_loss: 0.4578 - val_acc: 0.8031\n",
      "Epoch 56/150\n",
      "514/514 [==============================] - 0s 195us/step - loss: 0.4192 - acc: 0.8035 - val_loss: 0.4654 - val_acc: 0.7756\n",
      "Epoch 57/150\n",
      "514/514 [==============================] - 0s 169us/step - loss: 0.4343 - acc: 0.7724 - val_loss: 0.4492 - val_acc: 0.7756\n",
      "Epoch 58/150\n",
      "514/514 [==============================] - 0s 183us/step - loss: 0.4157 - acc: 0.8054 - val_loss: 0.4553 - val_acc: 0.7874\n",
      "Epoch 59/150\n",
      "514/514 [==============================] - 0s 179us/step - loss: 0.4219 - acc: 0.8093 - val_loss: 0.4531 - val_acc: 0.7953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/150\n",
      "514/514 [==============================] - 0s 175us/step - loss: 0.4196 - acc: 0.8054 - val_loss: 0.4665 - val_acc: 0.7874\n",
      "Epoch 61/150\n",
      "514/514 [==============================] - 0s 158us/step - loss: 0.4179 - acc: 0.8054 - val_loss: 0.4739 - val_acc: 0.7913\n",
      "Epoch 62/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.4240 - acc: 0.8054 - val_loss: 0.4578 - val_acc: 0.7795\n",
      "Epoch 63/150\n",
      "514/514 [==============================] - 0s 144us/step - loss: 0.4126 - acc: 0.8074 - val_loss: 0.5146 - val_acc: 0.7559\n",
      "Epoch 64/150\n",
      "514/514 [==============================] - 0s 148us/step - loss: 0.4348 - acc: 0.7957 - val_loss: 0.4655 - val_acc: 0.7835\n",
      "Epoch 65/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.4293 - acc: 0.7957 - val_loss: 0.4969 - val_acc: 0.7520\n",
      "Epoch 66/150\n",
      "514/514 [==============================] - 0s 164us/step - loss: 0.4285 - acc: 0.8054 - val_loss: 0.4508 - val_acc: 0.7992\n",
      "Epoch 67/150\n",
      "514/514 [==============================] - 0s 171us/step - loss: 0.4236 - acc: 0.7860 - val_loss: 0.4722 - val_acc: 0.7874\n",
      "Epoch 68/150\n",
      "514/514 [==============================] - 0s 158us/step - loss: 0.4271 - acc: 0.7802 - val_loss: 0.4662 - val_acc: 0.7874\n",
      "Epoch 69/150\n",
      "514/514 [==============================] - 0s 164us/step - loss: 0.4255 - acc: 0.7957 - val_loss: 0.4543 - val_acc: 0.7795\n",
      "Epoch 70/150\n",
      "514/514 [==============================] - ETA: 0s - loss: 0.4173 - acc: 0.812 - 0s 162us/step - loss: 0.4215 - acc: 0.8093 - val_loss: 0.4664 - val_acc: 0.7913\n",
      "Epoch 71/150\n",
      "514/514 [==============================] - 0s 165us/step - loss: 0.4241 - acc: 0.7996 - val_loss: 0.4551 - val_acc: 0.7756\n",
      "Epoch 72/150\n",
      "514/514 [==============================] - 0s 162us/step - loss: 0.4332 - acc: 0.7938 - val_loss: 0.4657 - val_acc: 0.8031\n",
      "Epoch 73/150\n",
      "514/514 [==============================] - 0s 175us/step - loss: 0.4284 - acc: 0.8035 - val_loss: 0.4826 - val_acc: 0.7795\n",
      "Epoch 74/150\n",
      "514/514 [==============================] - 0s 148us/step - loss: 0.4184 - acc: 0.8035 - val_loss: 0.4837 - val_acc: 0.7795\n",
      "Epoch 75/150\n",
      "514/514 [==============================] - 0s 167us/step - loss: 0.4218 - acc: 0.7918 - val_loss: 0.4646 - val_acc: 0.7953\n",
      "Epoch 76/150\n",
      "514/514 [==============================] - 0s 202us/step - loss: 0.4282 - acc: 0.7938 - val_loss: 0.4669 - val_acc: 0.7835\n",
      "Epoch 77/150\n",
      "514/514 [==============================] - 0s 197us/step - loss: 0.4080 - acc: 0.8152 - val_loss: 0.4502 - val_acc: 0.7835\n",
      "Epoch 78/150\n",
      "514/514 [==============================] - 0s 177us/step - loss: 0.4281 - acc: 0.7957 - val_loss: 0.4610 - val_acc: 0.7953\n",
      "Epoch 79/150\n",
      "514/514 [==============================] - 0s 183us/step - loss: 0.4192 - acc: 0.7957 - val_loss: 0.4792 - val_acc: 0.7795\n",
      "Epoch 80/150\n",
      "514/514 [==============================] - 0s 167us/step - loss: 0.4255 - acc: 0.8152 - val_loss: 0.4864 - val_acc: 0.7874\n",
      "Epoch 81/150\n",
      "514/514 [==============================] - 0s 158us/step - loss: 0.4184 - acc: 0.8074 - val_loss: 0.5098 - val_acc: 0.7559\n",
      "Epoch 82/150\n",
      "514/514 [==============================] - 0s 156us/step - loss: 0.4311 - acc: 0.7918 - val_loss: 0.4723 - val_acc: 0.7795\n",
      "Epoch 83/150\n",
      "514/514 [==============================] - 0s 154us/step - loss: 0.4166 - acc: 0.8113 - val_loss: 0.4547 - val_acc: 0.7874\n",
      "Epoch 84/150\n",
      "514/514 [==============================] - 0s 144us/step - loss: 0.4173 - acc: 0.8074 - val_loss: 0.4676 - val_acc: 0.7874\n",
      "Epoch 85/150\n",
      "514/514 [==============================] - 0s 152us/step - loss: 0.4122 - acc: 0.8132 - val_loss: 0.4884 - val_acc: 0.7638\n",
      "Epoch 86/150\n",
      "514/514 [==============================] - 0s 152us/step - loss: 0.4221 - acc: 0.7957 - val_loss: 0.4572 - val_acc: 0.7913\n",
      "Epoch 87/150\n",
      "514/514 [==============================] - 0s 164us/step - loss: 0.4200 - acc: 0.8230 - val_loss: 0.5223 - val_acc: 0.7362\n",
      "Epoch 88/150\n",
      "514/514 [==============================] - ETA: 0s - loss: 0.4146 - acc: 0.811 - 0s 140us/step - loss: 0.4136 - acc: 0.8113 - val_loss: 0.4612 - val_acc: 0.7717\n",
      "Epoch 89/150\n",
      "514/514 [==============================] - 0s 154us/step - loss: 0.4083 - acc: 0.8210 - val_loss: 0.4779 - val_acc: 0.7835\n",
      "Epoch 90/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.4179 - acc: 0.7899 - val_loss: 0.4624 - val_acc: 0.7638\n",
      "Epoch 91/150\n",
      "514/514 [==============================] - 0s 154us/step - loss: 0.4220 - acc: 0.7996 - val_loss: 0.4847 - val_acc: 0.7638\n",
      "Epoch 92/150\n",
      "514/514 [==============================] - 0s 150us/step - loss: 0.4021 - acc: 0.8132 - val_loss: 0.4794 - val_acc: 0.7795\n",
      "Epoch 93/150\n",
      "514/514 [==============================] - 0s 152us/step - loss: 0.4305 - acc: 0.7918 - val_loss: 0.5042 - val_acc: 0.7677\n",
      "Epoch 94/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.4197 - acc: 0.8016 - val_loss: 0.4810 - val_acc: 0.7795\n",
      "Epoch 95/150\n",
      "514/514 [==============================] - 0s 158us/step - loss: 0.4092 - acc: 0.8191 - val_loss: 0.4830 - val_acc: 0.7795\n",
      "Epoch 96/150\n",
      "514/514 [==============================] - 0s 185us/step - loss: 0.4131 - acc: 0.8152 - val_loss: 0.4741 - val_acc: 0.7874\n",
      "Epoch 97/150\n",
      "514/514 [==============================] - 0s 185us/step - loss: 0.4061 - acc: 0.8152 - val_loss: 0.4725 - val_acc: 0.7835\n",
      "Epoch 98/150\n",
      "514/514 [==============================] - 0s 165us/step - loss: 0.4025 - acc: 0.8113 - val_loss: 0.4929 - val_acc: 0.7480\n",
      "Epoch 99/150\n",
      "514/514 [==============================] - 0s 150us/step - loss: 0.4186 - acc: 0.7977 - val_loss: 0.4533 - val_acc: 0.7874\n",
      "Epoch 100/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.4227 - acc: 0.7918 - val_loss: 0.4695 - val_acc: 0.7953\n",
      "Epoch 101/150\n",
      "514/514 [==============================] - 0s 156us/step - loss: 0.4045 - acc: 0.8191 - val_loss: 0.4615 - val_acc: 0.7874\n",
      "Epoch 102/150\n",
      "514/514 [==============================] - ETA: 0s - loss: 0.4192 - acc: 0.802 - 0s 162us/step - loss: 0.4201 - acc: 0.8074 - val_loss: 0.4902 - val_acc: 0.7756\n",
      "Epoch 103/150\n",
      "514/514 [==============================] - ETA: 0s - loss: 0.4084 - acc: 0.822 - 0s 165us/step - loss: 0.4066 - acc: 0.8132 - val_loss: 0.4734 - val_acc: 0.7874\n",
      "Epoch 104/150\n",
      "514/514 [==============================] - 0s 154us/step - loss: 0.4103 - acc: 0.8054 - val_loss: 0.4730 - val_acc: 0.7953\n",
      "Epoch 105/150\n",
      "514/514 [==============================] - 0s 158us/step - loss: 0.4231 - acc: 0.8016 - val_loss: 0.4836 - val_acc: 0.7835\n",
      "Epoch 106/150\n",
      "514/514 [==============================] - 0s 156us/step - loss: 0.4072 - acc: 0.8035 - val_loss: 0.5219 - val_acc: 0.7441\n",
      "Epoch 107/150\n",
      "514/514 [==============================] - 0s 152us/step - loss: 0.4053 - acc: 0.8132 - val_loss: 0.4934 - val_acc: 0.7717\n",
      "Epoch 108/150\n",
      "514/514 [==============================] - 0s 158us/step - loss: 0.4206 - acc: 0.7996 - val_loss: 0.4820 - val_acc: 0.7717\n",
      "Epoch 109/150\n",
      "514/514 [==============================] - 0s 144us/step - loss: 0.4144 - acc: 0.7977 - val_loss: 0.4778 - val_acc: 0.7835\n",
      "Epoch 110/150\n",
      "514/514 [==============================] - 0s 167us/step - loss: 0.4005 - acc: 0.8152 - val_loss: 0.4862 - val_acc: 0.7756\n",
      "Epoch 111/150\n",
      "514/514 [==============================] - 0s 171us/step - loss: 0.4145 - acc: 0.8132 - val_loss: 0.4741 - val_acc: 0.7835\n",
      "Epoch 112/150\n",
      "514/514 [==============================] - 0s 177us/step - loss: 0.4033 - acc: 0.8074 - val_loss: 0.4947 - val_acc: 0.7480\n",
      "Epoch 113/150\n",
      "514/514 [==============================] - 0s 165us/step - loss: 0.4145 - acc: 0.7977 - val_loss: 0.5033 - val_acc: 0.7559\n",
      "Epoch 114/150\n",
      "514/514 [==============================] - 0s 175us/step - loss: 0.4049 - acc: 0.8074 - val_loss: 0.4672 - val_acc: 0.7953\n",
      "Epoch 115/150\n",
      "514/514 [==============================] - 0s 169us/step - loss: 0.4007 - acc: 0.8113 - val_loss: 0.4757 - val_acc: 0.7677\n",
      "Epoch 116/150\n",
      "514/514 [==============================] - 0s 156us/step - loss: 0.4105 - acc: 0.8016 - val_loss: 0.4929 - val_acc: 0.7717\n",
      "Epoch 117/150\n",
      "514/514 [==============================] - 0s 162us/step - loss: 0.4247 - acc: 0.7899 - val_loss: 0.4935 - val_acc: 0.7795\n",
      "Epoch 118/150\n",
      "514/514 [==============================] - 0s 140us/step - loss: 0.4021 - acc: 0.8191 - val_loss: 0.4624 - val_acc: 0.7756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119/150\n",
      "514/514 [==============================] - 0s 144us/step - loss: 0.4033 - acc: 0.8191 - val_loss: 0.4883 - val_acc: 0.7795\n",
      "Epoch 120/150\n",
      "514/514 [==============================] - 0s 140us/step - loss: 0.4006 - acc: 0.8132 - val_loss: 0.4898 - val_acc: 0.7717\n",
      "Epoch 121/150\n",
      "514/514 [==============================] - 0s 144us/step - loss: 0.4062 - acc: 0.8191 - val_loss: 0.4706 - val_acc: 0.7795\n",
      "Epoch 122/150\n",
      "514/514 [==============================] - 0s 146us/step - loss: 0.4169 - acc: 0.8016 - val_loss: 0.4920 - val_acc: 0.7835\n",
      "Epoch 123/150\n",
      "514/514 [==============================] - 0s 154us/step - loss: 0.4042 - acc: 0.8093 - val_loss: 0.4699 - val_acc: 0.7913\n",
      "Epoch 124/150\n",
      "514/514 [==============================] - 0s 140us/step - loss: 0.4068 - acc: 0.8132 - val_loss: 0.4804 - val_acc: 0.7717\n",
      "Epoch 125/150\n",
      "514/514 [==============================] - 0s 142us/step - loss: 0.4042 - acc: 0.8210 - val_loss: 0.4797 - val_acc: 0.7717\n",
      "Epoch 126/150\n",
      "514/514 [==============================] - 0s 140us/step - loss: 0.3976 - acc: 0.8210 - val_loss: 0.4674 - val_acc: 0.7835\n",
      "Epoch 127/150\n",
      "514/514 [==============================] - ETA: 0s - loss: 0.4026 - acc: 0.809 - 0s 140us/step - loss: 0.4004 - acc: 0.8113 - val_loss: 0.4902 - val_acc: 0.7717\n",
      "Epoch 128/150\n",
      "514/514 [==============================] - 0s 146us/step - loss: 0.4424 - acc: 0.7977 - val_loss: 0.4966 - val_acc: 0.7756\n",
      "Epoch 129/150\n",
      "514/514 [==============================] - 0s 142us/step - loss: 0.4025 - acc: 0.8152 - val_loss: 0.4804 - val_acc: 0.7874\n",
      "Epoch 130/150\n",
      "514/514 [==============================] - 0s 150us/step - loss: 0.3997 - acc: 0.8132 - val_loss: 0.5010 - val_acc: 0.7441\n",
      "Epoch 131/150\n",
      "514/514 [==============================] - 0s 144us/step - loss: 0.4031 - acc: 0.8171 - val_loss: 0.4963 - val_acc: 0.7638\n",
      "Epoch 132/150\n",
      "514/514 [==============================] - 0s 150us/step - loss: 0.3986 - acc: 0.8230 - val_loss: 0.4720 - val_acc: 0.7795\n",
      "Epoch 133/150\n",
      "514/514 [==============================] - 0s 117us/step - loss: 0.4038 - acc: 0.8093 - val_loss: 0.5020 - val_acc: 0.7638\n",
      "Epoch 134/150\n",
      "514/514 [==============================] - 0s 138us/step - loss: 0.3924 - acc: 0.8268 - val_loss: 0.4775 - val_acc: 0.7795\n",
      "Epoch 135/150\n",
      "514/514 [==============================] - 0s 140us/step - loss: 0.4058 - acc: 0.7996 - val_loss: 0.5111 - val_acc: 0.7638\n",
      "Epoch 136/150\n",
      "514/514 [==============================] - 0s 138us/step - loss: 0.4121 - acc: 0.8191 - val_loss: 0.4877 - val_acc: 0.7756\n",
      "Epoch 137/150\n",
      "514/514 [==============================] - 0s 138us/step - loss: 0.4103 - acc: 0.8054 - val_loss: 0.5029 - val_acc: 0.7480\n",
      "Epoch 138/150\n",
      "514/514 [==============================] - 0s 140us/step - loss: 0.4036 - acc: 0.8288 - val_loss: 0.4970 - val_acc: 0.7598\n",
      "Epoch 139/150\n",
      "514/514 [==============================] - 0s 146us/step - loss: 0.3900 - acc: 0.8210 - val_loss: 0.4840 - val_acc: 0.7598\n",
      "Epoch 140/150\n",
      "514/514 [==============================] - 0s 150us/step - loss: 0.4120 - acc: 0.7918 - val_loss: 0.4838 - val_acc: 0.7913\n",
      "Epoch 141/150\n",
      "514/514 [==============================] - 0s 142us/step - loss: 0.4012 - acc: 0.8171 - val_loss: 0.4906 - val_acc: 0.7717\n",
      "Epoch 142/150\n",
      "514/514 [==============================] - 0s 148us/step - loss: 0.3986 - acc: 0.8249 - val_loss: 0.4977 - val_acc: 0.7638\n",
      "Epoch 143/150\n",
      "514/514 [==============================] - 0s 158us/step - loss: 0.4047 - acc: 0.8171 - val_loss: 0.5009 - val_acc: 0.7480\n",
      "Epoch 144/150\n",
      "514/514 [==============================] - 0s 150us/step - loss: 0.4060 - acc: 0.8093 - val_loss: 0.4840 - val_acc: 0.7795\n",
      "Epoch 145/150\n",
      "514/514 [==============================] - 0s 144us/step - loss: 0.3961 - acc: 0.8152 - val_loss: 0.5099 - val_acc: 0.7559\n",
      "Epoch 146/150\n",
      "514/514 [==============================] - 0s 146us/step - loss: 0.3999 - acc: 0.8093 - val_loss: 0.5369 - val_acc: 0.7362\n",
      "Epoch 147/150\n",
      "514/514 [==============================] - 0s 144us/step - loss: 0.4135 - acc: 0.8113 - val_loss: 0.4717 - val_acc: 0.7795\n",
      "Epoch 148/150\n",
      "514/514 [==============================] - 0s 142us/step - loss: 0.4029 - acc: 0.8132 - val_loss: 0.4856 - val_acc: 0.7795\n",
      "Epoch 149/150\n",
      "514/514 [==============================] - 0s 144us/step - loss: 0.4054 - acc: 0.8152 - val_loss: 0.4979 - val_acc: 0.7677\n",
      "Epoch 150/150\n",
      "514/514 [==============================] - 0s 146us/step - loss: 0.4027 - acc: 0.8074 - val_loss: 0.4929 - val_acc: 0.7953\n"
     ]
    }
   ],
   "source": [
    "history3=model.fit(X_train,Y_train,validation_data=(X_test,Y_test),epochs=150,batch_size=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K折交叉验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "verbose：日志显示，0为不在标准输出流输出日志信息，1为输出进度条记录，2为每个epoch输出一行记录  就是上面那150个Epoch……不打印出来供人看。 verbose 信息冗余，多余的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/77 [==============================] - 0s 806us/step\n",
      "acc: 79.22%\n",
      "77/77 [==============================] - 0s 975us/step\n",
      "acc: 72.73%\n",
      "77/77 [==============================] - 0s 1ms/step\n",
      "acc: 74.03%\n",
      "77/77 [==============================] - 0s 1ms/step\n",
      "acc: 81.82%\n",
      "77/77 [==============================] - 0s 2ms/step\n",
      "acc: 77.92%\n",
      "77/77 [==============================] - 0s 2ms/step\n",
      "acc: 67.53%\n",
      "77/77 [==============================] - 0s 2ms/step\n",
      "acc: 76.62%\n",
      "77/77 [==============================] - 0s 2ms/step\n",
      "acc: 70.13%\n",
      "76/76 [==============================] - 0s 2ms/step\n",
      "acc: 72.37%\n",
      "76/76 [==============================] - 0s 3ms/step\n",
      "acc: 73.68%\n",
      "74.61% (+/- 4.10%)\n"
     ]
    }
   ],
   "source": [
    "kfold=StratifiedKFold(y=Y,n_folds=10,shuffle=True,random_state=seed) #7章最开始定义的 随机数种子 一直是7 \n",
    "# shuffle 在分组之前对数据进行'洗牌'  继续查看函数定义，return，可以知道\n",
    "# 这个函数返回的其实是Y对应的索引，(把Y的索引分成训练集和测试集)因为X和Y相互对应，知道其中一个索引，自然可以找到另一个的\n",
    "CVscores=[] #cross_validation 交叉验证\n",
    "for i,(train_index,test_index) in enumerate(kfold):\n",
    "    modelK=Sequential()\n",
    "    modelK.add(Dense(12,input_dim=8,kernel_initializer='uniform',activation='relu'))\n",
    "    modelK.add(Dense(8,kernel_initializer='uniform',activation='relu'))\n",
    "    modelK.add(Dense(1,kernel_initializer='uniform',activation='sigmoid'))\n",
    "    modelK.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    history4=modelK.fit(X[train_index],Y[train_index],epochs=150,batch_size=10,verbose=False)\n",
    "    scores=modelK.evaluate(X[test_index],Y[test_index])\n",
    "    print(\"%s: %.2f%%\" % (modelK.metrics_names[1], scores[1]*100))  #这里后面两个%% 是为了打印出%这个符号\n",
    "    CVscores.append(scores[1] * 100)\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(CVscores), np.std(CVscores)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意： 这里每次循环都重新生成了模型，而不是生成一次模型，使用不同的数据喂进去训练"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第9章  使用Scikit-Learn调用Keras的模型"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用scikit-learn验证深度学习模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.cross_validation import StratifiedKFold,cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "691/691 [==============================] - 1s 1ms/step - loss: 0.6816 - acc: 0.6512\n",
      "Epoch 2/150\n",
      "691/691 [==============================] - 0s 133us/step - loss: 0.6623 - acc: 0.6512\n",
      "Epoch 3/150\n",
      "691/691 [==============================] - 0s 133us/step - loss: 0.6546 - acc: 0.6469\n",
      "Epoch 4/150\n",
      "691/691 [==============================] - 0s 130us/step - loss: 0.6403 - acc: 0.6671\n",
      "Epoch 5/150\n",
      "691/691 [==============================] - 0s 129us/step - loss: 0.6253 - acc: 0.6758\n",
      "Epoch 6/150\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.6157 - acc: 0.6715\n",
      "Epoch 7/150\n",
      "691/691 [==============================] - 0s 129us/step - loss: 0.6105 - acc: 0.6874\n",
      "Epoch 8/150\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.6032 - acc: 0.6860\n",
      "Epoch 9/150\n",
      "691/691 [==============================] - 0s 126us/step - loss: 0.5998 - acc: 0.6802\n",
      "Epoch 10/150\n",
      "691/691 [==============================] - 0s 129us/step - loss: 0.5968 - acc: 0.6802\n",
      "Epoch 11/150\n",
      "691/691 [==============================] - 0s 129us/step - loss: 0.5888 - acc: 0.7077\n",
      "Epoch 12/150\n",
      "691/691 [==============================] - 0s 127us/step - loss: 0.5958 - acc: 0.6903\n",
      "Epoch 13/150\n",
      "691/691 [==============================] - 0s 133us/step - loss: 0.5868 - acc: 0.6889\n",
      "Epoch 14/150\n",
      "691/691 [==============================] - 0s 127us/step - loss: 0.5843 - acc: 0.7106\n",
      "Epoch 15/150\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.5863 - acc: 0.7019\n",
      "Epoch 16/150\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.5826 - acc: 0.7004\n",
      "Epoch 17/150\n",
      "691/691 [==============================] - 0s 168us/step - loss: 0.5754 - acc: 0.7106\n",
      "Epoch 18/150\n",
      "691/691 [==============================] - 0s 148us/step - loss: 0.5773 - acc: 0.6946\n",
      "Epoch 19/150\n",
      "691/691 [==============================] - 0s 127us/step - loss: 0.5701 - acc: 0.7106\n",
      "Epoch 20/150\n",
      "691/691 [==============================] - 0s 126us/step - loss: 0.5702 - acc: 0.7004\n",
      "Epoch 21/150\n",
      "691/691 [==============================] - 0s 126us/step - loss: 0.5699 - acc: 0.7279\n",
      "Epoch 22/150\n",
      "691/691 [==============================] - 0s 127us/step - loss: 0.5656 - acc: 0.7062\n",
      "Epoch 23/150\n",
      "691/691 [==============================] - 0s 133us/step - loss: 0.5813 - acc: 0.7048\n",
      "Epoch 24/150\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.5611 - acc: 0.7120\n",
      "Epoch 25/150\n",
      "691/691 [==============================] - 0s 130us/step - loss: 0.5610 - acc: 0.7149\n",
      "Epoch 26/150\n",
      "691/691 [==============================] - 0s 130us/step - loss: 0.5627 - acc: 0.7236\n",
      "Epoch 27/150\n",
      "691/691 [==============================] - 0s 129us/step - loss: 0.5585 - acc: 0.7294\n",
      "Epoch 28/150\n",
      "691/691 [==============================] - ETA: 0s - loss: 0.5523 - acc: 0.733 - 0s 129us/step - loss: 0.5565 - acc: 0.7207\n",
      "Epoch 29/150\n",
      "691/691 [==============================] - 0s 133us/step - loss: 0.5539 - acc: 0.7106\n",
      "Epoch 30/150\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.5521 - acc: 0.7207\n",
      "Epoch 31/150\n",
      "691/691 [==============================] - 0s 127us/step - loss: 0.5482 - acc: 0.7294\n",
      "Epoch 32/150\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.5493 - acc: 0.7221\n",
      "Epoch 33/150\n",
      "691/691 [==============================] - 0s 130us/step - loss: 0.5492 - acc: 0.7236\n",
      "Epoch 34/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.5474 - acc: 0.7207\n",
      "Epoch 35/150\n",
      "691/691 [==============================] - 0s 133us/step - loss: 0.5435 - acc: 0.7279\n",
      "Epoch 36/150\n",
      "691/691 [==============================] - 0s 123us/step - loss: 0.5480 - acc: 0.7294\n",
      "Epoch 37/150\n",
      "691/691 [==============================] - 0s 129us/step - loss: 0.5395 - acc: 0.7250\n",
      "Epoch 38/150\n",
      "691/691 [==============================] - 0s 129us/step - loss: 0.5481 - acc: 0.7164\n",
      "Epoch 39/150\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.5400 - acc: 0.7337\n",
      "Epoch 40/150\n",
      "691/691 [==============================] - 0s 129us/step - loss: 0.5366 - acc: 0.7366\n",
      "Epoch 41/150\n",
      "691/691 [==============================] - 0s 133us/step - loss: 0.5366 - acc: 0.7352\n",
      "Epoch 42/150\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.5379 - acc: 0.7279\n",
      "Epoch 43/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.5401 - acc: 0.7395\n",
      "Epoch 44/150\n",
      "691/691 [==============================] - 0s 129us/step - loss: 0.5272 - acc: 0.7352\n",
      "Epoch 45/150\n",
      "691/691 [==============================] - 0s 133us/step - loss: 0.5392 - acc: 0.7164\n",
      "Epoch 46/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.5401 - acc: 0.7294\n",
      "Epoch 47/150\n",
      "691/691 [==============================] - 0s 125us/step - loss: 0.5288 - acc: 0.7294\n",
      "Epoch 48/150\n",
      "691/691 [==============================] - 0s 129us/step - loss: 0.5327 - acc: 0.7250\n",
      "Epoch 49/150\n",
      "691/691 [==============================] - 0s 130us/step - loss: 0.5283 - acc: 0.7308\n",
      "Epoch 50/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.5297 - acc: 0.7323\n",
      "Epoch 51/150\n",
      "691/691 [==============================] - 0s 130us/step - loss: 0.5221 - acc: 0.7395\n",
      "Epoch 52/150\n",
      "691/691 [==============================] - 0s 127us/step - loss: 0.5283 - acc: 0.7337\n",
      "Epoch 53/150\n",
      "691/691 [==============================] - 0s 127us/step - loss: 0.5227 - acc: 0.7410\n",
      "Epoch 54/150\n",
      "691/691 [==============================] - 0s 130us/step - loss: 0.5216 - acc: 0.7395\n",
      "Epoch 55/150\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.5242 - acc: 0.7308\n",
      "Epoch 56/150\n",
      "691/691 [==============================] - 0s 129us/step - loss: 0.5244 - acc: 0.7438\n",
      "Epoch 57/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.5208 - acc: 0.7438\n",
      "Epoch 58/150\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.5261 - acc: 0.7482\n",
      "Epoch 59/150\n",
      "691/691 [==============================] - 0s 133us/step - loss: 0.5279 - acc: 0.7410\n",
      "Epoch 60/150\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.5194 - acc: 0.7438\n",
      "Epoch 61/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.5309 - acc: 0.7250\n",
      "Epoch 62/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.5211 - acc: 0.7540\n",
      "Epoch 63/150\n",
      "691/691 [==============================] - 0s 148us/step - loss: 0.5187 - acc: 0.7410\n",
      "Epoch 64/150\n",
      "691/691 [==============================] - 0s 172us/step - loss: 0.5213 - acc: 0.7511\n",
      "Epoch 65/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.5148 - acc: 0.7540\n",
      "Epoch 66/150\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.5123 - acc: 0.7496\n",
      "Epoch 67/150\n",
      "691/691 [==============================] - 0s 130us/step - loss: 0.5141 - acc: 0.7598\n",
      "Epoch 68/150\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.5222 - acc: 0.7641\n",
      "Epoch 69/150\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.5121 - acc: 0.7554\n",
      "Epoch 70/150\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.5048 - acc: 0.7496\n",
      "Epoch 71/150\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.5087 - acc: 0.7482\n",
      "Epoch 72/150\n",
      "691/691 [==============================] - 0s 133us/step - loss: 0.5027 - acc: 0.7540\n",
      "Epoch 73/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.5084 - acc: 0.7511\n",
      "Epoch 74/150\n",
      "691/691 [==============================] - 0s 133us/step - loss: 0.5081 - acc: 0.7583\n",
      "Epoch 75/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.5095 - acc: 0.7424\n",
      "Epoch 76/150\n",
      "691/691 [==============================] - 0s 130us/step - loss: 0.5113 - acc: 0.7641\n",
      "Epoch 77/150\n",
      "691/691 [==============================] - 0s 129us/step - loss: 0.5047 - acc: 0.7540\n",
      "Epoch 78/150\n",
      "691/691 [==============================] - 0s 130us/step - loss: 0.5004 - acc: 0.7569\n",
      "Epoch 79/150\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.5029 - acc: 0.7540\n",
      "Epoch 80/150\n",
      "691/691 [==============================] - 0s 130us/step - loss: 0.5047 - acc: 0.7525\n",
      "Epoch 81/150\n",
      "691/691 [==============================] - 0s 133us/step - loss: 0.5092 - acc: 0.7540\n",
      "Epoch 82/150\n",
      "691/691 [==============================] - 0s 130us/step - loss: 0.4970 - acc: 0.7525\n",
      "Epoch 83/150\n",
      "691/691 [==============================] - 0s 130us/step - loss: 0.4954 - acc: 0.7569\n",
      "Epoch 84/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4998 - acc: 0.7569\n",
      "Epoch 85/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.4981 - acc: 0.7554\n",
      "Epoch 86/150\n",
      "691/691 [==============================] - 0s 130us/step - loss: 0.4946 - acc: 0.7612\n",
      "Epoch 87/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.4959 - acc: 0.7598\n",
      "Epoch 88/150\n",
      "691/691 [==============================] - 0s 126us/step - loss: 0.4972 - acc: 0.7713\n",
      "Epoch 89/150\n",
      "691/691 [==============================] - 0s 168us/step - loss: 0.4899 - acc: 0.7641\n",
      "Epoch 90/150\n",
      "691/691 [==============================] - 0s 130us/step - loss: 0.4959 - acc: 0.7728\n",
      "Epoch 91/150\n",
      "691/691 [==============================] - 0s 129us/step - loss: 0.4978 - acc: 0.7482\n",
      "Epoch 92/150\n",
      "691/691 [==============================] - 0s 133us/step - loss: 0.5085 - acc: 0.7525\n",
      "Epoch 93/150\n",
      "691/691 [==============================] - 0s 133us/step - loss: 0.4947 - acc: 0.7612\n",
      "Epoch 94/150\n",
      "691/691 [==============================] - 0s 126us/step - loss: 0.4971 - acc: 0.7598\n",
      "Epoch 95/150\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.4824 - acc: 0.7685\n",
      "Epoch 96/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.4901 - acc: 0.7627\n",
      "Epoch 97/150\n",
      "691/691 [==============================] - 0s 133us/step - loss: 0.4913 - acc: 0.7583\n",
      "Epoch 98/150\n",
      "691/691 [==============================] - 0s 133us/step - loss: 0.4776 - acc: 0.7757\n",
      "Epoch 99/150\n",
      "691/691 [==============================] - 0s 133us/step - loss: 0.4843 - acc: 0.7713\n",
      "Epoch 100/150\n",
      "691/691 [==============================] - 0s 127us/step - loss: 0.4816 - acc: 0.7742\n",
      "Epoch 101/150\n",
      "691/691 [==============================] - ETA: 0s - loss: 0.4710 - acc: 0.767 - 0s 129us/step - loss: 0.4834 - acc: 0.7598\n",
      "Epoch 102/150\n",
      "691/691 [==============================] - 0s 133us/step - loss: 0.4781 - acc: 0.7656\n",
      "Epoch 103/150\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.4923 - acc: 0.7540\n",
      "Epoch 104/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.4819 - acc: 0.7713\n",
      "Epoch 105/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.4817 - acc: 0.7728\n",
      "Epoch 106/150\n",
      "691/691 [==============================] - 0s 129us/step - loss: 0.4837 - acc: 0.7641\n",
      "Epoch 107/150\n",
      "691/691 [==============================] - 0s 174us/step - loss: 0.4772 - acc: 0.7699\n",
      "Epoch 108/150\n",
      "691/691 [==============================] - 0s 129us/step - loss: 0.4802 - acc: 0.7598\n",
      "Epoch 109/150\n",
      "691/691 [==============================] - 0s 177us/step - loss: 0.4768 - acc: 0.7757\n",
      "Epoch 110/150\n",
      "691/691 [==============================] - 0s 126us/step - loss: 0.4676 - acc: 0.7685\n",
      "Epoch 111/150\n",
      "691/691 [==============================] - 0s 126us/step - loss: 0.4791 - acc: 0.7699\n",
      "Epoch 112/150\n",
      "691/691 [==============================] - 0s 130us/step - loss: 0.4789 - acc: 0.7656\n",
      "Epoch 113/150\n",
      "691/691 [==============================] - 0s 126us/step - loss: 0.4779 - acc: 0.7670\n",
      "Epoch 114/150\n",
      "691/691 [==============================] - 0s 133us/step - loss: 0.4744 - acc: 0.7670\n",
      "Epoch 115/150\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.4801 - acc: 0.7482\n",
      "Epoch 116/150\n",
      "691/691 [==============================] - 0s 127us/step - loss: 0.4699 - acc: 0.7757\n",
      "Epoch 117/150\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.4717 - acc: 0.7670\n",
      "Epoch 118/150\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.4663 - acc: 0.7873\n",
      "Epoch 119/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.4726 - acc: 0.7873\n",
      "Epoch 120/150\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.4766 - acc: 0.7699\n",
      "Epoch 121/150\n",
      "691/691 [==============================] - 0s 130us/step - loss: 0.4717 - acc: 0.7713\n",
      "Epoch 122/150\n",
      "691/691 [==============================] - 0s 127us/step - loss: 0.4689 - acc: 0.7757\n",
      "Epoch 123/150\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.4689 - acc: 0.7829\n",
      "Epoch 124/150\n",
      "691/691 [==============================] - 0s 129us/step - loss: 0.4624 - acc: 0.7771\n",
      "Epoch 125/150\n",
      "691/691 [==============================] - 0s 130us/step - loss: 0.4635 - acc: 0.7771\n",
      "Epoch 126/150\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.4601 - acc: 0.7728\n",
      "Epoch 127/150\n",
      "691/691 [==============================] - 0s 129us/step - loss: 0.4664 - acc: 0.7742\n",
      "Epoch 128/150\n",
      "691/691 [==============================] - 0s 129us/step - loss: 0.4680 - acc: 0.7656\n",
      "Epoch 129/150\n",
      "691/691 [==============================] - 0s 127us/step - loss: 0.4647 - acc: 0.7844\n",
      "Epoch 130/150\n",
      "691/691 [==============================] - 0s 129us/step - loss: 0.4601 - acc: 0.7829\n",
      "Epoch 131/150\n",
      "691/691 [==============================] - 0s 133us/step - loss: 0.4614 - acc: 0.7771\n",
      "Epoch 132/150\n",
      "691/691 [==============================] - 0s 130us/step - loss: 0.4618 - acc: 0.7713\n",
      "Epoch 133/150\n",
      "691/691 [==============================] - 0s 130us/step - loss: 0.4649 - acc: 0.7641\n",
      "Epoch 134/150\n",
      "691/691 [==============================] - 0s 127us/step - loss: 0.4665 - acc: 0.7685\n",
      "Epoch 135/150\n",
      "691/691 [==============================] - 0s 127us/step - loss: 0.4603 - acc: 0.7742\n",
      "Epoch 136/150\n",
      "691/691 [==============================] - 0s 129us/step - loss: 0.4586 - acc: 0.7685\n",
      "Epoch 137/150\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.4544 - acc: 0.7800\n",
      "Epoch 138/150\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.4566 - acc: 0.7844\n",
      "Epoch 139/150\n",
      "691/691 [==============================] - 0s 127us/step - loss: 0.4540 - acc: 0.7800\n",
      "Epoch 140/150\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.4559 - acc: 0.7771\n",
      "Epoch 141/150\n",
      "691/691 [==============================] - 0s 127us/step - loss: 0.4612 - acc: 0.7786\n",
      "Epoch 142/150\n",
      "691/691 [==============================] - 0s 133us/step - loss: 0.4469 - acc: 0.7829\n",
      "Epoch 143/150\n",
      "691/691 [==============================] - 0s 130us/step - loss: 0.4632 - acc: 0.7656\n",
      "Epoch 144/150\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.4549 - acc: 0.7800\n",
      "Epoch 145/150\n",
      "691/691 [==============================] - 0s 127us/step - loss: 0.4580 - acc: 0.7771\n",
      "Epoch 146/150\n",
      "691/691 [==============================] - 0s 130us/step - loss: 0.4649 - acc: 0.7858\n",
      "Epoch 147/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.4496 - acc: 0.7742\n",
      "Epoch 148/150\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.4527 - acc: 0.7858\n",
      "Epoch 149/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.4534 - acc: 0.7844\n",
      "Epoch 150/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.4579 - acc: 0.7786\n",
      "77/77 [==============================] - 0s 3ms/step\n",
      "Epoch 1/150\n",
      "691/691 [==============================] - 1s 1ms/step - loss: 0.6828 - acc: 0.6339\n",
      "Epoch 2/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.6670 - acc: 0.6498\n",
      "Epoch 3/150\n",
      "691/691 [==============================] - 0s 190us/step - loss: 0.6571 - acc: 0.6498\n",
      "Epoch 4/150\n",
      "691/691 [==============================] - 0s 182us/step - loss: 0.6492 - acc: 0.6556\n",
      "Epoch 5/150\n",
      "691/691 [==============================] - 0s 161us/step - loss: 0.6367 - acc: 0.6585\n",
      "Epoch 6/150\n",
      "691/691 [==============================] - 0s 188us/step - loss: 0.6256 - acc: 0.6700\n",
      "Epoch 7/150\n",
      "691/691 [==============================] - 0s 180us/step - loss: 0.6140 - acc: 0.6773\n",
      "Epoch 8/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.6084 - acc: 0.6845\n",
      "Epoch 9/150\n",
      "691/691 [==============================] - 0s 169us/step - loss: 0.6039 - acc: 0.6715\n",
      "Epoch 10/150\n",
      "691/691 [==============================] - 0s 314us/step - loss: 0.6051 - acc: 0.6744\n",
      "Epoch 11/150\n",
      "691/691 [==============================] - 0s 290us/step - loss: 0.5914 - acc: 0.7062\n",
      "Epoch 12/150\n",
      "691/691 [==============================] - 0s 214us/step - loss: 0.5956 - acc: 0.6845\n",
      "Epoch 13/150\n",
      "691/691 [==============================] - 0s 268us/step - loss: 0.5856 - acc: 0.6990\n",
      "Epoch 14/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "691/691 [==============================] - 0s 262us/step - loss: 0.5846 - acc: 0.7048\n",
      "Epoch 15/150\n",
      "691/691 [==============================] - 0s 210us/step - loss: 0.5951 - acc: 0.6802\n",
      "Epoch 16/150\n",
      "691/691 [==============================] - 0s 204us/step - loss: 0.5823 - acc: 0.6946\n",
      "Epoch 17/150\n",
      "691/691 [==============================] - 0s 207us/step - loss: 0.5779 - acc: 0.7106\n",
      "Epoch 18/150\n",
      "691/691 [==============================] - 0s 196us/step - loss: 0.5804 - acc: 0.7033\n",
      "Epoch 19/150\n",
      "691/691 [==============================] - 0s 261us/step - loss: 0.5757 - acc: 0.7106\n",
      "Epoch 20/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.5776 - acc: 0.7004\n",
      "Epoch 21/150\n",
      "691/691 [==============================] - 0s 146us/step - loss: 0.5723 - acc: 0.6975\n",
      "Epoch 22/150\n",
      "691/691 [==============================] - 0s 165us/step - loss: 0.5752 - acc: 0.6990\n",
      "Epoch 23/150\n",
      "691/691 [==============================] - 0s 162us/step - loss: 0.5747 - acc: 0.6903\n",
      "Epoch 24/150\n",
      "691/691 [==============================] - 0s 146us/step - loss: 0.5700 - acc: 0.7091\n",
      "Epoch 25/150\n",
      "691/691 [==============================] - 0s 168us/step - loss: 0.5676 - acc: 0.7019\n",
      "Epoch 26/150\n",
      "691/691 [==============================] - 0s 164us/step - loss: 0.5614 - acc: 0.7135\n",
      "Epoch 27/150\n",
      "691/691 [==============================] - 0s 133us/step - loss: 0.5647 - acc: 0.7178\n",
      "Epoch 28/150\n",
      "691/691 [==============================] - 0s 159us/step - loss: 0.5655 - acc: 0.7120\n",
      "Epoch 29/150\n",
      "691/691 [==============================] - 0s 149us/step - loss: 0.5670 - acc: 0.7033\n",
      "Epoch 30/150\n",
      "691/691 [==============================] - 0s 177us/step - loss: 0.5682 - acc: 0.7048\n",
      "Epoch 31/150\n",
      "691/691 [==============================] - 0s 204us/step - loss: 0.5625 - acc: 0.7033\n",
      "Epoch 32/150\n",
      "691/691 [==============================] - 0s 185us/step - loss: 0.5593 - acc: 0.7106\n",
      "Epoch 33/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.5607 - acc: 0.7149\n",
      "Epoch 34/150\n",
      "691/691 [==============================] - 0s 169us/step - loss: 0.5540 - acc: 0.7149\n",
      "Epoch 35/150\n",
      "691/691 [==============================] - 0s 171us/step - loss: 0.5555 - acc: 0.7207\n",
      "Epoch 36/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.5587 - acc: 0.7120\n",
      "Epoch 37/150\n",
      "691/691 [==============================] - 0s 291us/step - loss: 0.5532 - acc: 0.7091\n",
      "Epoch 38/150\n",
      "691/691 [==============================] - 0s 297us/step - loss: 0.5478 - acc: 0.7178\n",
      "Epoch 39/150\n",
      "691/691 [==============================] - 0s 262us/step - loss: 0.5513 - acc: 0.7265\n",
      "Epoch 40/150\n",
      "691/691 [==============================] - 0s 220us/step - loss: 0.5475 - acc: 0.7178\n",
      "Epoch 41/150\n",
      "691/691 [==============================] - 0s 326us/step - loss: 0.5471 - acc: 0.7091\n",
      "Epoch 42/150\n",
      "691/691 [==============================] - 0s 220us/step - loss: 0.5498 - acc: 0.7164\n",
      "Epoch 43/150\n",
      "691/691 [==============================] - 0s 261us/step - loss: 0.5479 - acc: 0.7164\n",
      "Epoch 44/150\n",
      "691/691 [==============================] - 0s 246us/step - loss: 0.5436 - acc: 0.7366\n",
      "Epoch 45/150\n",
      "691/691 [==============================] - 0s 213us/step - loss: 0.5439 - acc: 0.7294\n",
      "Epoch 46/150\n",
      "691/691 [==============================] - 0s 248us/step - loss: 0.5517 - acc: 0.7120\n",
      "Epoch 47/150\n",
      "691/691 [==============================] - 0s 252us/step - loss: 0.5489 - acc: 0.7120\n",
      "Epoch 48/150\n",
      "691/691 [==============================] - 0s 265us/step - loss: 0.5446 - acc: 0.7323 0s - loss: 0.5510 - acc: 0.728\n",
      "Epoch 49/150\n",
      "691/691 [==============================] - 0s 219us/step - loss: 0.5444 - acc: 0.7178\n",
      "Epoch 50/150\n",
      "691/691 [==============================] - 0s 209us/step - loss: 0.5427 - acc: 0.7366\n",
      "Epoch 51/150\n",
      "691/691 [==============================] - 0s 211us/step - loss: 0.5322 - acc: 0.7424\n",
      "Epoch 52/150\n",
      "691/691 [==============================] - 0s 255us/step - loss: 0.5375 - acc: 0.7192\n",
      "Epoch 53/150\n",
      "691/691 [==============================] - 0s 217us/step - loss: 0.5368 - acc: 0.7294 0s - loss: 0.5579 - acc: 0.71\n",
      "Epoch 54/150\n",
      "691/691 [==============================] - 0s 188us/step - loss: 0.5454 - acc: 0.7337\n",
      "Epoch 55/150\n",
      "691/691 [==============================] - 0s 201us/step - loss: 0.5430 - acc: 0.7424\n",
      "Epoch 56/150\n",
      "691/691 [==============================] - 0s 214us/step - loss: 0.5299 - acc: 0.7308\n",
      "Epoch 57/150\n",
      "691/691 [==============================] - 0s 249us/step - loss: 0.5323 - acc: 0.7438\n",
      "Epoch 58/150\n",
      "691/691 [==============================] - 0s 246us/step - loss: 0.5255 - acc: 0.7496\n",
      "Epoch 59/150\n",
      "691/691 [==============================] - 0s 204us/step - loss: 0.5375 - acc: 0.7381 0s - loss: 0.5307 - acc: 0.74\n",
      "Epoch 60/150\n",
      "691/691 [==============================] - 0s 233us/step - loss: 0.5337 - acc: 0.7221\n",
      "Epoch 61/150\n",
      "691/691 [==============================] - 0s 213us/step - loss: 0.5290 - acc: 0.7467\n",
      "Epoch 62/150\n",
      "691/691 [==============================] - 0s 198us/step - loss: 0.5237 - acc: 0.7410\n",
      "Epoch 63/150\n",
      "691/691 [==============================] - 0s 253us/step - loss: 0.5360 - acc: 0.7352\n",
      "Epoch 64/150\n",
      "691/691 [==============================] - 0s 282us/step - loss: 0.5275 - acc: 0.7569 0s - loss: 0.5476 - acc: 0.7\n",
      "Epoch 65/150\n",
      "691/691 [==============================] - 0s 216us/step - loss: 0.5258 - acc: 0.7438\n",
      "Epoch 66/150\n",
      "691/691 [==============================] - 0s 216us/step - loss: 0.5213 - acc: 0.7410\n",
      "Epoch 67/150\n",
      "691/691 [==============================] - 0s 214us/step - loss: 0.5226 - acc: 0.7525\n",
      "Epoch 68/150\n",
      "691/691 [==============================] - 0s 209us/step - loss: 0.5233 - acc: 0.7467\n",
      "Epoch 69/150\n",
      "691/691 [==============================] - 0s 201us/step - loss: 0.5190 - acc: 0.7467\n",
      "Epoch 70/150\n",
      "691/691 [==============================] - 0s 203us/step - loss: 0.5101 - acc: 0.7511\n",
      "Epoch 71/150\n",
      "691/691 [==============================] - 0s 201us/step - loss: 0.5144 - acc: 0.7540\n",
      "Epoch 72/150\n",
      "691/691 [==============================] - 0s 201us/step - loss: 0.5161 - acc: 0.7410\n",
      "Epoch 73/150\n",
      "691/691 [==============================] - 0s 297us/step - loss: 0.5117 - acc: 0.7612\n",
      "Epoch 74/150\n",
      "691/691 [==============================] - 0s 378us/step - loss: 0.5150 - acc: 0.7496\n",
      "Epoch 75/150\n",
      "691/691 [==============================] - 0s 268us/step - loss: 0.5146 - acc: 0.7583\n",
      "Epoch 76/150\n",
      "691/691 [==============================] - 0s 274us/step - loss: 0.5388 - acc: 0.7410\n",
      "Epoch 77/150\n",
      "691/691 [==============================] - 0s 259us/step - loss: 0.5156 - acc: 0.7525\n",
      "Epoch 78/150\n",
      "691/691 [==============================] - 0s 290us/step - loss: 0.5140 - acc: 0.7554\n",
      "Epoch 79/150\n",
      "691/691 [==============================] - 0s 259us/step - loss: 0.5100 - acc: 0.7554\n",
      "Epoch 80/150\n",
      "691/691 [==============================] - 0s 223us/step - loss: 0.5164 - acc: 0.7496\n",
      "Epoch 81/150\n",
      "691/691 [==============================] - 0s 294us/step - loss: 0.5098 - acc: 0.7482\n",
      "Epoch 82/150\n",
      "691/691 [==============================] - 0s 252us/step - loss: 0.5099 - acc: 0.7424\n",
      "Epoch 83/150\n",
      "691/691 [==============================] - 0s 194us/step - loss: 0.5104 - acc: 0.7569\n",
      "Epoch 84/150\n",
      "691/691 [==============================] - 0s 353us/step - loss: 0.5073 - acc: 0.7670\n",
      "Epoch 85/150\n",
      "691/691 [==============================] - 0s 291us/step - loss: 0.5033 - acc: 0.7685\n",
      "Epoch 86/150\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.5080 - acc: 0.7511\n",
      "Epoch 87/150\n",
      "691/691 [==============================] - 0s 232us/step - loss: 0.5068 - acc: 0.7569\n",
      "Epoch 88/150\n",
      "691/691 [==============================] - 0s 356us/step - loss: 0.5004 - acc: 0.7554 0s - loss: 0.4989 - acc: 0.\n",
      "Epoch 89/150\n",
      "691/691 [==============================] - 0s 281us/step - loss: 0.5219 - acc: 0.7496\n",
      "Epoch 90/150\n",
      "691/691 [==============================] - 0s 293us/step - loss: 0.4992 - acc: 0.7641\n",
      "Epoch 91/150\n",
      "691/691 [==============================] - 0s 297us/step - loss: 0.5000 - acc: 0.7641\n",
      "Epoch 92/150\n",
      "691/691 [==============================] - 0s 291us/step - loss: 0.4984 - acc: 0.7554\n",
      "Epoch 93/150\n",
      "691/691 [==============================] - 0s 285us/step - loss: 0.4998 - acc: 0.7699\n",
      "Epoch 94/150\n",
      "691/691 [==============================] - 0s 284us/step - loss: 0.4943 - acc: 0.7612\n",
      "Epoch 95/150\n",
      "691/691 [==============================] - 0s 256us/step - loss: 0.4976 - acc: 0.7569\n",
      "Epoch 96/150\n",
      "691/691 [==============================] - 0s 272us/step - loss: 0.4953 - acc: 0.7612 0s - loss: 0.4872 - acc: 0.7\n",
      "Epoch 97/150\n",
      "691/691 [==============================] - 0s 261us/step - loss: 0.5049 - acc: 0.7627\n",
      "Epoch 98/150\n",
      "691/691 [==============================] - 0s 214us/step - loss: 0.4924 - acc: 0.7612\n",
      "Epoch 99/150\n",
      "691/691 [==============================] - 0s 287us/step - loss: 0.4865 - acc: 0.7742\n",
      "Epoch 100/150\n",
      "691/691 [==============================] - 0s 261us/step - loss: 0.4914 - acc: 0.7757\n",
      "Epoch 101/150\n",
      "691/691 [==============================] - 0s 304us/step - loss: 0.4915 - acc: 0.7699\n",
      "Epoch 102/150\n",
      "691/691 [==============================] - 0s 272us/step - loss: 0.5009 - acc: 0.7525\n",
      "Epoch 103/150\n",
      "691/691 [==============================] - 0s 261us/step - loss: 0.4880 - acc: 0.7670\n",
      "Epoch 104/150\n",
      "691/691 [==============================] - ETA: 0s - loss: 0.4887 - acc: 0.7524- ETA: 0s - loss: 0.5343 - acc: 0.7 - 0s 259us/step - loss: 0.4882 - acc: 0.7540\n",
      "Epoch 105/150\n",
      "691/691 [==============================] - 0s 256us/step - loss: 0.4935 - acc: 0.7800\n",
      "Epoch 106/150\n",
      "691/691 [==============================] - 0s 216us/step - loss: 0.4916 - acc: 0.7699\n",
      "Epoch 107/150\n",
      "691/691 [==============================] - 0s 285us/step - loss: 0.4852 - acc: 0.7627\n",
      "Epoch 108/150\n",
      "691/691 [==============================] - 0s 242us/step - loss: 0.4901 - acc: 0.7699\n",
      "Epoch 109/150\n",
      "691/691 [==============================] - 0s 214us/step - loss: 0.4958 - acc: 0.7771\n",
      "Epoch 110/150\n",
      "691/691 [==============================] - 0s 222us/step - loss: 0.4927 - acc: 0.7656\n",
      "Epoch 111/150\n",
      "691/691 [==============================] - 0s 171us/step - loss: 0.4815 - acc: 0.7699\n",
      "Epoch 112/150\n",
      "691/691 [==============================] - 0s 203us/step - loss: 0.4889 - acc: 0.7685\n",
      "Epoch 113/150\n",
      "691/691 [==============================] - 0s 194us/step - loss: 0.4864 - acc: 0.7771\n",
      "Epoch 114/150\n",
      "691/691 [==============================] - 0s 156us/step - loss: 0.4941 - acc: 0.7641\n",
      "Epoch 115/150\n",
      "691/691 [==============================] - 0s 171us/step - loss: 0.4861 - acc: 0.7656\n",
      "Epoch 116/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4859 - acc: 0.7713\n",
      "Epoch 117/150\n",
      "691/691 [==============================] - 0s 174us/step - loss: 0.4806 - acc: 0.7641\n",
      "Epoch 118/150\n",
      "691/691 [==============================] - 0s 129us/step - loss: 0.4874 - acc: 0.7656\n",
      "Epoch 119/150\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.4794 - acc: 0.7728\n",
      "Epoch 120/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.4809 - acc: 0.7771\n",
      "Epoch 121/150\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.4862 - acc: 0.7757\n",
      "Epoch 122/150\n",
      "691/691 [==============================] - 0s 171us/step - loss: 0.4827 - acc: 0.7713\n",
      "Epoch 123/150\n",
      "691/691 [==============================] - 0s 188us/step - loss: 0.4814 - acc: 0.7641\n",
      "Epoch 124/150\n",
      "691/691 [==============================] - 0s 158us/step - loss: 0.4754 - acc: 0.7713\n",
      "Epoch 125/150\n",
      "691/691 [==============================] - 0s 203us/step - loss: 0.4753 - acc: 0.7713\n",
      "Epoch 126/150\n",
      "691/691 [==============================] - 0s 190us/step - loss: 0.4771 - acc: 0.7786\n",
      "Epoch 127/150\n",
      "691/691 [==============================] - 0s 174us/step - loss: 0.4741 - acc: 0.7742\n",
      "Epoch 128/150\n",
      "691/691 [==============================] - 0s 175us/step - loss: 0.4764 - acc: 0.7757\n",
      "Epoch 129/150\n",
      "691/691 [==============================] - 0s 146us/step - loss: 0.5000 - acc: 0.7395\n",
      "Epoch 130/150\n",
      "691/691 [==============================] - 0s 149us/step - loss: 0.4736 - acc: 0.7786\n",
      "Epoch 131/150\n",
      "691/691 [==============================] - 0s 181us/step - loss: 0.4697 - acc: 0.7844\n",
      "Epoch 132/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.4839 - acc: 0.7641\n",
      "Epoch 133/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.4725 - acc: 0.7685\n",
      "Epoch 134/150\n",
      "691/691 [==============================] - 0s 169us/step - loss: 0.4698 - acc: 0.7815\n",
      "Epoch 135/150\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.4686 - acc: 0.7844\n",
      "Epoch 136/150\n",
      "691/691 [==============================] - 0s 130us/step - loss: 0.4691 - acc: 0.7945\n",
      "Epoch 137/150\n",
      "691/691 [==============================] - 0s 133us/step - loss: 0.4765 - acc: 0.7757\n",
      "Epoch 138/150\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.4650 - acc: 0.7786\n",
      "Epoch 139/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.4645 - acc: 0.7757\n",
      "Epoch 140/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.4736 - acc: 0.7670\n",
      "Epoch 141/150\n",
      "691/691 [==============================] - 0s 188us/step - loss: 0.4686 - acc: 0.7670\n",
      "Epoch 142/150\n",
      "691/691 [==============================] - 0s 198us/step - loss: 0.4693 - acc: 0.7670\n",
      "Epoch 143/150\n",
      "691/691 [==============================] - 0s 172us/step - loss: 0.4703 - acc: 0.7800\n",
      "Epoch 144/150\n",
      "691/691 [==============================] - 0s 169us/step - loss: 0.4691 - acc: 0.7858\n",
      "Epoch 145/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.4722 - acc: 0.7656\n",
      "Epoch 146/150\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.4653 - acc: 0.7786\n",
      "Epoch 147/150\n",
      "691/691 [==============================] - 0s 146us/step - loss: 0.4731 - acc: 0.7742\n",
      "Epoch 148/150\n",
      "691/691 [==============================] - 0s 146us/step - loss: 0.4720 - acc: 0.7844\n",
      "Epoch 149/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.4664 - acc: 0.7916\n",
      "Epoch 150/150\n",
      "691/691 [==============================] - 0s 172us/step - loss: 0.4600 - acc: 0.7829\n",
      "77/77 [==============================] - 0s 3ms/step\n",
      "Epoch 1/150\n",
      "691/691 [==============================] - 1s 1ms/step - loss: 0.6793 - acc: 0.6266\n",
      "Epoch 2/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.6588 - acc: 0.6512\n",
      "Epoch 3/150\n",
      "691/691 [==============================] - 0s 133us/step - loss: 0.6522 - acc: 0.6556\n",
      "Epoch 4/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.6416 - acc: 0.6512\n",
      "Epoch 5/150\n",
      "691/691 [==============================] - 0s 151us/step - loss: 0.6337 - acc: 0.6700\n",
      "Epoch 6/150\n",
      "691/691 [==============================] - 0s 165us/step - loss: 0.6270 - acc: 0.6715\n",
      "Epoch 7/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.6174 - acc: 0.6845\n",
      "Epoch 8/150\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.6063 - acc: 0.6758\n",
      "Epoch 9/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.5986 - acc: 0.6903\n",
      "Epoch 10/150\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.5983 - acc: 0.6845\n",
      "Epoch 11/150\n",
      "691/691 [==============================] - 0s 209us/step - loss: 0.5954 - acc: 0.6816\n",
      "Epoch 12/150\n",
      "691/691 [==============================] - 0s 197us/step - loss: 0.5859 - acc: 0.7048\n",
      "Epoch 13/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.5858 - acc: 0.7106\n",
      "Epoch 14/150\n",
      "691/691 [==============================] - 0s 133us/step - loss: 0.5785 - acc: 0.7120\n",
      "Epoch 15/150\n",
      "691/691 [==============================] - 0s 127us/step - loss: 0.5736 - acc: 0.7135\n",
      "Epoch 16/150\n",
      "691/691 [==============================] - 0s 133us/step - loss: 0.5763 - acc: 0.7091\n",
      "Epoch 17/150\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.5720 - acc: 0.7192\n",
      "Epoch 18/150\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.5683 - acc: 0.7207\n",
      "Epoch 19/150\n",
      "691/691 [==============================] - 0s 133us/step - loss: 0.5680 - acc: 0.7250\n",
      "Epoch 20/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.5676 - acc: 0.7207\n",
      "Epoch 21/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.5665 - acc: 0.7265\n",
      "Epoch 22/150\n",
      "691/691 [==============================] - 0s 133us/step - loss: 0.5601 - acc: 0.7207\n",
      "Epoch 23/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.5631 - acc: 0.7294\n",
      "Epoch 24/150\n",
      "691/691 [==============================] - 0s 133us/step - loss: 0.5630 - acc: 0.7265\n",
      "Epoch 25/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "691/691 [==============================] - 0s 138us/step - loss: 0.5678 - acc: 0.7265\n",
      "Epoch 26/150\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.5523 - acc: 0.7424\n",
      "Epoch 27/150\n",
      "691/691 [==============================] - 0s 127us/step - loss: 0.5532 - acc: 0.7279\n",
      "Epoch 28/150\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.5468 - acc: 0.7352\n",
      "Epoch 29/150\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.5459 - acc: 0.7308\n",
      "Epoch 30/150\n",
      "691/691 [==============================] - 0s 130us/step - loss: 0.5405 - acc: 0.7424\n",
      "Epoch 31/150\n",
      "691/691 [==============================] - 0s 130us/step - loss: 0.5409 - acc: 0.7366\n",
      "Epoch 32/150\n",
      "691/691 [==============================] - 0s 127us/step - loss: 0.5372 - acc: 0.7438\n",
      "Epoch 33/150\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.5402 - acc: 0.7236\n",
      "Epoch 34/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.5357 - acc: 0.7381\n",
      "Epoch 35/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.5479 - acc: 0.7294\n",
      "Epoch 36/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.5416 - acc: 0.7424\n",
      "Epoch 37/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.5452 - acc: 0.7438\n",
      "Epoch 38/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.5399 - acc: 0.7410\n",
      "Epoch 39/150\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.5260 - acc: 0.7627\n",
      "Epoch 40/150\n",
      "691/691 [==============================] - 0s 133us/step - loss: 0.5345 - acc: 0.7482\n",
      "Epoch 41/150\n",
      "691/691 [==============================] - 0s 127us/step - loss: 0.5327 - acc: 0.7410\n",
      "Epoch 42/150\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.5325 - acc: 0.7323\n",
      "Epoch 43/150\n",
      "691/691 [==============================] - 0s 133us/step - loss: 0.5252 - acc: 0.7424\n",
      "Epoch 44/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.5287 - acc: 0.7453\n",
      "Epoch 45/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.5316 - acc: 0.7467\n",
      "Epoch 46/150\n",
      "691/691 [==============================] - 0s 129us/step - loss: 0.5251 - acc: 0.7395\n",
      "Epoch 47/150\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.5314 - acc: 0.7381\n",
      "Epoch 48/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.5238 - acc: 0.7424\n",
      "Epoch 49/150\n",
      "691/691 [==============================] - 0s 127us/step - loss: 0.5208 - acc: 0.7453\n",
      "Epoch 50/150\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.5284 - acc: 0.7525\n",
      "Epoch 51/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.5202 - acc: 0.7453\n",
      "Epoch 52/150\n",
      "691/691 [==============================] - 0s 175us/step - loss: 0.5182 - acc: 0.7467\n",
      "Epoch 53/150\n",
      "691/691 [==============================] - 0s 133us/step - loss: 0.5173 - acc: 0.7540\n",
      "Epoch 54/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.5182 - acc: 0.7482\n",
      "Epoch 55/150\n",
      "691/691 [==============================] - 0s 129us/step - loss: 0.5246 - acc: 0.7438\n",
      "Epoch 56/150\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.5258 - acc: 0.7424\n",
      "Epoch 57/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.5181 - acc: 0.7540\n",
      "Epoch 58/150\n",
      "691/691 [==============================] - 0s 129us/step - loss: 0.5161 - acc: 0.7496\n",
      "Epoch 59/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.5189 - acc: 0.7467\n",
      "Epoch 60/150\n",
      "691/691 [==============================] - 0s 148us/step - loss: 0.5179 - acc: 0.7424\n",
      "Epoch 61/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.5154 - acc: 0.7496\n",
      "Epoch 62/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.5134 - acc: 0.7424\n",
      "Epoch 63/150\n",
      "691/691 [==============================] - 0s 184us/step - loss: 0.5102 - acc: 0.7656\n",
      "Epoch 64/150\n",
      "691/691 [==============================] - 0s 177us/step - loss: 0.5101 - acc: 0.7482 0s - loss: 0.5077 - acc: 0.752\n",
      "Epoch 65/150\n",
      "691/691 [==============================] - 0s 180us/step - loss: 0.5165 - acc: 0.7467\n",
      "Epoch 66/150\n",
      "691/691 [==============================] - 0s 171us/step - loss: 0.5081 - acc: 0.7612\n",
      "Epoch 67/150\n",
      "691/691 [==============================] - 0s 181us/step - loss: 0.5093 - acc: 0.7612 0s - loss: 0.5095 - acc: 0.759\n",
      "Epoch 68/150\n",
      "691/691 [==============================] - 0s 171us/step - loss: 0.5084 - acc: 0.7728\n",
      "Epoch 69/150\n",
      "691/691 [==============================] - 0s 180us/step - loss: 0.5088 - acc: 0.7583\n",
      "Epoch 70/150\n",
      "691/691 [==============================] - 0s 174us/step - loss: 0.5205 - acc: 0.7511\n",
      "Epoch 71/150\n",
      "691/691 [==============================] - 0s 133us/step - loss: 0.5109 - acc: 0.7612\n",
      "Epoch 72/150\n",
      "691/691 [==============================] - 0s 133us/step - loss: 0.5045 - acc: 0.7598\n",
      "Epoch 73/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.5023 - acc: 0.7627\n",
      "Epoch 74/150\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.5017 - acc: 0.7583\n",
      "Epoch 75/150\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.4976 - acc: 0.7685\n",
      "Epoch 76/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.5061 - acc: 0.7627\n",
      "Epoch 77/150\n",
      "691/691 [==============================] - 0s 181us/step - loss: 0.4960 - acc: 0.7496\n",
      "Epoch 78/150\n",
      "691/691 [==============================] - 0s 196us/step - loss: 0.5298 - acc: 0.7323\n",
      "Epoch 79/150\n",
      "691/691 [==============================] - 0s 149us/step - loss: 0.4980 - acc: 0.7612\n",
      "Epoch 80/150\n",
      "691/691 [==============================] - 0s 193us/step - loss: 0.5010 - acc: 0.7554\n",
      "Epoch 81/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.4967 - acc: 0.7641\n",
      "Epoch 82/150\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.4988 - acc: 0.7670\n",
      "Epoch 83/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.5044 - acc: 0.7670\n",
      "Epoch 84/150\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.4981 - acc: 0.7525\n",
      "Epoch 85/150\n",
      "691/691 [==============================] - 0s 133us/step - loss: 0.4971 - acc: 0.7670\n",
      "Epoch 86/150\n",
      "691/691 [==============================] - 0s 130us/step - loss: 0.4935 - acc: 0.7728\n",
      "Epoch 87/150\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.5007 - acc: 0.7525\n",
      "Epoch 88/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.5080 - acc: 0.7670\n",
      "Epoch 89/150\n",
      "691/691 [==============================] - 0s 130us/step - loss: 0.4934 - acc: 0.7757\n",
      "Epoch 90/150\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.4928 - acc: 0.7728\n",
      "Epoch 91/150\n",
      "691/691 [==============================] - 0s 130us/step - loss: 0.4907 - acc: 0.7612\n",
      "Epoch 92/150\n",
      "691/691 [==============================] - 0s 130us/step - loss: 0.4912 - acc: 0.7670\n",
      "Epoch 93/150\n",
      "691/691 [==============================] - 0s 133us/step - loss: 0.4893 - acc: 0.7742\n",
      "Epoch 94/150\n",
      "691/691 [==============================] - 0s 133us/step - loss: 0.4900 - acc: 0.7742\n",
      "Epoch 95/150\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.5029 - acc: 0.7482\n",
      "Epoch 96/150\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.4930 - acc: 0.7583\n",
      "Epoch 97/150\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.4908 - acc: 0.7612\n",
      "Epoch 98/150\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.4951 - acc: 0.7569\n",
      "Epoch 99/150\n",
      "691/691 [==============================] - 0s 133us/step - loss: 0.4912 - acc: 0.7670\n",
      "Epoch 100/150\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.4843 - acc: 0.7844\n",
      "Epoch 101/150\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.4802 - acc: 0.7771\n",
      "Epoch 102/150\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.4931 - acc: 0.7656\n",
      "Epoch 103/150\n",
      "691/691 [==============================] - 0s 133us/step - loss: 0.4807 - acc: 0.7685\n",
      "Epoch 104/150\n",
      "691/691 [==============================] - 0s 146us/step - loss: 0.4851 - acc: 0.7670\n",
      "Epoch 105/150\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.4787 - acc: 0.7685\n",
      "Epoch 106/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.4805 - acc: 0.7728\n",
      "Epoch 107/150\n",
      "691/691 [==============================] - 0s 133us/step - loss: 0.4761 - acc: 0.7829\n",
      "Epoch 108/150\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.4850 - acc: 0.7685\n",
      "Epoch 109/150\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.4753 - acc: 0.7713\n",
      "Epoch 110/150\n",
      "691/691 [==============================] - 0s 130us/step - loss: 0.4777 - acc: 0.7670\n",
      "Epoch 111/150\n",
      "691/691 [==============================] - 0s 133us/step - loss: 0.4831 - acc: 0.7757\n",
      "Epoch 112/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.4782 - acc: 0.7598\n",
      "Epoch 113/150\n",
      "691/691 [==============================] - 0s 133us/step - loss: 0.4750 - acc: 0.7641\n",
      "Epoch 114/150\n",
      "691/691 [==============================] - 0s 133us/step - loss: 0.4695 - acc: 0.7974\n",
      "Epoch 115/150\n",
      "691/691 [==============================] - 0s 129us/step - loss: 0.4822 - acc: 0.7873\n",
      "Epoch 116/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.4728 - acc: 0.7786\n",
      "Epoch 117/150\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.4850 - acc: 0.7598\n",
      "Epoch 118/150\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.4694 - acc: 0.7815\n",
      "Epoch 119/150\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.4741 - acc: 0.7728\n",
      "Epoch 120/150\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.4756 - acc: 0.7757\n",
      "Epoch 121/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.4689 - acc: 0.7728\n",
      "Epoch 122/150\n",
      "691/691 [==============================] - 0s 130us/step - loss: 0.4651 - acc: 0.7786\n",
      "Epoch 123/150\n",
      "691/691 [==============================] - 0s 133us/step - loss: 0.4609 - acc: 0.7844\n",
      "Epoch 124/150\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.4712 - acc: 0.7670\n",
      "Epoch 125/150\n",
      "691/691 [==============================] - 0s 133us/step - loss: 0.4659 - acc: 0.7771\n",
      "Epoch 126/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.4649 - acc: 0.7771\n",
      "Epoch 127/150\n",
      "691/691 [==============================] - 0s 130us/step - loss: 0.4616 - acc: 0.7815\n",
      "Epoch 128/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.4645 - acc: 0.7815\n",
      "Epoch 129/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4583 - acc: 0.7757\n",
      "Epoch 130/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.4600 - acc: 0.7742\n",
      "Epoch 131/150\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.4697 - acc: 0.7800\n",
      "Epoch 132/150\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.4570 - acc: 0.7786\n",
      "Epoch 133/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4572 - acc: 0.7786\n",
      "Epoch 134/150\n",
      "691/691 [==============================] - 0s 129us/step - loss: 0.4748 - acc: 0.7598\n",
      "Epoch 135/150\n",
      "691/691 [==============================] - 0s 146us/step - loss: 0.4603 - acc: 0.7858\n",
      "Epoch 136/150\n",
      "691/691 [==============================] - 0s 191us/step - loss: 0.4692 - acc: 0.7786\n",
      "Epoch 137/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.4563 - acc: 0.7945\n",
      "Epoch 138/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.4590 - acc: 0.7742\n",
      "Epoch 139/150\n",
      "691/691 [==============================] - 0s 168us/step - loss: 0.4555 - acc: 0.7656\n",
      "Epoch 140/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.4515 - acc: 0.7742\n",
      "Epoch 141/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.4750 - acc: 0.7800\n",
      "Epoch 142/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4600 - acc: 0.7713\n",
      "Epoch 143/150\n",
      "691/691 [==============================] - 0s 133us/step - loss: 0.4542 - acc: 0.7844\n",
      "Epoch 144/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.4497 - acc: 0.7815\n",
      "Epoch 145/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.4490 - acc: 0.7757\n",
      "Epoch 146/150\n",
      "691/691 [==============================] - 0s 171us/step - loss: 0.4502 - acc: 0.7771\n",
      "Epoch 147/150\n",
      "691/691 [==============================] - 0s 217us/step - loss: 0.4543 - acc: 0.7829\n",
      "Epoch 148/150\n",
      "691/691 [==============================] - 0s 175us/step - loss: 0.4496 - acc: 0.7771\n",
      "Epoch 149/150\n",
      "691/691 [==============================] - 0s 226us/step - loss: 0.4483 - acc: 0.7887\n",
      "Epoch 150/150\n",
      "691/691 [==============================] - 0s 151us/step - loss: 0.4527 - acc: 0.7945\n",
      "77/77 [==============================] - 0s 3ms/step\n",
      "Epoch 1/150\n",
      "691/691 [==============================] - 1s 1ms/step - loss: 0.6788 - acc: 0.6498\n",
      "Epoch 2/150\n",
      "691/691 [==============================] - 0s 172us/step - loss: 0.6616 - acc: 0.6512\n",
      "Epoch 3/150\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.6519 - acc: 0.6512\n",
      "Epoch 4/150\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.6412 - acc: 0.6512\n",
      "Epoch 5/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.6344 - acc: 0.6512\n",
      "Epoch 6/150\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.6231 - acc: 0.6512\n",
      "Epoch 7/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.6202 - acc: 0.6498\n",
      "Epoch 8/150\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.6171 - acc: 0.6512\n",
      "Epoch 9/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.6059 - acc: 0.6860\n",
      "Epoch 10/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.6021 - acc: 0.6831\n",
      "Epoch 11/150\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.5932 - acc: 0.6874\n",
      "Epoch 12/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.5932 - acc: 0.6874\n",
      "Epoch 13/150\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.5867 - acc: 0.7062\n",
      "Epoch 14/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.5893 - acc: 0.7019\n",
      "Epoch 15/150\n",
      "691/691 [==============================] - 0s 206us/step - loss: 0.5857 - acc: 0.7019\n",
      "Epoch 16/150\n",
      "691/691 [==============================] - 0s 182us/step - loss: 0.5851 - acc: 0.6903\n",
      "Epoch 17/150\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.5754 - acc: 0.7091\n",
      "Epoch 18/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.5824 - acc: 0.6874\n",
      "Epoch 19/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.5724 - acc: 0.7033\n",
      "Epoch 20/150\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.5737 - acc: 0.6918\n",
      "Epoch 21/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.5745 - acc: 0.6946\n",
      "Epoch 22/150\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.5709 - acc: 0.6946\n",
      "Epoch 23/150\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.5698 - acc: 0.7164\n",
      "Epoch 24/150\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.5810 - acc: 0.6831\n",
      "Epoch 25/150\n",
      "691/691 [==============================] - 0s 130us/step - loss: 0.5715 - acc: 0.6961\n",
      "Epoch 26/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.5629 - acc: 0.7149\n",
      "Epoch 27/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.5659 - acc: 0.6961\n",
      "Epoch 28/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.5624 - acc: 0.7048\n",
      "Epoch 29/150\n",
      "691/691 [==============================] - 0s 130us/step - loss: 0.5584 - acc: 0.7091\n",
      "Epoch 30/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.5594 - acc: 0.7048\n",
      "Epoch 31/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.5709 - acc: 0.6889\n",
      "Epoch 32/150\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.5626 - acc: 0.7062\n",
      "Epoch 33/150\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.5671 - acc: 0.6990\n",
      "Epoch 34/150\n",
      "691/691 [==============================] - 0s 130us/step - loss: 0.5608 - acc: 0.7106\n",
      "Epoch 35/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.5614 - acc: 0.7062\n",
      "Epoch 36/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.5569 - acc: 0.7048\n",
      "Epoch 37/150\n",
      "691/691 [==============================] - 0s 130us/step - loss: 0.5544 - acc: 0.7091\n",
      "Epoch 38/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.5471 - acc: 0.7192\n",
      "Epoch 39/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "691/691 [==============================] - 0s 136us/step - loss: 0.5583 - acc: 0.7135\n",
      "Epoch 40/150\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.5515 - acc: 0.7164\n",
      "Epoch 41/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.5496 - acc: 0.7164\n",
      "Epoch 42/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.5451 - acc: 0.7149\n",
      "Epoch 43/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.5401 - acc: 0.7323\n",
      "Epoch 44/150\n",
      "691/691 [==============================] - 0s 146us/step - loss: 0.5420 - acc: 0.7294\n",
      "Epoch 45/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.5435 - acc: 0.7164\n",
      "Epoch 46/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.5457 - acc: 0.7135\n",
      "Epoch 47/150\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.5385 - acc: 0.7308\n",
      "Epoch 48/150\n",
      "691/691 [==============================] - 0s 130us/step - loss: 0.5411 - acc: 0.7236\n",
      "Epoch 49/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.5342 - acc: 0.7395\n",
      "Epoch 50/150\n",
      "691/691 [==============================] - 0s 168us/step - loss: 0.5382 - acc: 0.7279\n",
      "Epoch 51/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.5360 - acc: 0.7366 0s - loss: 0.5402 - acc: 0.736\n",
      "Epoch 52/150\n",
      "691/691 [==============================] - 0s 133us/step - loss: 0.5345 - acc: 0.7366\n",
      "Epoch 53/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.5356 - acc: 0.7352\n",
      "Epoch 54/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.5368 - acc: 0.7236\n",
      "Epoch 55/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.5348 - acc: 0.7323\n",
      "Epoch 56/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.5270 - acc: 0.7496\n",
      "Epoch 57/150\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.5279 - acc: 0.7395\n",
      "Epoch 58/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.5259 - acc: 0.7410\n",
      "Epoch 59/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.5217 - acc: 0.7366\n",
      "Epoch 60/150\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.5174 - acc: 0.7410\n",
      "Epoch 61/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.5209 - acc: 0.7438\n",
      "Epoch 62/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.5176 - acc: 0.7424\n",
      "Epoch 63/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.5168 - acc: 0.7381\n",
      "Epoch 64/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.5231 - acc: 0.7323\n",
      "Epoch 65/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.5211 - acc: 0.7323\n",
      "Epoch 66/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.5099 - acc: 0.7467\n",
      "Epoch 67/150\n",
      "691/691 [==============================] - 0s 133us/step - loss: 0.5215 - acc: 0.7381\n",
      "Epoch 68/150\n",
      "691/691 [==============================] - 0s 133us/step - loss: 0.5171 - acc: 0.7583\n",
      "Epoch 69/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.5219 - acc: 0.7395\n",
      "Epoch 70/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.5101 - acc: 0.7540\n",
      "Epoch 71/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.5144 - acc: 0.7366\n",
      "Epoch 72/150\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.5107 - acc: 0.7540\n",
      "Epoch 73/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.5093 - acc: 0.7525\n",
      "Epoch 74/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.5062 - acc: 0.7569\n",
      "Epoch 75/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.5062 - acc: 0.7583\n",
      "Epoch 76/150\n",
      "691/691 [==============================] - 0s 178us/step - loss: 0.5032 - acc: 0.7656\n",
      "Epoch 77/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.5122 - acc: 0.7540\n",
      "Epoch 78/150\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.5030 - acc: 0.7612\n",
      "Epoch 79/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.4963 - acc: 0.7569\n",
      "Epoch 80/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.4954 - acc: 0.7627\n",
      "Epoch 81/150\n",
      "691/691 [==============================] - 0s 177us/step - loss: 0.5027 - acc: 0.7525\n",
      "Epoch 82/150\n",
      "691/691 [==============================] - 0s 133us/step - loss: 0.4961 - acc: 0.7670\n",
      "Epoch 83/150\n",
      "691/691 [==============================] - 0s 133us/step - loss: 0.4958 - acc: 0.7728\n",
      "Epoch 84/150\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.4878 - acc: 0.7656\n",
      "Epoch 85/150\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.4916 - acc: 0.7482\n",
      "Epoch 86/150\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.4961 - acc: 0.7656\n",
      "Epoch 87/150\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.4875 - acc: 0.7612\n",
      "Epoch 88/150\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.4963 - acc: 0.7569\n",
      "Epoch 89/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.4839 - acc: 0.7786\n",
      "Epoch 90/150\n",
      "691/691 [==============================] - 0s 133us/step - loss: 0.4904 - acc: 0.7496\n",
      "Epoch 91/150\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.4854 - acc: 0.7757\n",
      "Epoch 92/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.4900 - acc: 0.7771\n",
      "Epoch 93/150\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.4875 - acc: 0.7713\n",
      "Epoch 94/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.4855 - acc: 0.7656\n",
      "Epoch 95/150\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.4853 - acc: 0.7786\n",
      "Epoch 96/150\n",
      "691/691 [==============================] - 0s 236us/step - loss: 0.4823 - acc: 0.7786\n",
      "Epoch 97/150\n",
      "691/691 [==============================] - 0s 190us/step - loss: 0.4892 - acc: 0.7685\n",
      "Epoch 98/150\n",
      "691/691 [==============================] - 0s 148us/step - loss: 0.4858 - acc: 0.7786\n",
      "Epoch 99/150\n",
      "691/691 [==============================] - 0s 206us/step - loss: 0.4765 - acc: 0.7713\n",
      "Epoch 100/150\n",
      "691/691 [==============================] - 0s 158us/step - loss: 0.4749 - acc: 0.7713\n",
      "Epoch 101/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4739 - acc: 0.7873\n",
      "Epoch 102/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.4806 - acc: 0.7627\n",
      "Epoch 103/150\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.4739 - acc: 0.7670\n",
      "Epoch 104/150\n",
      "691/691 [==============================] - 0s 196us/step - loss: 0.4756 - acc: 0.7728\n",
      "Epoch 105/150\n",
      "691/691 [==============================] - ETA: 0s - loss: 0.4815 - acc: 0.776 - 0s 203us/step - loss: 0.4793 - acc: 0.7713\n",
      "Epoch 106/150\n",
      "691/691 [==============================] - 0s 151us/step - loss: 0.4706 - acc: 0.7757\n",
      "Epoch 107/150\n",
      "691/691 [==============================] - ETA: 0s - loss: 0.4732 - acc: 0.775 - 0s 169us/step - loss: 0.4735 - acc: 0.7728\n",
      "Epoch 108/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.4751 - acc: 0.7713\n",
      "Epoch 109/150\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.4701 - acc: 0.7641\n",
      "Epoch 110/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.4748 - acc: 0.7916\n",
      "Epoch 111/150\n",
      "691/691 [==============================] - 0s 133us/step - loss: 0.4702 - acc: 0.7742\n",
      "Epoch 112/150\n",
      "691/691 [==============================] - 0s 133us/step - loss: 0.4659 - acc: 0.7815\n",
      "Epoch 113/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.4814 - acc: 0.7699\n",
      "Epoch 114/150\n",
      "691/691 [==============================] - 0s 130us/step - loss: 0.4686 - acc: 0.7800\n",
      "Epoch 115/150\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.4927 - acc: 0.7627\n",
      "Epoch 116/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.4762 - acc: 0.7771\n",
      "Epoch 117/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.4625 - acc: 0.7858\n",
      "Epoch 118/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.4862 - acc: 0.7583\n",
      "Epoch 119/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.4679 - acc: 0.7800\n",
      "Epoch 120/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.4672 - acc: 0.7844\n",
      "Epoch 121/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.4718 - acc: 0.7670\n",
      "Epoch 122/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.4611 - acc: 0.7786\n",
      "Epoch 123/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.4656 - acc: 0.7728\n",
      "Epoch 124/150\n",
      "691/691 [==============================] - 0s 185us/step - loss: 0.4702 - acc: 0.7728\n",
      "Epoch 125/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4658 - acc: 0.7771\n",
      "Epoch 126/150\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.4615 - acc: 0.7815\n",
      "Epoch 127/150\n",
      "691/691 [==============================] - 0s 133us/step - loss: 0.4737 - acc: 0.7685\n",
      "Epoch 128/150\n",
      "691/691 [==============================] - 0s 130us/step - loss: 0.4590 - acc: 0.7713\n",
      "Epoch 129/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4579 - acc: 0.7757\n",
      "Epoch 130/150\n",
      "691/691 [==============================] - 0s 130us/step - loss: 0.4700 - acc: 0.7742\n",
      "Epoch 131/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.4510 - acc: 0.7916\n",
      "Epoch 132/150\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.4568 - acc: 0.7829\n",
      "Epoch 133/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.4641 - acc: 0.7815\n",
      "Epoch 134/150\n",
      "691/691 [==============================] - 0s 133us/step - loss: 0.4562 - acc: 0.7844\n",
      "Epoch 135/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.4619 - acc: 0.7713\n",
      "Epoch 136/150\n",
      "691/691 [==============================] - 0s 130us/step - loss: 0.4591 - acc: 0.7815\n",
      "Epoch 137/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.4649 - acc: 0.7713\n",
      "Epoch 138/150\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.4558 - acc: 0.7887\n",
      "Epoch 139/150\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.4602 - acc: 0.7742\n",
      "Epoch 140/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.4581 - acc: 0.7641\n",
      "Epoch 141/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.4506 - acc: 0.7858\n",
      "Epoch 142/150\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.4501 - acc: 0.7815\n",
      "Epoch 143/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.4546 - acc: 0.7713\n",
      "Epoch 144/150\n",
      "691/691 [==============================] - 0s 151us/step - loss: 0.4641 - acc: 0.7699\n",
      "Epoch 145/150\n",
      "691/691 [==============================] - 0s 167us/step - loss: 0.4515 - acc: 0.7873\n",
      "Epoch 146/150\n",
      "691/691 [==============================] - 0s 178us/step - loss: 0.4549 - acc: 0.7916\n",
      "Epoch 147/150\n",
      "691/691 [==============================] - 0s 180us/step - loss: 0.4508 - acc: 0.7815\n",
      "Epoch 148/150\n",
      "691/691 [==============================] - 0s 187us/step - loss: 0.4513 - acc: 0.7887\n",
      "Epoch 149/150\n",
      "691/691 [==============================] - 0s 182us/step - loss: 0.4517 - acc: 0.7771\n",
      "Epoch 150/150\n",
      "691/691 [==============================] - 0s 172us/step - loss: 0.4523 - acc: 0.7858\n",
      "77/77 [==============================] - 0s 3ms/step\n",
      "Epoch 1/150\n",
      "691/691 [==============================] - 1s 1ms/step - loss: 0.6846 - acc: 0.6093\n",
      "Epoch 2/150\n",
      "691/691 [==============================] - 0s 185us/step - loss: 0.6632 - acc: 0.6512\n",
      "Epoch 3/150\n",
      "691/691 [==============================] - 0s 193us/step - loss: 0.6586 - acc: 0.6512\n",
      "Epoch 4/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.6508 - acc: 0.6512\n",
      "Epoch 5/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.6365 - acc: 0.6556\n",
      "Epoch 6/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.6293 - acc: 0.6614\n",
      "Epoch 7/150\n",
      "691/691 [==============================] - 0s 149us/step - loss: 0.6185 - acc: 0.6628\n",
      "Epoch 8/150\n",
      "691/691 [==============================] - 0s 165us/step - loss: 0.6186 - acc: 0.6773\n",
      "Epoch 9/150\n",
      "691/691 [==============================] - 0s 148us/step - loss: 0.6069 - acc: 0.6816\n",
      "Epoch 10/150\n",
      "691/691 [==============================] - 0s 178us/step - loss: 0.6059 - acc: 0.6816\n",
      "Epoch 11/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.5928 - acc: 0.6961\n",
      "Epoch 12/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.5968 - acc: 0.6889\n",
      "Epoch 13/150\n",
      "691/691 [==============================] - 0s 171us/step - loss: 0.6015 - acc: 0.6918\n",
      "Epoch 14/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.5865 - acc: 0.7033\n",
      "Epoch 15/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.5857 - acc: 0.7062\n",
      "Epoch 16/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.5819 - acc: 0.6946\n",
      "Epoch 17/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.5807 - acc: 0.7004\n",
      "Epoch 18/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.5794 - acc: 0.7019\n",
      "Epoch 19/150\n",
      "691/691 [==============================] - 0s 167us/step - loss: 0.5751 - acc: 0.7033\n",
      "Epoch 20/150\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.5666 - acc: 0.6975\n",
      "Epoch 21/150\n",
      "691/691 [==============================] - 0s 149us/step - loss: 0.5833 - acc: 0.6903\n",
      "Epoch 22/150\n",
      "691/691 [==============================] - 0s 149us/step - loss: 0.5712 - acc: 0.7192\n",
      "Epoch 23/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.5758 - acc: 0.6918\n",
      "Epoch 24/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.5715 - acc: 0.7135\n",
      "Epoch 25/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.5687 - acc: 0.6918\n",
      "Epoch 26/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.5691 - acc: 0.7048\n",
      "Epoch 27/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.5663 - acc: 0.7091\n",
      "Epoch 28/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.5656 - acc: 0.7019\n",
      "Epoch 29/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.5689 - acc: 0.7019\n",
      "Epoch 30/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.5679 - acc: 0.7048\n",
      "Epoch 31/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.5568 - acc: 0.7091\n",
      "Epoch 32/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.5643 - acc: 0.7120\n",
      "Epoch 33/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.5571 - acc: 0.7149\n",
      "Epoch 34/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.5581 - acc: 0.7149\n",
      "Epoch 35/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.5558 - acc: 0.7308\n",
      "Epoch 36/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.5775 - acc: 0.7048\n",
      "Epoch 37/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.5535 - acc: 0.7294\n",
      "Epoch 38/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.5542 - acc: 0.7294\n",
      "Epoch 39/150\n",
      "691/691 [==============================] - 0s 171us/step - loss: 0.5543 - acc: 0.7135\n",
      "Epoch 40/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.5494 - acc: 0.7221\n",
      "Epoch 41/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.5522 - acc: 0.7135\n",
      "Epoch 42/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.5615 - acc: 0.6975\n",
      "Epoch 43/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.5462 - acc: 0.7207\n",
      "Epoch 44/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.5543 - acc: 0.7250\n",
      "Epoch 45/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.5486 - acc: 0.7294\n",
      "Epoch 46/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.5541 - acc: 0.7221\n",
      "Epoch 47/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.5563 - acc: 0.7178\n",
      "Epoch 48/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.5405 - acc: 0.7410\n",
      "Epoch 49/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.5422 - acc: 0.7424\n",
      "Epoch 50/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.5418 - acc: 0.7496\n",
      "Epoch 51/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.5496 - acc: 0.7192\n",
      "Epoch 52/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "691/691 [==============================] - 0s 148us/step - loss: 0.5392 - acc: 0.7424\n",
      "Epoch 53/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.5423 - acc: 0.7410\n",
      "Epoch 54/150\n",
      "691/691 [==============================] - 0s 167us/step - loss: 0.5371 - acc: 0.7221\n",
      "Epoch 55/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.5421 - acc: 0.7337\n",
      "Epoch 56/150\n",
      "691/691 [==============================] - 0s 198us/step - loss: 0.5337 - acc: 0.7352\n",
      "Epoch 57/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.5405 - acc: 0.7424\n",
      "Epoch 58/150\n",
      "691/691 [==============================] - 0s 187us/step - loss: 0.5287 - acc: 0.7525\n",
      "Epoch 59/150\n",
      "691/691 [==============================] - 0s 191us/step - loss: 0.5286 - acc: 0.7308\n",
      "Epoch 60/150\n",
      "691/691 [==============================] - 0s 164us/step - loss: 0.5346 - acc: 0.7424\n",
      "Epoch 61/150\n",
      "691/691 [==============================] - 0s 169us/step - loss: 0.5367 - acc: 0.7482\n",
      "Epoch 62/150\n",
      "691/691 [==============================] - 0s 162us/step - loss: 0.5312 - acc: 0.7410\n",
      "Epoch 63/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.5269 - acc: 0.7583\n",
      "Epoch 64/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.5306 - acc: 0.7540 0s - loss: 0.5420 - acc: 0.735\n",
      "Epoch 65/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.5250 - acc: 0.7366 0s - loss: 0.5134 - acc: 0.753\n",
      "Epoch 66/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.5226 - acc: 0.7381\n",
      "Epoch 67/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.5242 - acc: 0.7554\n",
      "Epoch 68/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.5183 - acc: 0.7598\n",
      "Epoch 69/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.5211 - acc: 0.7540\n",
      "Epoch 70/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.5197 - acc: 0.7656\n",
      "Epoch 71/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.5171 - acc: 0.7496\n",
      "Epoch 72/150\n",
      "691/691 [==============================] - 0s 146us/step - loss: 0.5049 - acc: 0.7627\n",
      "Epoch 73/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.5282 - acc: 0.7395\n",
      "Epoch 74/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.5389 - acc: 0.7511\n",
      "Epoch 75/150\n",
      "691/691 [==============================] - 0s 149us/step - loss: 0.5070 - acc: 0.7685\n",
      "Epoch 76/150\n",
      "691/691 [==============================] - ETA: 0s - loss: 0.5139 - acc: 0.7485- ETA: 0s - loss: 0.4925 - acc: 0.77 - 0s 167us/step - loss: 0.5160 - acc: 0.7467\n",
      "Epoch 77/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.5058 - acc: 0.7627\n",
      "Epoch 78/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.5048 - acc: 0.7627\n",
      "Epoch 79/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.5053 - acc: 0.7554\n",
      "Epoch 80/150\n",
      "691/691 [==============================] - 0s 174us/step - loss: 0.5080 - acc: 0.7598\n",
      "Epoch 81/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.5088 - acc: 0.7583\n",
      "Epoch 82/150\n",
      "691/691 [==============================] - 0s 149us/step - loss: 0.5056 - acc: 0.7583\n",
      "Epoch 83/150\n",
      "691/691 [==============================] - 0s 148us/step - loss: 0.5045 - acc: 0.7598\n",
      "Epoch 84/150\n",
      "691/691 [==============================] - 0s 146us/step - loss: 0.4960 - acc: 0.7641\n",
      "Epoch 85/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.5199 - acc: 0.7583\n",
      "Epoch 86/150\n",
      "691/691 [==============================] - 0s 146us/step - loss: 0.5008 - acc: 0.7699\n",
      "Epoch 87/150\n",
      "691/691 [==============================] - 0s 172us/step - loss: 0.4934 - acc: 0.7757 0s - loss: 0.4674 - acc: 0.80\n",
      "Epoch 88/150\n",
      "691/691 [==============================] - 0s 148us/step - loss: 0.4965 - acc: 0.7670\n",
      "Epoch 89/150\n",
      "691/691 [==============================] - 0s 148us/step - loss: 0.5017 - acc: 0.7713\n",
      "Epoch 90/150\n",
      "691/691 [==============================] - 0s 149us/step - loss: 0.4922 - acc: 0.7800\n",
      "Epoch 91/150\n",
      "691/691 [==============================] - 0s 156us/step - loss: 0.4949 - acc: 0.7699\n",
      "Epoch 92/150\n",
      "691/691 [==============================] - 0s 207us/step - loss: 0.5022 - acc: 0.7569\n",
      "Epoch 93/150\n",
      "691/691 [==============================] - 0s 171us/step - loss: 0.4978 - acc: 0.7612\n",
      "Epoch 94/150\n",
      "691/691 [==============================] - 0s 148us/step - loss: 0.4979 - acc: 0.7656\n",
      "Epoch 95/150\n",
      "691/691 [==============================] - 0s 146us/step - loss: 0.4916 - acc: 0.7685 0s - loss: 0.4854 - acc: 0.771\n",
      "Epoch 96/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4959 - acc: 0.7713\n",
      "Epoch 97/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.4999 - acc: 0.7583\n",
      "Epoch 98/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4936 - acc: 0.7670\n",
      "Epoch 99/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4894 - acc: 0.7786\n",
      "Epoch 100/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.4935 - acc: 0.7641\n",
      "Epoch 101/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.4928 - acc: 0.7771\n",
      "Epoch 102/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.4847 - acc: 0.7612\n",
      "Epoch 103/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.4841 - acc: 0.7786\n",
      "Epoch 104/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.4821 - acc: 0.7728\n",
      "Epoch 105/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.4881 - acc: 0.7656\n",
      "Epoch 106/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.4915 - acc: 0.7627\n",
      "Epoch 107/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.4949 - acc: 0.7569\n",
      "Epoch 108/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4830 - acc: 0.7583\n",
      "Epoch 109/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.4837 - acc: 0.7699\n",
      "Epoch 110/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.4810 - acc: 0.7815\n",
      "Epoch 111/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4827 - acc: 0.7786\n",
      "Epoch 112/150\n",
      "691/691 [==============================] - 0s 168us/step - loss: 0.4811 - acc: 0.7844\n",
      "Epoch 113/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.4882 - acc: 0.7685\n",
      "Epoch 114/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.4772 - acc: 0.7902\n",
      "Epoch 115/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.4869 - acc: 0.7771\n",
      "Epoch 116/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.4979 - acc: 0.7612\n",
      "Epoch 117/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.4723 - acc: 0.7699\n",
      "Epoch 118/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.4767 - acc: 0.7728\n",
      "Epoch 119/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.4813 - acc: 0.7641\n",
      "Epoch 120/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.4730 - acc: 0.7815\n",
      "Epoch 121/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4811 - acc: 0.7670\n",
      "Epoch 122/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.4797 - acc: 0.7771\n",
      "Epoch 123/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.4788 - acc: 0.7786\n",
      "Epoch 124/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.4711 - acc: 0.7728\n",
      "Epoch 125/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.4680 - acc: 0.7887\n",
      "Epoch 126/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.4788 - acc: 0.7699\n",
      "Epoch 127/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4783 - acc: 0.7815\n",
      "Epoch 128/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4819 - acc: 0.7699\n",
      "Epoch 129/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.4700 - acc: 0.7800\n",
      "Epoch 130/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.4743 - acc: 0.7844\n",
      "Epoch 131/150\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.4700 - acc: 0.7771\n",
      "Epoch 132/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.4763 - acc: 0.7858\n",
      "Epoch 133/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.4940 - acc: 0.7656\n",
      "Epoch 134/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4804 - acc: 0.7829\n",
      "Epoch 135/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.4676 - acc: 0.7800\n",
      "Epoch 136/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.4706 - acc: 0.7598\n",
      "Epoch 137/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4707 - acc: 0.7887\n",
      "Epoch 138/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.4731 - acc: 0.7844\n",
      "Epoch 139/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.4656 - acc: 0.7829\n",
      "Epoch 140/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4628 - acc: 0.7815\n",
      "Epoch 141/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.4732 - acc: 0.7887\n",
      "Epoch 142/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.4610 - acc: 0.7873\n",
      "Epoch 143/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4647 - acc: 0.7931\n",
      "Epoch 144/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.4614 - acc: 0.7757\n",
      "Epoch 145/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.4612 - acc: 0.7786\n",
      "Epoch 146/150\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.4637 - acc: 0.8003\n",
      "Epoch 147/150\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.4637 - acc: 0.7931\n",
      "Epoch 148/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.4596 - acc: 0.7887\n",
      "Epoch 149/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.4672 - acc: 0.7829 0s - loss: 0.4655 - acc: 0.780\n",
      "Epoch 150/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.4957 - acc: 0.7685\n",
      "77/77 [==============================] - 0s 4ms/step\n",
      "Epoch 1/150\n",
      "691/691 [==============================] - 1s 1ms/step - loss: 0.6824 - acc: 0.6512\n",
      "Epoch 2/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.6660 - acc: 0.6512\n",
      "Epoch 3/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.6572 - acc: 0.6483\n",
      "Epoch 4/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.6451 - acc: 0.6585\n",
      "Epoch 5/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.6300 - acc: 0.6628\n",
      "Epoch 6/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.6201 - acc: 0.6700\n",
      "Epoch 7/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.6133 - acc: 0.6700\n",
      "Epoch 8/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.6114 - acc: 0.6686\n",
      "Epoch 9/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.6007 - acc: 0.6889\n",
      "Epoch 10/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.5948 - acc: 0.7004\n",
      "Epoch 11/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.5960 - acc: 0.6874\n",
      "Epoch 12/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.5898 - acc: 0.7077\n",
      "Epoch 13/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.5947 - acc: 0.6975\n",
      "Epoch 14/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.5907 - acc: 0.6961\n",
      "Epoch 15/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.5859 - acc: 0.7149\n",
      "Epoch 16/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.5811 - acc: 0.7019\n",
      "Epoch 17/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.5937 - acc: 0.6990\n",
      "Epoch 18/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.5822 - acc: 0.7120\n",
      "Epoch 19/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.5756 - acc: 0.7120\n",
      "Epoch 20/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.5785 - acc: 0.7004\n",
      "Epoch 21/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.5796 - acc: 0.7091\n",
      "Epoch 22/150\n",
      "691/691 [==============================] - 0s 172us/step - loss: 0.5733 - acc: 0.7120 0s - loss: 0.5984 - acc: 0.70\n",
      "Epoch 23/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.5685 - acc: 0.7236\n",
      "Epoch 24/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.5748 - acc: 0.7192\n",
      "Epoch 25/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.5695 - acc: 0.7178\n",
      "Epoch 26/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.5649 - acc: 0.7192\n",
      "Epoch 27/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.5635 - acc: 0.7221\n",
      "Epoch 28/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.5678 - acc: 0.7221\n",
      "Epoch 29/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.5605 - acc: 0.7410\n",
      "Epoch 30/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.5597 - acc: 0.7337\n",
      "Epoch 31/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.5563 - acc: 0.7279\n",
      "Epoch 32/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.5658 - acc: 0.7135\n",
      "Epoch 33/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.5516 - acc: 0.7323\n",
      "Epoch 34/150\n",
      "691/691 [==============================] - ETA: 0s - loss: 0.5479 - acc: 0.728 - 0s 169us/step - loss: 0.5490 - acc: 0.7323\n",
      "Epoch 35/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.5588 - acc: 0.7265\n",
      "Epoch 36/150\n",
      "691/691 [==============================] - 0s 156us/step - loss: 0.5518 - acc: 0.7381\n",
      "Epoch 37/150\n",
      "691/691 [==============================] - 0s 165us/step - loss: 0.5465 - acc: 0.7424\n",
      "Epoch 38/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.5444 - acc: 0.7424 0s - loss: 0.5545 - acc: 0.736\n",
      "Epoch 39/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.5405 - acc: 0.7496\n",
      "Epoch 40/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.5379 - acc: 0.7496\n",
      "Epoch 41/150\n",
      "691/691 [==============================] - ETA: 0s - loss: 0.5437 - acc: 0.741 - 0s 172us/step - loss: 0.5421 - acc: 0.7424\n",
      "Epoch 42/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.5446 - acc: 0.7352\n",
      "Epoch 43/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.5391 - acc: 0.7265\n",
      "Epoch 44/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.5388 - acc: 0.7482\n",
      "Epoch 45/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.5301 - acc: 0.7540\n",
      "Epoch 46/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.5284 - acc: 0.7525\n",
      "Epoch 47/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.5508 - acc: 0.7250\n",
      "Epoch 48/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.5384 - acc: 0.7641\n",
      "Epoch 49/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.5339 - acc: 0.7395\n",
      "Epoch 50/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.5305 - acc: 0.7395\n",
      "Epoch 51/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.5276 - acc: 0.7424\n",
      "Epoch 52/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.5379 - acc: 0.7381\n",
      "Epoch 53/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.5265 - acc: 0.7612\n",
      "Epoch 54/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.5223 - acc: 0.7496\n",
      "Epoch 55/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.5249 - acc: 0.7424\n",
      "Epoch 56/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.5171 - acc: 0.7554\n",
      "Epoch 57/150\n",
      "691/691 [==============================] - 0s 148us/step - loss: 0.5293 - acc: 0.7453\n",
      "Epoch 58/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.5199 - acc: 0.7496\n",
      "Epoch 59/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.5162 - acc: 0.7554\n",
      "Epoch 60/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.5179 - acc: 0.7453\n",
      "Epoch 61/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.5187 - acc: 0.7583\n",
      "Epoch 62/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "691/691 [==============================] - 0s 140us/step - loss: 0.5170 - acc: 0.7583\n",
      "Epoch 63/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.5130 - acc: 0.7583\n",
      "Epoch 64/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.5115 - acc: 0.7482\n",
      "Epoch 65/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.5250 - acc: 0.7540\n",
      "Epoch 66/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.5098 - acc: 0.7540\n",
      "Epoch 67/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.5134 - acc: 0.7627\n",
      "Epoch 68/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.5120 - acc: 0.7641\n",
      "Epoch 69/150\n",
      "691/691 [==============================] - 0s 146us/step - loss: 0.5046 - acc: 0.7771\n",
      "Epoch 70/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.5007 - acc: 0.7699\n",
      "Epoch 71/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.4985 - acc: 0.7757\n",
      "Epoch 72/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.5064 - acc: 0.7713\n",
      "Epoch 73/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.5096 - acc: 0.7424\n",
      "Epoch 74/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.5108 - acc: 0.7627\n",
      "Epoch 75/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.5060 - acc: 0.7641\n",
      "Epoch 76/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.5042 - acc: 0.7583\n",
      "Epoch 77/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.4966 - acc: 0.7641\n",
      "Epoch 78/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.5063 - acc: 0.7525\n",
      "Epoch 79/150\n",
      "691/691 [==============================] - 0s 148us/step - loss: 0.4927 - acc: 0.7742\n",
      "Epoch 80/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.5020 - acc: 0.7496\n",
      "Epoch 81/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.4931 - acc: 0.7873\n",
      "Epoch 82/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.5011 - acc: 0.7496\n",
      "Epoch 83/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.4946 - acc: 0.7771\n",
      "Epoch 84/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.4957 - acc: 0.7685\n",
      "Epoch 85/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.4904 - acc: 0.7902\n",
      "Epoch 86/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.4983 - acc: 0.7612\n",
      "Epoch 87/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.4888 - acc: 0.7771\n",
      "Epoch 88/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.4964 - acc: 0.7569\n",
      "Epoch 89/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.4900 - acc: 0.7685\n",
      "Epoch 90/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4912 - acc: 0.7699\n",
      "Epoch 91/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.4847 - acc: 0.7728\n",
      "Epoch 92/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.4852 - acc: 0.7713\n",
      "Epoch 93/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4901 - acc: 0.7742\n",
      "Epoch 94/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.4833 - acc: 0.7641\n",
      "Epoch 95/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.4869 - acc: 0.7670\n",
      "Epoch 96/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.4935 - acc: 0.7670\n",
      "Epoch 97/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4840 - acc: 0.7641\n",
      "Epoch 98/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4842 - acc: 0.7771\n",
      "Epoch 99/150\n",
      "691/691 [==============================] - 0s 169us/step - loss: 0.4878 - acc: 0.7554\n",
      "Epoch 100/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.4833 - acc: 0.7598\n",
      "Epoch 101/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4870 - acc: 0.7656\n",
      "Epoch 102/150\n",
      "691/691 [==============================] - 0s 167us/step - loss: 0.4772 - acc: 0.7786\n",
      "Epoch 103/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4773 - acc: 0.7713\n",
      "Epoch 104/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.4837 - acc: 0.7670\n",
      "Epoch 105/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.4738 - acc: 0.7858\n",
      "Epoch 106/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4798 - acc: 0.7685\n",
      "Epoch 107/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.4770 - acc: 0.7800\n",
      "Epoch 108/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4775 - acc: 0.7713\n",
      "Epoch 109/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.4745 - acc: 0.7713\n",
      "Epoch 110/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4765 - acc: 0.7685\n",
      "Epoch 111/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4750 - acc: 0.7728\n",
      "Epoch 112/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4887 - acc: 0.7583\n",
      "Epoch 113/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4735 - acc: 0.7728\n",
      "Epoch 114/150\n",
      "691/691 [==============================] - 0s 181us/step - loss: 0.4753 - acc: 0.7728\n",
      "Epoch 115/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4665 - acc: 0.7858\n",
      "Epoch 116/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.4740 - acc: 0.7728\n",
      "Epoch 117/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.4712 - acc: 0.7656\n",
      "Epoch 118/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.4693 - acc: 0.7786\n",
      "Epoch 119/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.4658 - acc: 0.7786\n",
      "Epoch 120/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.4614 - acc: 0.7800\n",
      "Epoch 121/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.4704 - acc: 0.7815\n",
      "Epoch 122/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.4766 - acc: 0.7670\n",
      "Epoch 123/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.4671 - acc: 0.7742\n",
      "Epoch 124/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.4731 - acc: 0.7685\n",
      "Epoch 125/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4782 - acc: 0.7800\n",
      "Epoch 126/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4744 - acc: 0.7728\n",
      "Epoch 127/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4678 - acc: 0.7728\n",
      "Epoch 128/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.4710 - acc: 0.7641\n",
      "Epoch 129/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4605 - acc: 0.7815\n",
      "Epoch 130/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.4667 - acc: 0.7699\n",
      "Epoch 131/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.4664 - acc: 0.7902\n",
      "Epoch 132/150\n",
      "691/691 [==============================] - 0s 172us/step - loss: 0.4625 - acc: 0.7685\n",
      "Epoch 133/150\n",
      "691/691 [==============================] - 0s 177us/step - loss: 0.4617 - acc: 0.7771\n",
      "Epoch 134/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.4578 - acc: 0.7858\n",
      "Epoch 135/150\n",
      "691/691 [==============================] - 0s 146us/step - loss: 0.4598 - acc: 0.7713\n",
      "Epoch 136/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4619 - acc: 0.7902\n",
      "Epoch 137/150\n",
      "691/691 [==============================] - 0s 146us/step - loss: 0.4563 - acc: 0.7829\n",
      "Epoch 138/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4544 - acc: 0.7858\n",
      "Epoch 139/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.4648 - acc: 0.7757\n",
      "Epoch 140/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.4633 - acc: 0.7742\n",
      "Epoch 141/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.4601 - acc: 0.7844\n",
      "Epoch 142/150\n",
      "691/691 [==============================] - 0s 148us/step - loss: 0.4600 - acc: 0.7815\n",
      "Epoch 143/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4545 - acc: 0.7786\n",
      "Epoch 144/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.4546 - acc: 0.7844\n",
      "Epoch 145/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.4656 - acc: 0.7800\n",
      "Epoch 146/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.4603 - acc: 0.7873\n",
      "Epoch 147/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.4554 - acc: 0.7829\n",
      "Epoch 148/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.4503 - acc: 0.7829\n",
      "Epoch 149/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.4493 - acc: 0.7873\n",
      "Epoch 150/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4419 - acc: 0.7945\n",
      "77/77 [==============================] - 0s 4ms/step\n",
      "Epoch 1/150\n",
      "691/691 [==============================] - 1s 1ms/step - loss: 0.6833 - acc: 0.6368\n",
      "Epoch 2/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.6647 - acc: 0.6512\n",
      "Epoch 3/150\n",
      "691/691 [==============================] - 0s 149us/step - loss: 0.6568 - acc: 0.6570\n",
      "Epoch 4/150\n",
      "691/691 [==============================] - 0s 146us/step - loss: 0.6477 - acc: 0.6614\n",
      "Epoch 5/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.6441 - acc: 0.6614\n",
      "Epoch 6/150\n",
      "691/691 [==============================] - 0s 149us/step - loss: 0.6336 - acc: 0.6614\n",
      "Epoch 7/150\n",
      "691/691 [==============================] - 0s 148us/step - loss: 0.6270 - acc: 0.6556\n",
      "Epoch 8/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.6180 - acc: 0.6729\n",
      "Epoch 9/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.6062 - acc: 0.6990\n",
      "Epoch 10/150\n",
      "691/691 [==============================] - 0s 149us/step - loss: 0.5967 - acc: 0.6932\n",
      "Epoch 11/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.5952 - acc: 0.6889\n",
      "Epoch 12/150\n",
      "691/691 [==============================] - 0s 146us/step - loss: 0.5938 - acc: 0.6889 0s - loss: 0.5856 - acc: 0.712\n",
      "Epoch 13/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.5936 - acc: 0.7120\n",
      "Epoch 14/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.5935 - acc: 0.6889\n",
      "Epoch 15/150\n",
      "691/691 [==============================] - 0s 148us/step - loss: 0.5918 - acc: 0.6845 0s - loss: 0.6016 - acc: 0.670\n",
      "Epoch 16/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.5802 - acc: 0.7062\n",
      "Epoch 17/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.5823 - acc: 0.7091\n",
      "Epoch 18/150\n",
      "691/691 [==============================] - 0s 148us/step - loss: 0.5744 - acc: 0.7236\n",
      "Epoch 19/150\n",
      "691/691 [==============================] - 0s 165us/step - loss: 0.5773 - acc: 0.7106\n",
      "Epoch 20/150\n",
      "691/691 [==============================] - 0s 171us/step - loss: 0.5697 - acc: 0.7004\n",
      "Epoch 21/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.5697 - acc: 0.7135\n",
      "Epoch 22/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.5822 - acc: 0.7077\n",
      "Epoch 23/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.5666 - acc: 0.7192\n",
      "Epoch 24/150\n",
      "691/691 [==============================] - 0s 148us/step - loss: 0.5619 - acc: 0.7164\n",
      "Epoch 25/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.5633 - acc: 0.7221\n",
      "Epoch 26/150\n",
      "691/691 [==============================] - 0s 146us/step - loss: 0.5574 - acc: 0.7250\n",
      "Epoch 27/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.5623 - acc: 0.7178\n",
      "Epoch 28/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.5667 - acc: 0.7033\n",
      "Epoch 29/150\n",
      "691/691 [==============================] - 0s 146us/step - loss: 0.5574 - acc: 0.7323\n",
      "Epoch 30/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.5461 - acc: 0.7323\n",
      "Epoch 31/150\n",
      "691/691 [==============================] - 0s 148us/step - loss: 0.5571 - acc: 0.7164\n",
      "Epoch 32/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.5479 - acc: 0.7323\n",
      "Epoch 33/150\n",
      "691/691 [==============================] - 0s 146us/step - loss: 0.5488 - acc: 0.7337\n",
      "Epoch 34/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.5483 - acc: 0.7236\n",
      "Epoch 35/150\n",
      "691/691 [==============================] - 0s 164us/step - loss: 0.5438 - acc: 0.7395\n",
      "Epoch 36/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.5479 - acc: 0.7294\n",
      "Epoch 37/150\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.5460 - acc: 0.7395\n",
      "Epoch 38/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.5501 - acc: 0.7192\n",
      "Epoch 39/150\n",
      "691/691 [==============================] - 0s 148us/step - loss: 0.5444 - acc: 0.7352\n",
      "Epoch 40/150\n",
      "691/691 [==============================] - ETA: 0s - loss: 0.5364 - acc: 0.729 - 0s 177us/step - loss: 0.5385 - acc: 0.7308\n",
      "Epoch 41/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.5408 - acc: 0.7366\n",
      "Epoch 42/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.5397 - acc: 0.7467\n",
      "Epoch 43/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.5346 - acc: 0.7265\n",
      "Epoch 44/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.5390 - acc: 0.7410\n",
      "Epoch 45/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.5300 - acc: 0.7438\n",
      "Epoch 46/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.5287 - acc: 0.7424\n",
      "Epoch 47/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.5310 - acc: 0.7410\n",
      "Epoch 48/150\n",
      "691/691 [==============================] - 0s 148us/step - loss: 0.5279 - acc: 0.7525\n",
      "Epoch 49/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.5279 - acc: 0.7352\n",
      "Epoch 50/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.5236 - acc: 0.7410\n",
      "Epoch 51/150\n",
      "691/691 [==============================] - 0s 148us/step - loss: 0.5222 - acc: 0.7540\n",
      "Epoch 52/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.5173 - acc: 0.7467\n",
      "Epoch 53/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.5262 - acc: 0.7352\n",
      "Epoch 54/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.5221 - acc: 0.7540\n",
      "Epoch 55/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.5244 - acc: 0.7482\n",
      "Epoch 56/150\n",
      "691/691 [==============================] - 0s 148us/step - loss: 0.5242 - acc: 0.7482\n",
      "Epoch 57/150\n",
      "691/691 [==============================] - 0s 151us/step - loss: 0.5204 - acc: 0.7554\n",
      "Epoch 58/150\n",
      "691/691 [==============================] - 0s 146us/step - loss: 0.5192 - acc: 0.7525\n",
      "Epoch 59/150\n",
      "691/691 [==============================] - 0s 171us/step - loss: 0.5174 - acc: 0.7482\n",
      "Epoch 60/150\n",
      "691/691 [==============================] - 0s 149us/step - loss: 0.5182 - acc: 0.7453\n",
      "Epoch 61/150\n",
      "691/691 [==============================] - 0s 165us/step - loss: 0.5109 - acc: 0.7641\n",
      "Epoch 62/150\n",
      "691/691 [==============================] - 0s 164us/step - loss: 0.5114 - acc: 0.7569\n",
      "Epoch 63/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.5163 - acc: 0.7438\n",
      "Epoch 64/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.5121 - acc: 0.7583\n",
      "Epoch 65/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.5110 - acc: 0.7583\n",
      "Epoch 66/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.5091 - acc: 0.7453\n",
      "Epoch 67/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.5026 - acc: 0.7569\n",
      "Epoch 68/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.5111 - acc: 0.7554\n",
      "Epoch 69/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.5071 - acc: 0.7496\n",
      "Epoch 70/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.5057 - acc: 0.7569\n",
      "Epoch 71/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.5078 - acc: 0.7569\n",
      "Epoch 72/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.5042 - acc: 0.7554\n",
      "Epoch 73/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.5106 - acc: 0.7496\n",
      "Epoch 74/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.5008 - acc: 0.7656\n",
      "Epoch 75/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "691/691 [==============================] - 0s 145us/step - loss: 0.5033 - acc: 0.7612\n",
      "Epoch 76/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.5165 - acc: 0.7554\n",
      "Epoch 77/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.5013 - acc: 0.7641\n",
      "Epoch 78/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.5014 - acc: 0.7641\n",
      "Epoch 79/150\n",
      "691/691 [==============================] - 0s 148us/step - loss: 0.4943 - acc: 0.7656\n",
      "Epoch 80/150\n",
      "691/691 [==============================] - 0s 148us/step - loss: 0.4949 - acc: 0.7656\n",
      "Epoch 81/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.4980 - acc: 0.7656\n",
      "Epoch 82/150\n",
      "691/691 [==============================] - 0s 148us/step - loss: 0.4974 - acc: 0.7583\n",
      "Epoch 83/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.4927 - acc: 0.7728\n",
      "Epoch 84/150\n",
      "691/691 [==============================] - 0s 146us/step - loss: 0.4930 - acc: 0.7713\n",
      "Epoch 85/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.4970 - acc: 0.7713\n",
      "Epoch 86/150\n",
      "691/691 [==============================] - 0s 148us/step - loss: 0.4936 - acc: 0.7685\n",
      "Epoch 87/150\n",
      "691/691 [==============================] - 0s 146us/step - loss: 0.4920 - acc: 0.7757\n",
      "Epoch 88/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.4910 - acc: 0.7699\n",
      "Epoch 89/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.4915 - acc: 0.7612\n",
      "Epoch 90/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.4890 - acc: 0.7728\n",
      "Epoch 91/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.4895 - acc: 0.7540\n",
      "Epoch 92/150\n",
      "691/691 [==============================] - 0s 146us/step - loss: 0.4868 - acc: 0.7685\n",
      "Epoch 93/150\n",
      "691/691 [==============================] - 0s 146us/step - loss: 0.4852 - acc: 0.7670\n",
      "Epoch 94/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.4938 - acc: 0.7656\n",
      "Epoch 95/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4833 - acc: 0.7713\n",
      "Epoch 96/150\n",
      "691/691 [==============================] - 0s 146us/step - loss: 0.4884 - acc: 0.7583\n",
      "Epoch 97/150\n",
      "691/691 [==============================] - 0s 148us/step - loss: 0.4883 - acc: 0.7713\n",
      "Epoch 98/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.4895 - acc: 0.7699 0s - loss: 0.4544 - acc: 0.787\n",
      "Epoch 99/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.4949 - acc: 0.7670\n",
      "Epoch 100/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.4846 - acc: 0.7583\n",
      "Epoch 101/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.4796 - acc: 0.7728\n",
      "Epoch 102/150\n",
      "691/691 [==============================] - 0s 169us/step - loss: 0.4780 - acc: 0.7771 0s - loss: 0.4613 - acc: 0.78\n",
      "Epoch 103/150\n",
      "691/691 [==============================] - 0s 149us/step - loss: 0.4792 - acc: 0.7699\n",
      "Epoch 104/150\n",
      "691/691 [==============================] - 0s 167us/step - loss: 0.4834 - acc: 0.7699\n",
      "Epoch 105/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.4800 - acc: 0.7742\n",
      "Epoch 106/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4744 - acc: 0.7844\n",
      "Epoch 107/150\n",
      "691/691 [==============================] - 0s 168us/step - loss: 0.4734 - acc: 0.7829\n",
      "Epoch 108/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.4805 - acc: 0.7800\n",
      "Epoch 109/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.4771 - acc: 0.7800\n",
      "Epoch 110/150\n",
      "691/691 [==============================] - 0s 146us/step - loss: 0.4765 - acc: 0.7699\n",
      "Epoch 111/150\n",
      "691/691 [==============================] - 0s 148us/step - loss: 0.4745 - acc: 0.7771\n",
      "Epoch 112/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.4745 - acc: 0.7627\n",
      "Epoch 113/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.4671 - acc: 0.7829\n",
      "Epoch 114/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.4823 - acc: 0.7540\n",
      "Epoch 115/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.4656 - acc: 0.7873\n",
      "Epoch 116/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.4870 - acc: 0.7612\n",
      "Epoch 117/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.4641 - acc: 0.7786\n",
      "Epoch 118/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4623 - acc: 0.7858\n",
      "Epoch 119/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.4644 - acc: 0.7815\n",
      "Epoch 120/150\n",
      "691/691 [==============================] - 0s 146us/step - loss: 0.4660 - acc: 0.7713\n",
      "Epoch 121/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.4662 - acc: 0.7800\n",
      "Epoch 122/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4640 - acc: 0.7931\n",
      "Epoch 123/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.4585 - acc: 0.7887\n",
      "Epoch 124/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4626 - acc: 0.7800\n",
      "Epoch 125/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.4655 - acc: 0.7757\n",
      "Epoch 126/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.4589 - acc: 0.7902\n",
      "Epoch 127/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.4667 - acc: 0.7887\n",
      "Epoch 128/150\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.4695 - acc: 0.7728\n",
      "Epoch 129/150\n",
      "691/691 [==============================] - 0s 148us/step - loss: 0.4761 - acc: 0.7728\n",
      "Epoch 130/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.4578 - acc: 0.7771\n",
      "Epoch 131/150\n",
      "691/691 [==============================] - 0s 175us/step - loss: 0.4554 - acc: 0.7815\n",
      "Epoch 132/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.4587 - acc: 0.7873\n",
      "Epoch 133/150\n",
      "691/691 [==============================] - 0s 146us/step - loss: 0.4549 - acc: 0.7757\n",
      "Epoch 134/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.4563 - acc: 0.7829\n",
      "Epoch 135/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.4510 - acc: 0.7887\n",
      "Epoch 136/150\n",
      "691/691 [==============================] - 0s 148us/step - loss: 0.4540 - acc: 0.7815\n",
      "Epoch 137/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.4560 - acc: 0.7685\n",
      "Epoch 138/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.4544 - acc: 0.7786\n",
      "Epoch 139/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.4538 - acc: 0.7815\n",
      "Epoch 140/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.4506 - acc: 0.7742\n",
      "Epoch 141/150\n",
      "691/691 [==============================] - 0s 148us/step - loss: 0.4579 - acc: 0.7902\n",
      "Epoch 142/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4536 - acc: 0.7757\n",
      "Epoch 143/150\n",
      "691/691 [==============================] - 0s 148us/step - loss: 0.4494 - acc: 0.7873\n",
      "Epoch 144/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4485 - acc: 0.7829\n",
      "Epoch 145/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.4459 - acc: 0.7887\n",
      "Epoch 146/150\n",
      "691/691 [==============================] - 0s 182us/step - loss: 0.4651 - acc: 0.7728\n",
      "Epoch 147/150\n",
      "691/691 [==============================] - ETA: 0s - loss: 0.4697 - acc: 0.761 - 0s 145us/step - loss: 0.4537 - acc: 0.7699\n",
      "Epoch 148/150\n",
      "691/691 [==============================] - 0s 151us/step - loss: 0.4531 - acc: 0.7800\n",
      "Epoch 149/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.4594 - acc: 0.7873\n",
      "Epoch 150/150\n",
      "691/691 [==============================] - 0s 178us/step - loss: 0.4532 - acc: 0.7916 0s - loss: 0.4707 - acc: 0.76\n",
      "77/77 [==============================] - 0s 4ms/step\n",
      "Epoch 1/150\n",
      "691/691 [==============================] - 1s 1ms/step - loss: 0.6772 - acc: 0.6339\n",
      "Epoch 2/150\n",
      "691/691 [==============================] - 0s 177us/step - loss: 0.6602 - acc: 0.6512 0s - loss: 0.6621 - acc: 0.647\n",
      "Epoch 3/150\n",
      "691/691 [==============================] - 0s 200us/step - loss: 0.6513 - acc: 0.6498\n",
      "Epoch 4/150\n",
      "691/691 [==============================] - 0s 148us/step - loss: 0.6443 - acc: 0.6527\n",
      "Epoch 5/150\n",
      "691/691 [==============================] - 0s 181us/step - loss: 0.6361 - acc: 0.6541\n",
      "Epoch 6/150\n",
      "691/691 [==============================] - 0s 154us/step - loss: 0.6274 - acc: 0.6512\n",
      "Epoch 7/150\n",
      "691/691 [==============================] - 0s 169us/step - loss: 0.6146 - acc: 0.6715\n",
      "Epoch 8/150\n",
      "691/691 [==============================] - 0s 149us/step - loss: 0.6070 - acc: 0.6773\n",
      "Epoch 9/150\n",
      "691/691 [==============================] - 0s 146us/step - loss: 0.6023 - acc: 0.6758\n",
      "Epoch 10/150\n",
      "691/691 [==============================] - 0s 169us/step - loss: 0.5964 - acc: 0.6787\n",
      "Epoch 11/150\n",
      "691/691 [==============================] - 0s 185us/step - loss: 0.5997 - acc: 0.6802\n",
      "Epoch 12/150\n",
      "691/691 [==============================] - 0s 148us/step - loss: 0.5973 - acc: 0.6715\n",
      "Epoch 13/150\n",
      "691/691 [==============================] - 0s 146us/step - loss: 0.5924 - acc: 0.7062\n",
      "Epoch 14/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.5858 - acc: 0.7164\n",
      "Epoch 15/150\n",
      "691/691 [==============================] - 0s 168us/step - loss: 0.5869 - acc: 0.7236\n",
      "Epoch 16/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.5946 - acc: 0.7149\n",
      "Epoch 17/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.5779 - acc: 0.7250\n",
      "Epoch 18/150\n",
      "691/691 [==============================] - 0s 146us/step - loss: 0.5758 - acc: 0.7135\n",
      "Epoch 19/150\n",
      "691/691 [==============================] - 0s 151us/step - loss: 0.5777 - acc: 0.7221\n",
      "Epoch 20/150\n",
      "691/691 [==============================] - 0s 146us/step - loss: 0.5812 - acc: 0.7048\n",
      "Epoch 21/150\n",
      "691/691 [==============================] - 0s 146us/step - loss: 0.5710 - acc: 0.7236\n",
      "Epoch 22/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.5717 - acc: 0.7019\n",
      "Epoch 23/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.5673 - acc: 0.7265\n",
      "Epoch 24/150\n",
      "691/691 [==============================] - 0s 149us/step - loss: 0.5700 - acc: 0.7323\n",
      "Epoch 25/150\n",
      "691/691 [==============================] - 0s 149us/step - loss: 0.5687 - acc: 0.7178 0s - loss: 0.5456 - acc: 0.723\n",
      "Epoch 26/150\n",
      "691/691 [==============================] - 0s 148us/step - loss: 0.5570 - acc: 0.7308\n",
      "Epoch 27/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.5682 - acc: 0.7091\n",
      "Epoch 28/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.5594 - acc: 0.7279\n",
      "Epoch 29/150\n",
      "691/691 [==============================] - 0s 165us/step - loss: 0.5577 - acc: 0.7323\n",
      "Epoch 30/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.5697 - acc: 0.7294\n",
      "Epoch 31/150\n",
      "691/691 [==============================] - 0s 146us/step - loss: 0.5665 - acc: 0.7236\n",
      "Epoch 32/150\n",
      "691/691 [==============================] - 0s 149us/step - loss: 0.5582 - acc: 0.7366\n",
      "Epoch 33/150\n",
      "691/691 [==============================] - 0s 175us/step - loss: 0.5552 - acc: 0.7192\n",
      "Epoch 34/150\n",
      "691/691 [==============================] - 0s 148us/step - loss: 0.5556 - acc: 0.7395\n",
      "Epoch 35/150\n",
      "691/691 [==============================] - 0s 180us/step - loss: 0.5478 - acc: 0.7337\n",
      "Epoch 36/150\n",
      "691/691 [==============================] - 0s 164us/step - loss: 0.5501 - acc: 0.7265\n",
      "Epoch 37/150\n",
      "691/691 [==============================] - 0s 171us/step - loss: 0.5511 - acc: 0.7381\n",
      "Epoch 38/150\n",
      "691/691 [==============================] - 0s 177us/step - loss: 0.5505 - acc: 0.7438\n",
      "Epoch 39/150\n",
      "691/691 [==============================] - 0s 161us/step - loss: 0.5450 - acc: 0.7337 0s - loss: 0.5174 - acc: 0.73\n",
      "Epoch 40/150\n",
      "691/691 [==============================] - 0s 168us/step - loss: 0.5426 - acc: 0.7366\n",
      "Epoch 41/150\n",
      "691/691 [==============================] - 0s 146us/step - loss: 0.5522 - acc: 0.7250\n",
      "Epoch 42/150\n",
      "691/691 [==============================] - 0s 162us/step - loss: 0.5505 - acc: 0.7352\n",
      "Epoch 43/150\n",
      "691/691 [==============================] - 0s 148us/step - loss: 0.5423 - acc: 0.7337\n",
      "Epoch 44/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.5417 - acc: 0.7424\n",
      "Epoch 45/150\n",
      "691/691 [==============================] - 0s 169us/step - loss: 0.5440 - acc: 0.7323\n",
      "Epoch 46/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.5384 - acc: 0.7352\n",
      "Epoch 47/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.5354 - acc: 0.7279\n",
      "Epoch 48/150\n",
      "691/691 [==============================] - 0s 148us/step - loss: 0.5401 - acc: 0.7381\n",
      "Epoch 49/150\n",
      "691/691 [==============================] - 0s 149us/step - loss: 0.5331 - acc: 0.7467\n",
      "Epoch 50/150\n",
      "691/691 [==============================] - 0s 149us/step - loss: 0.5263 - acc: 0.7467\n",
      "Epoch 51/150\n",
      "691/691 [==============================] - 0s 180us/step - loss: 0.5299 - acc: 0.7482\n",
      "Epoch 52/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.5298 - acc: 0.7511\n",
      "Epoch 53/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.5335 - acc: 0.7294\n",
      "Epoch 54/150\n",
      "691/691 [==============================] - ETA: 0s - loss: 0.5243 - acc: 0.745 - 0s 168us/step - loss: 0.5269 - acc: 0.7410\n",
      "Epoch 55/150\n",
      "691/691 [==============================] - 0s 148us/step - loss: 0.5262 - acc: 0.7366\n",
      "Epoch 56/150\n",
      "691/691 [==============================] - 0s 146us/step - loss: 0.5235 - acc: 0.7453\n",
      "Epoch 57/150\n",
      "691/691 [==============================] - 0s 165us/step - loss: 0.5238 - acc: 0.7366\n",
      "Epoch 58/150\n",
      "691/691 [==============================] - 0s 177us/step - loss: 0.5313 - acc: 0.7366\n",
      "Epoch 59/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.5199 - acc: 0.7583\n",
      "Epoch 60/150\n",
      "691/691 [==============================] - 0s 148us/step - loss: 0.5261 - acc: 0.7554\n",
      "Epoch 61/150\n",
      "691/691 [==============================] - 0s 148us/step - loss: 0.5153 - acc: 0.7540\n",
      "Epoch 62/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.5218 - acc: 0.7323\n",
      "Epoch 63/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.5125 - acc: 0.7554\n",
      "Epoch 64/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.5305 - acc: 0.7540\n",
      "Epoch 65/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.5166 - acc: 0.7410\n",
      "Epoch 66/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.5136 - acc: 0.7612\n",
      "Epoch 67/150\n",
      "691/691 [==============================] - 0s 168us/step - loss: 0.5185 - acc: 0.7438\n",
      "Epoch 68/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.5153 - acc: 0.7467\n",
      "Epoch 69/150\n",
      "691/691 [==============================] - 0s 264us/step - loss: 0.5205 - acc: 0.7540\n",
      "Epoch 70/150\n",
      "691/691 [==============================] - 0s 219us/step - loss: 0.5090 - acc: 0.7569\n",
      "Epoch 71/150\n",
      "691/691 [==============================] - 0s 149us/step - loss: 0.5150 - acc: 0.7598\n",
      "Epoch 72/150\n",
      "691/691 [==============================] - 0s 216us/step - loss: 0.5169 - acc: 0.7685\n",
      "Epoch 73/150\n",
      "691/691 [==============================] - 0s 224us/step - loss: 0.5124 - acc: 0.7525\n",
      "Epoch 74/150\n",
      "691/691 [==============================] - 0s 187us/step - loss: 0.5115 - acc: 0.7554\n",
      "Epoch 75/150\n",
      "691/691 [==============================] - 0s 146us/step - loss: 0.5071 - acc: 0.7641\n",
      "Epoch 76/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.5132 - acc: 0.7467\n",
      "Epoch 77/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.5146 - acc: 0.7525\n",
      "Epoch 78/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.5100 - acc: 0.7627\n",
      "Epoch 79/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.5047 - acc: 0.7453\n",
      "Epoch 80/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.5055 - acc: 0.7583\n",
      "Epoch 81/150\n",
      "691/691 [==============================] - 0s 148us/step - loss: 0.4968 - acc: 0.7757\n",
      "Epoch 82/150\n",
      "691/691 [==============================] - 0s 148us/step - loss: 0.5087 - acc: 0.7612 0s - loss: 0.4926 - acc: 0.767\n",
      "Epoch 83/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.5004 - acc: 0.7598\n",
      "Epoch 84/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.5081 - acc: 0.7656\n",
      "Epoch 85/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.5023 - acc: 0.7554\n",
      "Epoch 86/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "691/691 [==============================] - 0s 143us/step - loss: 0.4967 - acc: 0.7627\n",
      "Epoch 87/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4941 - acc: 0.7627\n",
      "Epoch 88/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.4936 - acc: 0.7656\n",
      "Epoch 89/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.4996 - acc: 0.7670\n",
      "Epoch 90/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.4881 - acc: 0.7496\n",
      "Epoch 91/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.4973 - acc: 0.7525\n",
      "Epoch 92/150\n",
      "691/691 [==============================] - 0s 146us/step - loss: 0.4949 - acc: 0.7728\n",
      "Epoch 93/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4997 - acc: 0.7569\n",
      "Epoch 94/150\n",
      "691/691 [==============================] - 0s 148us/step - loss: 0.4972 - acc: 0.7627\n",
      "Epoch 95/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4994 - acc: 0.7612\n",
      "Epoch 96/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.5025 - acc: 0.7540\n",
      "Epoch 97/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.5030 - acc: 0.7598\n",
      "Epoch 98/150\n",
      "691/691 [==============================] - 0s 146us/step - loss: 0.4874 - acc: 0.7656\n",
      "Epoch 99/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.4935 - acc: 0.7612\n",
      "Epoch 100/150\n",
      "691/691 [==============================] - 0s 146us/step - loss: 0.4883 - acc: 0.7685\n",
      "Epoch 101/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.4934 - acc: 0.7656\n",
      "Epoch 102/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.4901 - acc: 0.7786\n",
      "Epoch 103/150\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.4905 - acc: 0.7627\n",
      "Epoch 104/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.4780 - acc: 0.7771\n",
      "Epoch 105/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4796 - acc: 0.7757\n",
      "Epoch 106/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4797 - acc: 0.7829\n",
      "Epoch 107/150\n",
      "691/691 [==============================] - 0s 146us/step - loss: 0.4834 - acc: 0.7641\n",
      "Epoch 108/150\n",
      "691/691 [==============================] - 0s 146us/step - loss: 0.4873 - acc: 0.7656\n",
      "Epoch 109/150\n",
      "691/691 [==============================] - 0s 269us/step - loss: 0.4778 - acc: 0.7771\n",
      "Epoch 110/150\n",
      "691/691 [==============================] - 0s 188us/step - loss: 0.4730 - acc: 0.7757\n",
      "Epoch 111/150\n",
      "691/691 [==============================] - 0s 169us/step - loss: 0.4852 - acc: 0.7670\n",
      "Epoch 112/150\n",
      "691/691 [==============================] - 0s 169us/step - loss: 0.4724 - acc: 0.7815 0s - loss: 0.4878 - acc: 0.76\n",
      "Epoch 113/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.4805 - acc: 0.7728\n",
      "Epoch 114/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4779 - acc: 0.7612\n",
      "Epoch 115/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.4738 - acc: 0.7815\n",
      "Epoch 116/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.4805 - acc: 0.7641\n",
      "Epoch 117/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.4802 - acc: 0.7583\n",
      "Epoch 118/150\n",
      "691/691 [==============================] - 0s 162us/step - loss: 0.4806 - acc: 0.7656\n",
      "Epoch 119/150\n",
      "691/691 [==============================] - 0s 209us/step - loss: 0.4737 - acc: 0.7699\n",
      "Epoch 120/150\n",
      "691/691 [==============================] - 0s 191us/step - loss: 0.4705 - acc: 0.7685\n",
      "Epoch 121/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.4801 - acc: 0.7728\n",
      "Epoch 122/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.4783 - acc: 0.7728\n",
      "Epoch 123/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4828 - acc: 0.7612\n",
      "Epoch 124/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.4696 - acc: 0.7728 0s - loss: 0.4835 - acc: 0.771\n",
      "Epoch 125/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.4663 - acc: 0.7829\n",
      "Epoch 126/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4753 - acc: 0.7699\n",
      "Epoch 127/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4712 - acc: 0.7771\n",
      "Epoch 128/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4756 - acc: 0.7699\n",
      "Epoch 129/150\n",
      "691/691 [==============================] - 0s 152us/step - loss: 0.4705 - acc: 0.7757\n",
      "Epoch 130/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4836 - acc: 0.7641\n",
      "Epoch 131/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.4668 - acc: 0.7829\n",
      "Epoch 132/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.4802 - acc: 0.7742\n",
      "Epoch 133/150\n",
      "691/691 [==============================] - 0s 146us/step - loss: 0.4697 - acc: 0.7540\n",
      "Epoch 134/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.4811 - acc: 0.7699\n",
      "Epoch 135/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.4695 - acc: 0.7656\n",
      "Epoch 136/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.4763 - acc: 0.7641\n",
      "Epoch 137/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.4645 - acc: 0.7685\n",
      "Epoch 138/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.4659 - acc: 0.7800\n",
      "Epoch 139/150\n",
      "691/691 [==============================] - 0s 171us/step - loss: 0.4742 - acc: 0.7771\n",
      "Epoch 140/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.4646 - acc: 0.7742\n",
      "Epoch 141/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4590 - acc: 0.7771\n",
      "Epoch 142/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.4692 - acc: 0.7786\n",
      "Epoch 143/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4635 - acc: 0.7786\n",
      "Epoch 144/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4578 - acc: 0.7742\n",
      "Epoch 145/150\n",
      "691/691 [==============================] - 0s 146us/step - loss: 0.4535 - acc: 0.7800\n",
      "Epoch 146/150\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.4588 - acc: 0.7771\n",
      "Epoch 147/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.4567 - acc: 0.7656\n",
      "Epoch 148/150\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.4618 - acc: 0.7800\n",
      "Epoch 149/150\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.4669 - acc: 0.7627\n",
      "Epoch 150/150\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4664 - acc: 0.7612\n",
      "77/77 [==============================] - 0s 4ms/step\n",
      "Epoch 1/150\n",
      "692/692 [==============================] - 1s 2ms/step - loss: 0.6807 - acc: 0.6460\n",
      "Epoch 2/150\n",
      "692/692 [==============================] - 0s 146us/step - loss: 0.6642 - acc: 0.6503\n",
      "Epoch 3/150\n",
      "692/692 [==============================] - 0s 166us/step - loss: 0.6565 - acc: 0.6517\n",
      "Epoch 4/150\n",
      "692/692 [==============================] - 0s 149us/step - loss: 0.6501 - acc: 0.6532\n",
      "Epoch 5/150\n",
      "692/692 [==============================] - 0s 148us/step - loss: 0.6419 - acc: 0.6517 0s - loss: 0.6512 - acc: 0.631\n",
      "Epoch 6/150\n",
      "692/692 [==============================] - 0s 143us/step - loss: 0.6329 - acc: 0.6532\n",
      "Epoch 7/150\n",
      "692/692 [==============================] - 0s 142us/step - loss: 0.6174 - acc: 0.6806\n",
      "Epoch 8/150\n",
      "692/692 [==============================] - 0s 143us/step - loss: 0.6148 - acc: 0.6734\n",
      "Epoch 9/150\n",
      "692/692 [==============================] - 0s 205us/step - loss: 0.6035 - acc: 0.6720 0s - loss: 0.6031 - acc: 0.68\n",
      "Epoch 10/150\n",
      "692/692 [==============================] - 0s 243us/step - loss: 0.5969 - acc: 0.6922\n",
      "Epoch 11/150\n",
      "692/692 [==============================] - 0s 142us/step - loss: 0.5896 - acc: 0.6879\n",
      "Epoch 12/150\n",
      "692/692 [==============================] - 0s 163us/step - loss: 0.5941 - acc: 0.6792\n",
      "Epoch 13/150\n",
      "692/692 [==============================] - 0s 140us/step - loss: 0.5874 - acc: 0.6922\n",
      "Epoch 14/150\n",
      "692/692 [==============================] - 0s 143us/step - loss: 0.5851 - acc: 0.6821\n",
      "Epoch 15/150\n",
      "692/692 [==============================] - 0s 145us/step - loss: 0.5878 - acc: 0.6835\n",
      "Epoch 16/150\n",
      "692/692 [==============================] - 0s 145us/step - loss: 0.5803 - acc: 0.6980\n",
      "Epoch 17/150\n",
      "692/692 [==============================] - 0s 145us/step - loss: 0.5933 - acc: 0.6806\n",
      "Epoch 18/150\n",
      "692/692 [==============================] - 0s 148us/step - loss: 0.5778 - acc: 0.6980\n",
      "Epoch 19/150\n",
      "692/692 [==============================] - 0s 142us/step - loss: 0.5765 - acc: 0.7124\n",
      "Epoch 20/150\n",
      "692/692 [==============================] - 0s 143us/step - loss: 0.5748 - acc: 0.6980\n",
      "Epoch 21/150\n",
      "692/692 [==============================] - 0s 149us/step - loss: 0.5722 - acc: 0.7038\n",
      "Epoch 22/150\n",
      "692/692 [==============================] - 0s 145us/step - loss: 0.5733 - acc: 0.6980\n",
      "Epoch 23/150\n",
      "692/692 [==============================] - 0s 146us/step - loss: 0.5718 - acc: 0.7009\n",
      "Epoch 24/150\n",
      "692/692 [==============================] - 0s 163us/step - loss: 0.5706 - acc: 0.7110\n",
      "Epoch 25/150\n",
      "692/692 [==============================] - 0s 149us/step - loss: 0.5757 - acc: 0.6936 0s - loss: 0.5727 - acc: 0.694\n",
      "Epoch 26/150\n",
      "692/692 [==============================] - 0s 142us/step - loss: 0.5725 - acc: 0.7066\n",
      "Epoch 27/150\n",
      "692/692 [==============================] - 0s 143us/step - loss: 0.5671 - acc: 0.7153\n",
      "Epoch 28/150\n",
      "692/692 [==============================] - 0s 149us/step - loss: 0.5672 - acc: 0.7095\n",
      "Epoch 29/150\n",
      "692/692 [==============================] - 0s 142us/step - loss: 0.5677 - acc: 0.6951\n",
      "Epoch 30/150\n",
      "692/692 [==============================] - 0s 143us/step - loss: 0.5675 - acc: 0.7066\n",
      "Epoch 31/150\n",
      "692/692 [==============================] - 0s 143us/step - loss: 0.5674 - acc: 0.7066\n",
      "Epoch 32/150\n",
      "692/692 [==============================] - 0s 145us/step - loss: 0.5622 - acc: 0.7139\n",
      "Epoch 33/150\n",
      "692/692 [==============================] - 0s 145us/step - loss: 0.5695 - acc: 0.7009\n",
      "Epoch 34/150\n",
      "692/692 [==============================] - 0s 145us/step - loss: 0.5624 - acc: 0.7139\n",
      "Epoch 35/150\n",
      "692/692 [==============================] - 0s 140us/step - loss: 0.5678 - acc: 0.7124\n",
      "Epoch 36/150\n",
      "692/692 [==============================] - 0s 143us/step - loss: 0.5626 - acc: 0.7153\n",
      "Epoch 37/150\n",
      "692/692 [==============================] - 0s 143us/step - loss: 0.5610 - acc: 0.7139\n",
      "Epoch 38/150\n",
      "692/692 [==============================] - 0s 142us/step - loss: 0.5539 - acc: 0.7182\n",
      "Epoch 39/150\n",
      "692/692 [==============================] - 0s 146us/step - loss: 0.5579 - acc: 0.7081\n",
      "Epoch 40/150\n",
      "692/692 [==============================] - 0s 260us/step - loss: 0.5593 - acc: 0.7197\n",
      "Epoch 41/150\n",
      "692/692 [==============================] - 0s 189us/step - loss: 0.5580 - acc: 0.7095\n",
      "Epoch 42/150\n",
      "692/692 [==============================] - 0s 220us/step - loss: 0.5585 - acc: 0.6980\n",
      "Epoch 43/150\n",
      "692/692 [==============================] - 0s 218us/step - loss: 0.5511 - acc: 0.7240\n",
      "Epoch 44/150\n",
      "692/692 [==============================] - 0s 220us/step - loss: 0.5594 - acc: 0.7211\n",
      "Epoch 45/150\n",
      "692/692 [==============================] - 0s 263us/step - loss: 0.5581 - acc: 0.7225\n",
      "Epoch 46/150\n",
      "692/692 [==============================] - 0s 211us/step - loss: 0.5495 - acc: 0.7283\n",
      "Epoch 47/150\n",
      "692/692 [==============================] - 0s 142us/step - loss: 0.5472 - acc: 0.7298\n",
      "Epoch 48/150\n",
      "692/692 [==============================] - 0s 146us/step - loss: 0.5510 - acc: 0.7413\n",
      "Epoch 49/150\n",
      "692/692 [==============================] - 0s 143us/step - loss: 0.5468 - acc: 0.7370\n",
      "Epoch 50/150\n",
      "692/692 [==============================] - 0s 142us/step - loss: 0.5535 - acc: 0.7182\n",
      "Epoch 51/150\n",
      "692/692 [==============================] - 0s 143us/step - loss: 0.5580 - acc: 0.7428\n",
      "Epoch 52/150\n",
      "692/692 [==============================] - 0s 146us/step - loss: 0.5430 - acc: 0.7355\n",
      "Epoch 53/150\n",
      "692/692 [==============================] - 0s 143us/step - loss: 0.5447 - acc: 0.7486\n",
      "Epoch 54/150\n",
      "692/692 [==============================] - 0s 143us/step - loss: 0.5413 - acc: 0.7341\n",
      "Epoch 55/150\n",
      "692/692 [==============================] - 0s 149us/step - loss: 0.5395 - acc: 0.7225\n",
      "Epoch 56/150\n",
      "692/692 [==============================] - 0s 269us/step - loss: 0.5378 - acc: 0.7399\n",
      "Epoch 57/150\n",
      "692/692 [==============================] - ETA: 0s - loss: 0.5235 - acc: 0.743 - 0s 210us/step - loss: 0.5399 - acc: 0.7355\n",
      "Epoch 58/150\n",
      "692/692 [==============================] - 0s 178us/step - loss: 0.5436 - acc: 0.7442\n",
      "Epoch 59/150\n",
      "692/692 [==============================] - 0s 146us/step - loss: 0.5414 - acc: 0.7399\n",
      "Epoch 60/150\n",
      "692/692 [==============================] - 0s 146us/step - loss: 0.5345 - acc: 0.7442\n",
      "Epoch 61/150\n",
      "692/692 [==============================] - 0s 142us/step - loss: 0.5359 - acc: 0.7370\n",
      "Epoch 62/150\n",
      "692/692 [==============================] - 0s 143us/step - loss: 0.5323 - acc: 0.7413\n",
      "Epoch 63/150\n",
      "692/692 [==============================] - 0s 146us/step - loss: 0.5379 - acc: 0.7442 0s - loss: 0.5149 - acc: 0.784\n",
      "Epoch 64/150\n",
      "692/692 [==============================] - 0s 145us/step - loss: 0.5366 - acc: 0.7370\n",
      "Epoch 65/150\n",
      "692/692 [==============================] - 0s 143us/step - loss: 0.5280 - acc: 0.7500\n",
      "Epoch 66/150\n",
      "692/692 [==============================] - 0s 146us/step - loss: 0.5351 - acc: 0.7514\n",
      "Epoch 67/150\n",
      "692/692 [==============================] - 0s 137us/step - loss: 0.5246 - acc: 0.7514\n",
      "Epoch 68/150\n",
      "692/692 [==============================] - 0s 143us/step - loss: 0.5270 - acc: 0.7500\n",
      "Epoch 69/150\n",
      "692/692 [==============================] - 0s 142us/step - loss: 0.5271 - acc: 0.7514\n",
      "Epoch 70/150\n",
      "692/692 [==============================] - 0s 207us/step - loss: 0.5232 - acc: 0.7355\n",
      "Epoch 71/150\n",
      "692/692 [==============================] - 0s 257us/step - loss: 0.5300 - acc: 0.7471\n",
      "Epoch 72/150\n",
      "692/692 [==============================] - 0s 205us/step - loss: 0.5281 - acc: 0.7500\n",
      "Epoch 73/150\n",
      "692/692 [==============================] - 0s 146us/step - loss: 0.5289 - acc: 0.7355\n",
      "Epoch 74/150\n",
      "692/692 [==============================] - 0s 149us/step - loss: 0.5175 - acc: 0.7645\n",
      "Epoch 75/150\n",
      "692/692 [==============================] - ETA: 0s - loss: 0.5155 - acc: 0.763 - 0s 255us/step - loss: 0.5168 - acc: 0.7630\n",
      "Epoch 76/150\n",
      "692/692 [==============================] - 0s 162us/step - loss: 0.5305 - acc: 0.7558\n",
      "Epoch 77/150\n",
      "692/692 [==============================] - 0s 174us/step - loss: 0.5237 - acc: 0.7471\n",
      "Epoch 78/150\n",
      "692/692 [==============================] - 0s 146us/step - loss: 0.5164 - acc: 0.7601\n",
      "Epoch 79/150\n",
      "692/692 [==============================] - 0s 140us/step - loss: 0.5170 - acc: 0.7558\n",
      "Epoch 80/150\n",
      "692/692 [==============================] - 0s 246us/step - loss: 0.5182 - acc: 0.7601\n",
      "Epoch 81/150\n",
      "692/692 [==============================] - 0s 191us/step - loss: 0.5106 - acc: 0.7645\n",
      "Epoch 82/150\n",
      "692/692 [==============================] - 0s 166us/step - loss: 0.5080 - acc: 0.7746\n",
      "Epoch 83/150\n",
      "692/692 [==============================] - 0s 148us/step - loss: 0.5091 - acc: 0.7587\n",
      "Epoch 84/150\n",
      "692/692 [==============================] - 0s 143us/step - loss: 0.5147 - acc: 0.7659\n",
      "Epoch 85/150\n",
      "692/692 [==============================] - 0s 142us/step - loss: 0.5123 - acc: 0.7717\n",
      "Epoch 86/150\n",
      "692/692 [==============================] - 0s 143us/step - loss: 0.5108 - acc: 0.7601\n",
      "Epoch 87/150\n",
      "692/692 [==============================] - 0s 166us/step - loss: 0.5027 - acc: 0.7731\n",
      "Epoch 88/150\n",
      "692/692 [==============================] - 0s 148us/step - loss: 0.5010 - acc: 0.7572\n",
      "Epoch 89/150\n",
      "692/692 [==============================] - 0s 142us/step - loss: 0.5042 - acc: 0.7673\n",
      "Epoch 90/150\n",
      "692/692 [==============================] - 0s 163us/step - loss: 0.4992 - acc: 0.7717\n",
      "Epoch 91/150\n",
      "692/692 [==============================] - 0s 145us/step - loss: 0.5047 - acc: 0.7717\n",
      "Epoch 92/150\n",
      "692/692 [==============================] - 0s 139us/step - loss: 0.4972 - acc: 0.7688\n",
      "Epoch 93/150\n",
      "692/692 [==============================] - 0s 142us/step - loss: 0.4989 - acc: 0.7659\n",
      "Epoch 94/150\n",
      "692/692 [==============================] - 0s 142us/step - loss: 0.4976 - acc: 0.7775\n",
      "Epoch 95/150\n",
      "692/692 [==============================] - 0s 148us/step - loss: 0.4975 - acc: 0.7731\n",
      "Epoch 96/150\n",
      "692/692 [==============================] - 0s 143us/step - loss: 0.4966 - acc: 0.7673\n",
      "Epoch 97/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "692/692 [==============================] - 0s 143us/step - loss: 0.5012 - acc: 0.7789\n",
      "Epoch 98/150\n",
      "692/692 [==============================] - 0s 142us/step - loss: 0.5015 - acc: 0.7731\n",
      "Epoch 99/150\n",
      "692/692 [==============================] - 0s 142us/step - loss: 0.4915 - acc: 0.7847\n",
      "Epoch 100/150\n",
      "692/692 [==============================] - 0s 145us/step - loss: 0.4918 - acc: 0.7702\n",
      "Epoch 101/150\n",
      "692/692 [==============================] - 0s 142us/step - loss: 0.4885 - acc: 0.7803\n",
      "Epoch 102/150\n",
      "692/692 [==============================] - 0s 143us/step - loss: 0.4925 - acc: 0.7746\n",
      "Epoch 103/150\n",
      "692/692 [==============================] - 0s 145us/step - loss: 0.4906 - acc: 0.7746\n",
      "Epoch 104/150\n",
      "692/692 [==============================] - 0s 143us/step - loss: 0.5011 - acc: 0.7645\n",
      "Epoch 105/150\n",
      "692/692 [==============================] - 0s 143us/step - loss: 0.4890 - acc: 0.7803\n",
      "Epoch 106/150\n",
      "692/692 [==============================] - 0s 149us/step - loss: 0.4823 - acc: 0.7861\n",
      "Epoch 107/150\n",
      "692/692 [==============================] - 0s 143us/step - loss: 0.4981 - acc: 0.7717\n",
      "Epoch 108/150\n",
      "692/692 [==============================] - 0s 143us/step - loss: 0.4908 - acc: 0.7659\n",
      "Epoch 109/150\n",
      "692/692 [==============================] - 0s 145us/step - loss: 0.4852 - acc: 0.7746\n",
      "Epoch 110/150\n",
      "692/692 [==============================] - 0s 143us/step - loss: 0.4881 - acc: 0.7760\n",
      "Epoch 111/150\n",
      "692/692 [==============================] - 0s 145us/step - loss: 0.4873 - acc: 0.7587\n",
      "Epoch 112/150\n",
      "692/692 [==============================] - 0s 143us/step - loss: 0.4807 - acc: 0.7717\n",
      "Epoch 113/150\n",
      "692/692 [==============================] - 0s 149us/step - loss: 0.4809 - acc: 0.7861\n",
      "Epoch 114/150\n",
      "692/692 [==============================] - 0s 139us/step - loss: 0.4759 - acc: 0.7905\n",
      "Epoch 115/150\n",
      "692/692 [==============================] - 0s 140us/step - loss: 0.4845 - acc: 0.7731\n",
      "Epoch 116/150\n",
      "692/692 [==============================] - 0s 136us/step - loss: 0.4794 - acc: 0.7731\n",
      "Epoch 117/150\n",
      "692/692 [==============================] - 0s 143us/step - loss: 0.4825 - acc: 0.7775\n",
      "Epoch 118/150\n",
      "692/692 [==============================] - 0s 143us/step - loss: 0.4777 - acc: 0.7847\n",
      "Epoch 119/150\n",
      "692/692 [==============================] - 0s 145us/step - loss: 0.4742 - acc: 0.7746\n",
      "Epoch 120/150\n",
      "692/692 [==============================] - 0s 145us/step - loss: 0.4917 - acc: 0.7601\n",
      "Epoch 121/150\n",
      "692/692 [==============================] - 0s 143us/step - loss: 0.4733 - acc: 0.7659\n",
      "Epoch 122/150\n",
      "692/692 [==============================] - 0s 145us/step - loss: 0.4920 - acc: 0.7702\n",
      "Epoch 123/150\n",
      "692/692 [==============================] - 0s 143us/step - loss: 0.4759 - acc: 0.7803\n",
      "Epoch 124/150\n",
      "692/692 [==============================] - 0s 142us/step - loss: 0.4751 - acc: 0.7861\n",
      "Epoch 125/150\n",
      "692/692 [==============================] - 0s 142us/step - loss: 0.4792 - acc: 0.7832\n",
      "Epoch 126/150\n",
      "692/692 [==============================] - 0s 145us/step - loss: 0.4801 - acc: 0.7847 0s - loss: 0.4925 - acc: 0.784\n",
      "Epoch 127/150\n",
      "692/692 [==============================] - 0s 148us/step - loss: 0.4743 - acc: 0.7688\n",
      "Epoch 128/150\n",
      "692/692 [==============================] - 0s 143us/step - loss: 0.4791 - acc: 0.7688\n",
      "Epoch 129/150\n",
      "692/692 [==============================] - 0s 142us/step - loss: 0.4758 - acc: 0.7876\n",
      "Epoch 130/150\n",
      "692/692 [==============================] - 0s 218us/step - loss: 0.4801 - acc: 0.7789\n",
      "Epoch 131/150\n",
      "692/692 [==============================] - 0s 207us/step - loss: 0.4719 - acc: 0.7861\n",
      "Epoch 132/150\n",
      "692/692 [==============================] - 0s 145us/step - loss: 0.4701 - acc: 0.7789\n",
      "Epoch 133/150\n",
      "692/692 [==============================] - 0s 168us/step - loss: 0.4708 - acc: 0.7746\n",
      "Epoch 134/150\n",
      "692/692 [==============================] - ETA: 0s - loss: 0.4848 - acc: 0.767 - 0s 146us/step - loss: 0.4699 - acc: 0.7760\n",
      "Epoch 135/150\n",
      "692/692 [==============================] - 0s 145us/step - loss: 0.4709 - acc: 0.7789\n",
      "Epoch 136/150\n",
      "692/692 [==============================] - 0s 172us/step - loss: 0.4764 - acc: 0.7702\n",
      "Epoch 137/150\n",
      "692/692 [==============================] - 0s 166us/step - loss: 0.4755 - acc: 0.7818 0s - loss: 0.4754 - acc: 0.780\n",
      "Epoch 138/150\n",
      "692/692 [==============================] - 0s 149us/step - loss: 0.4736 - acc: 0.7789\n",
      "Epoch 139/150\n",
      "692/692 [==============================] - 0s 166us/step - loss: 0.4659 - acc: 0.7789\n",
      "Epoch 140/150\n",
      "692/692 [==============================] - 0s 176us/step - loss: 0.4740 - acc: 0.7832\n",
      "Epoch 141/150\n",
      "692/692 [==============================] - 0s 169us/step - loss: 0.4748 - acc: 0.7746\n",
      "Epoch 142/150\n",
      "692/692 [==============================] - 0s 155us/step - loss: 0.4683 - acc: 0.7789\n",
      "Epoch 143/150\n",
      "692/692 [==============================] - 0s 169us/step - loss: 0.4825 - acc: 0.7702\n",
      "Epoch 144/150\n",
      "692/692 [==============================] - 0s 194us/step - loss: 0.4628 - acc: 0.7948\n",
      "Epoch 145/150\n",
      "692/692 [==============================] - 0s 171us/step - loss: 0.4666 - acc: 0.7847\n",
      "Epoch 146/150\n",
      "692/692 [==============================] - 0s 182us/step - loss: 0.4622 - acc: 0.7861\n",
      "Epoch 147/150\n",
      "692/692 [==============================] - 0s 194us/step - loss: 0.4655 - acc: 0.7905\n",
      "Epoch 148/150\n",
      "692/692 [==============================] - 0s 169us/step - loss: 0.4735 - acc: 0.7803\n",
      "Epoch 149/150\n",
      "692/692 [==============================] - 0s 253us/step - loss: 0.4680 - acc: 0.7789\n",
      "Epoch 150/150\n",
      "692/692 [==============================] - 0s 168us/step - loss: 0.4675 - acc: 0.7876\n",
      "76/76 [==============================] - 0s 5ms/step\n",
      "Epoch 1/150\n",
      "692/692 [==============================] - 1s 2ms/step - loss: 0.6830 - acc: 0.6387\n",
      "Epoch 2/150\n",
      "692/692 [==============================] - 0s 148us/step - loss: 0.6618 - acc: 0.6488\n",
      "Epoch 3/150\n",
      "692/692 [==============================] - 0s 145us/step - loss: 0.6532 - acc: 0.6575\n",
      "Epoch 4/150\n",
      "692/692 [==============================] - 0s 148us/step - loss: 0.6450 - acc: 0.6604\n",
      "Epoch 5/150\n",
      "692/692 [==============================] - 0s 150us/step - loss: 0.6333 - acc: 0.6575\n",
      "Epoch 6/150\n",
      "692/692 [==============================] - 0s 146us/step - loss: 0.6257 - acc: 0.6763 0s - loss: 0.6156 - acc: 0.687\n",
      "Epoch 7/150\n",
      "692/692 [==============================] - 0s 148us/step - loss: 0.6190 - acc: 0.6575 0s - loss: 0.6228 - acc: 0.665\n",
      "Epoch 8/150\n",
      "692/692 [==============================] - 0s 146us/step - loss: 0.6139 - acc: 0.6792\n",
      "Epoch 9/150\n",
      "692/692 [==============================] - 0s 188us/step - loss: 0.6031 - acc: 0.6705\n",
      "Epoch 10/150\n",
      "692/692 [==============================] - 0s 213us/step - loss: 0.6028 - acc: 0.6763\n",
      "Epoch 11/150\n",
      "692/692 [==============================] - 0s 145us/step - loss: 0.5950 - acc: 0.6835\n",
      "Epoch 12/150\n",
      "692/692 [==============================] - 0s 163us/step - loss: 0.5961 - acc: 0.6821\n",
      "Epoch 13/150\n",
      "692/692 [==============================] - 0s 148us/step - loss: 0.5921 - acc: 0.6922\n",
      "Epoch 14/150\n",
      "692/692 [==============================] - 0s 172us/step - loss: 0.5942 - acc: 0.6908\n",
      "Epoch 15/150\n",
      "692/692 [==============================] - 0s 163us/step - loss: 0.5862 - acc: 0.6951\n",
      "Epoch 16/150\n",
      "692/692 [==============================] - 0s 148us/step - loss: 0.5825 - acc: 0.6908\n",
      "Epoch 17/150\n",
      "692/692 [==============================] - 0s 145us/step - loss: 0.5865 - acc: 0.6965\n",
      "Epoch 18/150\n",
      "692/692 [==============================] - 0s 145us/step - loss: 0.5838 - acc: 0.6951\n",
      "Epoch 19/150\n",
      "692/692 [==============================] - 0s 181us/step - loss: 0.5794 - acc: 0.7038\n",
      "Epoch 20/150\n",
      "692/692 [==============================] - ETA: 0s - loss: 0.5621 - acc: 0.729 - 0s 146us/step - loss: 0.5793 - acc: 0.7153\n",
      "Epoch 21/150\n",
      "692/692 [==============================] - 0s 208us/step - loss: 0.5809 - acc: 0.6980\n",
      "Epoch 22/150\n",
      "692/692 [==============================] - 0s 204us/step - loss: 0.5774 - acc: 0.7009\n",
      "Epoch 23/150\n",
      "692/692 [==============================] - 0s 163us/step - loss: 0.5764 - acc: 0.7081\n",
      "Epoch 24/150\n",
      "692/692 [==============================] - 0s 149us/step - loss: 0.5722 - acc: 0.7038\n",
      "Epoch 25/150\n",
      "692/692 [==============================] - 0s 143us/step - loss: 0.5725 - acc: 0.7081\n",
      "Epoch 26/150\n",
      "692/692 [==============================] - 0s 165us/step - loss: 0.5703 - acc: 0.7066\n",
      "Epoch 27/150\n",
      "692/692 [==============================] - 0s 159us/step - loss: 0.5701 - acc: 0.7225\n",
      "Epoch 28/150\n",
      "692/692 [==============================] - 0s 165us/step - loss: 0.5763 - acc: 0.7066\n",
      "Epoch 29/150\n",
      "692/692 [==============================] - 0s 192us/step - loss: 0.5723 - acc: 0.7124\n",
      "Epoch 30/150\n",
      "692/692 [==============================] - 0s 178us/step - loss: 0.5718 - acc: 0.6893\n",
      "Epoch 31/150\n",
      "692/692 [==============================] - 0s 179us/step - loss: 0.5659 - acc: 0.7081\n",
      "Epoch 32/150\n",
      "692/692 [==============================] - 0s 188us/step - loss: 0.5712 - acc: 0.7038\n",
      "Epoch 33/150\n",
      "692/692 [==============================] - 0s 194us/step - loss: 0.5639 - acc: 0.7225\n",
      "Epoch 34/150\n",
      "692/692 [==============================] - 0s 285us/step - loss: 0.5641 - acc: 0.7211\n",
      "Epoch 35/150\n",
      "692/692 [==============================] - 0s 200us/step - loss: 0.5605 - acc: 0.7110\n",
      "Epoch 36/150\n",
      "692/692 [==============================] - 0s 202us/step - loss: 0.5631 - acc: 0.7110\n",
      "Epoch 37/150\n",
      "692/692 [==============================] - 0s 184us/step - loss: 0.5571 - acc: 0.7225\n",
      "Epoch 38/150\n",
      "692/692 [==============================] - 0s 187us/step - loss: 0.5560 - acc: 0.7124\n",
      "Epoch 39/150\n",
      "692/692 [==============================] - 0s 156us/step - loss: 0.5548 - acc: 0.7298\n",
      "Epoch 40/150\n",
      "692/692 [==============================] - 0s 149us/step - loss: 0.5574 - acc: 0.7399\n",
      "Epoch 41/150\n",
      "692/692 [==============================] - 0s 145us/step - loss: 0.5524 - acc: 0.7153\n",
      "Epoch 42/150\n",
      "692/692 [==============================] - 0s 149us/step - loss: 0.5619 - acc: 0.7341\n",
      "Epoch 43/150\n",
      "692/692 [==============================] - 0s 174us/step - loss: 0.5527 - acc: 0.7240\n",
      "Epoch 44/150\n",
      "692/692 [==============================] - 0s 201us/step - loss: 0.5529 - acc: 0.7269\n",
      "Epoch 45/150\n",
      "692/692 [==============================] - 0s 201us/step - loss: 0.5505 - acc: 0.7240\n",
      "Epoch 46/150\n",
      "692/692 [==============================] - 0s 178us/step - loss: 0.5477 - acc: 0.7254\n",
      "Epoch 47/150\n",
      "692/692 [==============================] - 0s 262us/step - loss: 0.5503 - acc: 0.7240 0s - loss: 0.5535 - acc: 0.722\n",
      "Epoch 48/150\n",
      "692/692 [==============================] - 0s 215us/step - loss: 0.5556 - acc: 0.7240\n",
      "Epoch 49/150\n",
      "692/692 [==============================] - 0s 181us/step - loss: 0.5459 - acc: 0.7168\n",
      "Epoch 50/150\n",
      "692/692 [==============================] - 0s 174us/step - loss: 0.5472 - acc: 0.7327\n",
      "Epoch 51/150\n",
      "692/692 [==============================] - 0s 166us/step - loss: 0.5431 - acc: 0.7225\n",
      "Epoch 52/150\n",
      "692/692 [==============================] - ETA: 0s - loss: 0.5469 - acc: 0.732 - 0s 176us/step - loss: 0.5471 - acc: 0.7298\n",
      "Epoch 53/150\n",
      "692/692 [==============================] - 0s 163us/step - loss: 0.5375 - acc: 0.7283\n",
      "Epoch 54/150\n",
      "692/692 [==============================] - 0s 149us/step - loss: 0.5475 - acc: 0.7312\n",
      "Epoch 55/150\n",
      "692/692 [==============================] - 0s 174us/step - loss: 0.5420 - acc: 0.7341\n",
      "Epoch 56/150\n",
      "692/692 [==============================] - 0s 185us/step - loss: 0.5342 - acc: 0.7442\n",
      "Epoch 57/150\n",
      "692/692 [==============================] - 0s 178us/step - loss: 0.5375 - acc: 0.7442\n",
      "Epoch 58/150\n",
      "692/692 [==============================] - 0s 194us/step - loss: 0.5433 - acc: 0.7225\n",
      "Epoch 59/150\n",
      "692/692 [==============================] - ETA: 0s - loss: 0.5260 - acc: 0.753 - 0s 200us/step - loss: 0.5346 - acc: 0.7471\n",
      "Epoch 60/150\n",
      "692/692 [==============================] - 0s 273us/step - loss: 0.5321 - acc: 0.7327\n",
      "Epoch 61/150\n",
      "692/692 [==============================] - 0s 202us/step - loss: 0.5315 - acc: 0.7486\n",
      "Epoch 62/150\n",
      "692/692 [==============================] - 0s 185us/step - loss: 0.5366 - acc: 0.7486\n",
      "Epoch 63/150\n",
      "692/692 [==============================] - 0s 168us/step - loss: 0.5305 - acc: 0.7442\n",
      "Epoch 64/150\n",
      "692/692 [==============================] - 0s 174us/step - loss: 0.5306 - acc: 0.7428\n",
      "Epoch 65/150\n",
      "692/692 [==============================] - 0s 163us/step - loss: 0.5276 - acc: 0.7471\n",
      "Epoch 66/150\n",
      "692/692 [==============================] - 0s 175us/step - loss: 0.5264 - acc: 0.7384\n",
      "Epoch 67/150\n",
      "692/692 [==============================] - 0s 169us/step - loss: 0.5307 - acc: 0.7457\n",
      "Epoch 68/150\n",
      "692/692 [==============================] - 0s 171us/step - loss: 0.5225 - acc: 0.7384\n",
      "Epoch 69/150\n",
      "692/692 [==============================] - 0s 172us/step - loss: 0.5296 - acc: 0.7269\n",
      "Epoch 70/150\n",
      "692/692 [==============================] - 0s 168us/step - loss: 0.5214 - acc: 0.7471\n",
      "Epoch 71/150\n",
      "692/692 [==============================] - 0s 149us/step - loss: 0.5184 - acc: 0.7601\n",
      "Epoch 72/150\n",
      "692/692 [==============================] - 0s 145us/step - loss: 0.5308 - acc: 0.7355\n",
      "Epoch 73/150\n",
      "692/692 [==============================] - 0s 149us/step - loss: 0.5227 - acc: 0.7399\n",
      "Epoch 74/150\n",
      "692/692 [==============================] - ETA: 0s - loss: 0.5316 - acc: 0.747 - 0s 179us/step - loss: 0.5286 - acc: 0.7486\n",
      "Epoch 75/150\n",
      "692/692 [==============================] - 0s 171us/step - loss: 0.5254 - acc: 0.7413\n",
      "Epoch 76/150\n",
      "692/692 [==============================] - ETA: 0s - loss: 0.5187 - acc: 0.747 - 0s 252us/step - loss: 0.5139 - acc: 0.7514\n",
      "Epoch 77/150\n",
      "692/692 [==============================] - 0s 242us/step - loss: 0.5145 - acc: 0.7558 0s - loss: 0.5071 - acc: 0.76\n",
      "Epoch 78/150\n",
      "692/692 [==============================] - 0s 259us/step - loss: 0.5161 - acc: 0.7543\n",
      "Epoch 79/150\n",
      "692/692 [==============================] - 0s 207us/step - loss: 0.5131 - acc: 0.7702\n",
      "Epoch 80/150\n",
      "692/692 [==============================] - 0s 182us/step - loss: 0.5108 - acc: 0.7486\n",
      "Epoch 81/150\n",
      "692/692 [==============================] - 0s 176us/step - loss: 0.5093 - acc: 0.7543\n",
      "Epoch 82/150\n",
      "692/692 [==============================] - 0s 159us/step - loss: 0.5078 - acc: 0.7630\n",
      "Epoch 83/150\n",
      "692/692 [==============================] - 0s 166us/step - loss: 0.5041 - acc: 0.7529\n",
      "Epoch 84/150\n",
      "692/692 [==============================] - 0s 146us/step - loss: 0.5114 - acc: 0.7384\n",
      "Epoch 85/150\n",
      "692/692 [==============================] - 0s 168us/step - loss: 0.5048 - acc: 0.7543\n",
      "Epoch 86/150\n",
      "692/692 [==============================] - 0s 171us/step - loss: 0.5066 - acc: 0.7601\n",
      "Epoch 87/150\n",
      "692/692 [==============================] - 0s 174us/step - loss: 0.5020 - acc: 0.7500\n",
      "Epoch 88/150\n",
      "692/692 [==============================] - 0s 169us/step - loss: 0.5151 - acc: 0.7529\n",
      "Epoch 89/150\n",
      "692/692 [==============================] - 0s 159us/step - loss: 0.5139 - acc: 0.7514\n",
      "Epoch 90/150\n",
      "692/692 [==============================] - 0s 189us/step - loss: 0.4988 - acc: 0.7659\n",
      "Epoch 91/150\n",
      "692/692 [==============================] - 0s 162us/step - loss: 0.5198 - acc: 0.7471\n",
      "Epoch 92/150\n",
      "692/692 [==============================] - 0s 178us/step - loss: 0.5001 - acc: 0.7601\n",
      "Epoch 93/150\n",
      "692/692 [==============================] - 0s 185us/step - loss: 0.4993 - acc: 0.7572\n",
      "Epoch 94/150\n",
      "692/692 [==============================] - 0s 188us/step - loss: 0.5003 - acc: 0.7616\n",
      "Epoch 95/150\n",
      "692/692 [==============================] - 0s 185us/step - loss: 0.5049 - acc: 0.7587\n",
      "Epoch 96/150\n",
      "692/692 [==============================] - 0s 249us/step - loss: 0.5019 - acc: 0.7572\n",
      "Epoch 97/150\n",
      "692/692 [==============================] - 0s 220us/step - loss: 0.4951 - acc: 0.7673 0s - loss: 0.4977 - acc: 0.76\n",
      "Epoch 98/150\n",
      "692/692 [==============================] - 0s 197us/step - loss: 0.5025 - acc: 0.7587\n",
      "Epoch 99/150\n",
      "692/692 [==============================] - 0s 178us/step - loss: 0.4955 - acc: 0.7543\n",
      "Epoch 100/150\n",
      "692/692 [==============================] - 0s 165us/step - loss: 0.4969 - acc: 0.7572\n",
      "Epoch 101/150\n",
      "692/692 [==============================] - 0s 145us/step - loss: 0.4958 - acc: 0.7673\n",
      "Epoch 102/150\n",
      "692/692 [==============================] - 0s 142us/step - loss: 0.4910 - acc: 0.7587 0s - loss: 0.5002 - acc: 0.760\n",
      "Epoch 103/150\n",
      "692/692 [==============================] - 0s 142us/step - loss: 0.4970 - acc: 0.7601\n",
      "Epoch 104/150\n",
      "692/692 [==============================] - 0s 166us/step - loss: 0.4903 - acc: 0.7616\n",
      "Epoch 105/150\n",
      "692/692 [==============================] - 0s 175us/step - loss: 0.4930 - acc: 0.7746\n",
      "Epoch 106/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "692/692 [==============================] - 0s 142us/step - loss: 0.4917 - acc: 0.7587\n",
      "Epoch 107/150\n",
      "692/692 [==============================] - 0s 166us/step - loss: 0.5026 - acc: 0.7587\n",
      "Epoch 108/150\n",
      "692/692 [==============================] - 0s 145us/step - loss: 0.4935 - acc: 0.7659\n",
      "Epoch 109/150\n",
      "692/692 [==============================] - 0s 142us/step - loss: 0.4840 - acc: 0.7673\n",
      "Epoch 110/150\n",
      "692/692 [==============================] - 0s 179us/step - loss: 0.4862 - acc: 0.7630\n",
      "Epoch 111/150\n",
      "692/692 [==============================] - 0s 163us/step - loss: 0.4928 - acc: 0.7659\n",
      "Epoch 112/150\n",
      "692/692 [==============================] - 0s 171us/step - loss: 0.4894 - acc: 0.7789\n",
      "Epoch 113/150\n",
      "692/692 [==============================] - 0s 150us/step - loss: 0.4957 - acc: 0.7587\n",
      "Epoch 114/150\n",
      "692/692 [==============================] - 0s 140us/step - loss: 0.4834 - acc: 0.7659\n",
      "Epoch 115/150\n",
      "692/692 [==============================] - 0s 143us/step - loss: 0.4872 - acc: 0.7775\n",
      "Epoch 116/150\n",
      "692/692 [==============================] - 0s 145us/step - loss: 0.4894 - acc: 0.7659\n",
      "Epoch 117/150\n",
      "692/692 [==============================] - 0s 169us/step - loss: 0.4847 - acc: 0.7587\n",
      "Epoch 118/150\n",
      "692/692 [==============================] - 0s 146us/step - loss: 0.4851 - acc: 0.7659\n",
      "Epoch 119/150\n",
      "692/692 [==============================] - 0s 143us/step - loss: 0.4932 - acc: 0.7630\n",
      "Epoch 120/150\n",
      "692/692 [==============================] - 0s 145us/step - loss: 0.4833 - acc: 0.7616\n",
      "Epoch 121/150\n",
      "692/692 [==============================] - 0s 145us/step - loss: 0.4808 - acc: 0.7659\n",
      "Epoch 122/150\n",
      "692/692 [==============================] - 0s 165us/step - loss: 0.4884 - acc: 0.7775\n",
      "Epoch 123/150\n",
      "692/692 [==============================] - 0s 146us/step - loss: 0.4811 - acc: 0.7688\n",
      "Epoch 124/150\n",
      "692/692 [==============================] - 0s 146us/step - loss: 0.4840 - acc: 0.7746\n",
      "Epoch 125/150\n",
      "692/692 [==============================] - 0s 145us/step - loss: 0.4901 - acc: 0.7746\n",
      "Epoch 126/150\n",
      "692/692 [==============================] - 0s 143us/step - loss: 0.4769 - acc: 0.7775\n",
      "Epoch 127/150\n",
      "692/692 [==============================] - 0s 165us/step - loss: 0.4818 - acc: 0.7775\n",
      "Epoch 128/150\n",
      "692/692 [==============================] - ETA: 0s - loss: 0.4792 - acc: 0.769 - 0s 166us/step - loss: 0.4807 - acc: 0.7673\n",
      "Epoch 129/150\n",
      "692/692 [==============================] - 0s 145us/step - loss: 0.4797 - acc: 0.7688\n",
      "Epoch 130/150\n",
      "692/692 [==============================] - 0s 145us/step - loss: 0.4911 - acc: 0.7702\n",
      "Epoch 131/150\n",
      "692/692 [==============================] - 0s 149us/step - loss: 0.4799 - acc: 0.7717\n",
      "Epoch 132/150\n",
      "692/692 [==============================] - 0s 148us/step - loss: 0.4804 - acc: 0.7688 0s - loss: 0.4899 - acc: 0.751\n",
      "Epoch 133/150\n",
      "692/692 [==============================] - 0s 142us/step - loss: 0.4868 - acc: 0.7717\n",
      "Epoch 134/150\n",
      "692/692 [==============================] - 0s 143us/step - loss: 0.4840 - acc: 0.7688\n",
      "Epoch 135/150\n",
      "692/692 [==============================] - 0s 148us/step - loss: 0.4797 - acc: 0.7717\n",
      "Epoch 136/150\n",
      "692/692 [==============================] - 0s 168us/step - loss: 0.4798 - acc: 0.7702\n",
      "Epoch 137/150\n",
      "692/692 [==============================] - ETA: 0s - loss: 0.4768 - acc: 0.769 - 0s 169us/step - loss: 0.4775 - acc: 0.7688\n",
      "Epoch 138/150\n",
      "692/692 [==============================] - ETA: 0s - loss: 0.4814 - acc: 0.750 - 0s 168us/step - loss: 0.4766 - acc: 0.7543\n",
      "Epoch 139/150\n",
      "692/692 [==============================] - ETA: 0s - loss: 0.4940 - acc: 0.762 - 0s 146us/step - loss: 0.4889 - acc: 0.7659\n",
      "Epoch 140/150\n",
      "692/692 [==============================] - 0s 145us/step - loss: 0.4791 - acc: 0.7717\n",
      "Epoch 141/150\n",
      "692/692 [==============================] - 0s 146us/step - loss: 0.4794 - acc: 0.7630\n",
      "Epoch 142/150\n",
      "692/692 [==============================] - 0s 149us/step - loss: 0.4754 - acc: 0.7775 0s - loss: 0.4755 - acc: 0.786\n",
      "Epoch 143/150\n",
      "692/692 [==============================] - 0s 143us/step - loss: 0.4706 - acc: 0.7717\n",
      "Epoch 144/150\n",
      "692/692 [==============================] - 0s 142us/step - loss: 0.4746 - acc: 0.7803\n",
      "Epoch 145/150\n",
      "692/692 [==============================] - 0s 146us/step - loss: 0.4756 - acc: 0.7789\n",
      "Epoch 146/150\n",
      "692/692 [==============================] - 0s 148us/step - loss: 0.4799 - acc: 0.7688\n",
      "Epoch 147/150\n",
      "692/692 [==============================] - 0s 150us/step - loss: 0.4759 - acc: 0.7789\n",
      "Epoch 148/150\n",
      "692/692 [==============================] - 0s 168us/step - loss: 0.4734 - acc: 0.7760\n",
      "Epoch 149/150\n",
      "692/692 [==============================] - 0s 140us/step - loss: 0.4786 - acc: 0.7673\n",
      "Epoch 150/150\n",
      "692/692 [==============================] - 0s 166us/step - loss: 0.4882 - acc: 0.7587\n",
      "76/76 [==============================] - 0s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "# https://keras-cn.readthedocs.io/en/latest/scikit-learn_API/ \n",
    "# 可以去查看这个类的相关说明 就知道为什么要定义一个函数来构造模型了\n",
    "def create_model():\n",
    "    model=Sequential()\n",
    "    model.add(Dense(12,input_dim=8,kernel_initializer='uniform',activation='relu'))\n",
    "    model.add(Dense(8,kernel_initializer='uniform',activation='relu'))\n",
    "    model.add(Dense(1,kernel_initializer='uniform',activation='sigmoid'))\n",
    "    model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    return model\n",
    "#只是构建和编译，拟合训练是要放在验证里的\n",
    "modelS=KerasClassifier(build_fn=create_model,epochs=150,batch_size=10)\n",
    "kfold=StratifiedKFold(y=Y,n_folds=10,shuffle=True,random_state=seed) #返回测试集训练集划分后的索引\n",
    "results=cross_val_score(modelS,X,Y,cv=kfold) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7668489441112631"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于 cross_val_score的CV参数说明：（可以是整数，CV产生器，决定交叉验证分割的策略）\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "    Determines the cross-validation splitting strategy. Possible inputs for cv are:\n",
    "\n",
    "    None, to use the default 3-fold cross validation,\n",
    "    integer, to specify the number of folds in a (Stratified)KFold,\n",
    "    CV splitter,\n",
    "    An iterable yielding (train, test) splits as arrays of indices.\n",
    "    For integer/None inputs, if the estimator is a classifier and y is either binary or multiclass, StratifiedKFold is used. In all other cases, KFold is used.\n",
    "\n",
    "    Refer User Guide for the various cross-validation strategies that can be used here.\n",
    "\n",
    "    Changed in version 0.20: cv default value if None will change from 3-fold to 5-fold in v0.22."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "无非就是把人工写的for循环改成了一个函数，但是确实简单了一些，而且准确率好像还高了一点？"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用网格搜索调整深度学习模型的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best:0.755208 using {'batch_size': 20, 'epochs': 150, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
      "0.653646(0.027498) with:{'batch_size': 5, 'epochs': 50, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "0.563802(0.158491) with:{'batch_size': 5, 'epochs': 50, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "0.699219(0.013902) with:{'batch_size': 5, 'epochs': 50, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
      "0.722656(0.024910) with:{'batch_size': 5, 'epochs': 50, 'init': 'normal', 'optimizer': 'adam'}\n",
      "0.677083(0.023073) with:{'batch_size': 5, 'epochs': 50, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
      "0.684896(0.036272) with:{'batch_size': 5, 'epochs': 50, 'init': 'uniform', 'optimizer': 'adam'}\n",
      "0.661458(0.023073) with:{'batch_size': 5, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "0.677083(0.049650) with:{'batch_size': 5, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "0.720052(0.021710) with:{'batch_size': 5, 'epochs': 100, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
      "0.735677(0.017566) with:{'batch_size': 5, 'epochs': 100, 'init': 'normal', 'optimizer': 'adam'}\n",
      "0.743490(0.025582) with:{'batch_size': 5, 'epochs': 100, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
      "0.744792(0.033804) with:{'batch_size': 5, 'epochs': 100, 'init': 'uniform', 'optimizer': 'adam'}\n",
      "0.720052(0.019225) with:{'batch_size': 5, 'epochs': 150, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "0.585938(0.177208) with:{'batch_size': 5, 'epochs': 150, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "0.739583(0.009744) with:{'batch_size': 5, 'epochs': 150, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
      "0.733073(0.024360) with:{'batch_size': 5, 'epochs': 150, 'init': 'normal', 'optimizer': 'adam'}\n",
      "0.733073(0.010253) with:{'batch_size': 5, 'epochs': 150, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
      "0.738281(0.041829) with:{'batch_size': 5, 'epochs': 150, 'init': 'uniform', 'optimizer': 'adam'}\n",
      "0.651042(0.028940) with:{'batch_size': 10, 'epochs': 50, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "0.691406(0.009568) with:{'batch_size': 10, 'epochs': 50, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "0.686198(0.012890) with:{'batch_size': 10, 'epochs': 50, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
      "0.695313(0.012758) with:{'batch_size': 10, 'epochs': 50, 'init': 'normal', 'optimizer': 'adam'}\n",
      "0.690104(0.028940) with:{'batch_size': 10, 'epochs': 50, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
      "0.692708(0.015733) with:{'batch_size': 10, 'epochs': 50, 'init': 'uniform', 'optimizer': 'adam'}\n",
      "0.696615(0.021236) with:{'batch_size': 10, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "0.709635(0.024774) with:{'batch_size': 10, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "0.726563(0.022326) with:{'batch_size': 10, 'epochs': 100, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
      "0.723958(0.019225) with:{'batch_size': 10, 'epochs': 100, 'init': 'normal', 'optimizer': 'adam'}\n",
      "0.729167(0.008027) with:{'batch_size': 10, 'epochs': 100, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
      "0.735677(0.012075) with:{'batch_size': 10, 'epochs': 100, 'init': 'uniform', 'optimizer': 'adam'}\n",
      "0.687500(0.022326) with:{'batch_size': 10, 'epochs': 150, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "0.727865(0.004872) with:{'batch_size': 10, 'epochs': 150, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "0.699219(0.038801) with:{'batch_size': 10, 'epochs': 150, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
      "0.747396(0.015073) with:{'batch_size': 10, 'epochs': 150, 'init': 'normal', 'optimizer': 'adam'}\n",
      "0.726563(0.011049) with:{'batch_size': 10, 'epochs': 150, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
      "0.733073(0.012890) with:{'batch_size': 10, 'epochs': 150, 'init': 'uniform', 'optimizer': 'adam'}\n",
      "0.675781(0.009568) with:{'batch_size': 20, 'epochs': 50, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "0.553385(0.162171) with:{'batch_size': 20, 'epochs': 50, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "0.705729(0.010253) with:{'batch_size': 20, 'epochs': 50, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
      "0.694010(0.009744) with:{'batch_size': 20, 'epochs': 50, 'init': 'normal', 'optimizer': 'adam'}\n",
      "0.700521(0.013279) with:{'batch_size': 20, 'epochs': 50, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
      "0.696615(0.025582) with:{'batch_size': 20, 'epochs': 50, 'init': 'uniform', 'optimizer': 'adam'}\n",
      "0.686198(0.038450) with:{'batch_size': 20, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "0.674479(0.025976) with:{'batch_size': 20, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "0.710938(0.017758) with:{'batch_size': 20, 'epochs': 100, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
      "0.710938(0.030758) with:{'batch_size': 20, 'epochs': 100, 'init': 'normal', 'optimizer': 'adam'}\n",
      "0.717448(0.006639) with:{'batch_size': 20, 'epochs': 100, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
      "0.712240(0.023073) with:{'batch_size': 20, 'epochs': 100, 'init': 'uniform', 'optimizer': 'adam'}\n",
      "0.692708(0.027126) with:{'batch_size': 20, 'epochs': 150, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "0.695313(0.019137) with:{'batch_size': 20, 'epochs': 150, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "0.729167(0.020505) with:{'batch_size': 20, 'epochs': 150, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
      "0.717448(0.019225) with:{'batch_size': 20, 'epochs': 150, 'init': 'normal', 'optimizer': 'adam'}\n",
      "0.755208(0.021710) with:{'batch_size': 20, 'epochs': 150, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
      "0.699219(0.022326) with:{'batch_size': 20, 'epochs': 150, 'init': 'uniform', 'optimizer': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "def create_model(optimizer='rmsprop',init='glorot_uniform'):\n",
    "    model=Sequential()\n",
    "    model.add(Dense(12,input_dim=8,kernel_initializer=init,activation='relu'))\n",
    "    model.add(Dense(8,kernel_initializer=init,activation='relu'))\n",
    "    model.add(Dense(1,kernel_initializer=init,activation='sigmoid'))\n",
    "    model.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    return model\n",
    "#只是构建和编译，拟合训练是要放在验证里的\n",
    "model=KerasClassifier(build_fn=create_model,verbose=False)\n",
    "optimizers=['rmsprop','adam']\n",
    "inits=['glorot_uniform','normal','uniform']\n",
    "epoch=np.array([50,100,150])\n",
    "batches=np.array([5,10,20])\n",
    "param_grid=dict(optimizer=optimizers,init=inits,epochs=epoch,batch_size=batches)\n",
    "\n",
    "grid=GridSearchCV(model,param_grid=param_grid)\n",
    "#构建参数词典  2*3*3*3  不传入cv参数，使用默认的3折交叉验证,H或者也可以再使用\n",
    "# kfold=StratifiedKFold(y=Y,n_folds=10,shuffle=True,random_state=seed) 把这个参数传进去，用这个来作为交叉验证的方式\n",
    "grid_Results=grid.fit(X,Y)\n",
    "print('Best:%f using %s'%(grid_Results.best_score_,grid_Results.best_params_))\n",
    "for param,mean_score,scores in grid_Results.grid_scores_:\n",
    "    print('%f(%f) with:%r'%(scores.mean(),scores.std(),param))\n",
    "# %r类似于%s 用来输出字符串，但是%r保存原始的转义字符，比如\\n不会表现为换行，而会直接打印出来，%r row"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可惜verbose参数只在fit方法里有，所以要传到KerasClassifier里面，不然会输出很多东西，看着很乱"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我得到的最优的参数组合和原文给出的不同，但是过程是这样。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第13章 用序列化保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 77.60416666666666\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "seed=7\n",
    "np.random.seed(seed)\n",
    "\n",
    "filepath='datasets/'\n",
    "dataset=np.loadtxt(filepath+'diabetes.csv',delimiter=',',skiprows=1) \n",
    "X=dataset[:,0:8]\n",
    "Y=dataset[:,8]\n",
    "\n",
    "import keras\n",
    "from  keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Dense(12,input_dim=8,kernel_initializer='uniform',activation='relu'))\n",
    "model.add(Dense(8,kernel_initializer='uniform',activation='relu'))\n",
    "model.add(Dense(1,kernel_initializer='uniform',activation='sigmoid'))\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model.fit(X,Y,epochs=150,batch_size=10,verbose=0)\n",
    "scores=model.evaluate(X,Y,verbose=0)\n",
    "print(model.metrics_names[1],scores[1]*100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存和读取HDF5（权重文件保存格式）及JSON（网络结构）"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    model.to_json() returns a representation of the model as a JSON string. Note that the representation does not include the weights, only the architecture. You can reinstantiate the same model (with reinitialized weights) from the JSON string via:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "搜索后，发现 https://machinelearningmastery.com/save-load-keras-deep-learning-models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelJson=model.to_json()  #此时还没有保存成文件，只是序列化成一个json格式的一行 \n",
    "with open('model.json','w') as json_file:\n",
    "    json_file.write(modelJson)\n",
    "#此时才写入文件\n",
    "model.save_weights('model.h5') \n",
    "model.save('modelPer.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这时可以看到，同级文件下出现了model.h5和model.json以及modelPer.h5三个文件，保存成功，文件说明：\n",
    "\n",
    "参考 https://blog.gtwang.org/programming/keras-save-and-load-model-tutorial/\n",
    "+ 只保存模型的权重，使用model.save_weights('model.h5') ，载入权重时，使用model.load_weights('model.h5')\n",
    "+ 只保存模型的结构，使用model.to_json() 再将json文件写入文件，\n",
    "+ 保存整个模型，使用model.save('modelPer.h5')存储后，可以直接使用keras.models.load_model('modelPer.h5')来加载模型\n",
    "\n",
    "注意： 模型权重和结构分别保存的模型，最后读取之后需要再编译一次，才可以运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 77.60416666666666\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "modelPer=keras.models.load_model('modelPer.h5')\n",
    "modelPer.evaluate(X,Y,verbose=0)\n",
    "print(model.metrics_names[1],scores[1]*100)  #和上面一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 77.60416666666666\n"
     ]
    }
   ],
   "source": [
    "with open('model.json','r') as f:\n",
    "    modelJson=f.read()\n",
    "model=keras.models.model_from_json(modelJson)\n",
    "model.load_weights('model.h5')\n",
    "\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model.evaluate(X,Y,verbose=0)\n",
    "print(model.metrics_names[1],scores[1]*100)  #和上面一样"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第14章 保存训练过程中性能最好的模型"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 效果变好就保存"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每轮后（epochs）在测试数据集上验证，如果比之前效果好就保存权重（monitor='val_acc',mode='max'）。文件名可以以weights-improvement-val_acc=.2f.hdf5 这样的形式保存，文件名里包含一些说明词语，准确率提高等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "seed=7\n",
    "np.random.seed(seed)\n",
    "\n",
    "filepath='datasets/'\n",
    "dataset=np.loadtxt(filepath+'diabetes.csv',delimiter=',',skiprows=1) \n",
    "X=dataset[:,0:8]\n",
    "Y=dataset[:,8]\n",
    "\n",
    "import keras\n",
    "from  keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.67323, saving model to weights-Up-01-0.67\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.67323\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.67323\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.67323\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.67323 to 0.70079, saving model to weights-Up-05-0.70\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.70079\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.70079\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.70079\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.70079 to 0.70079, saving model to weights-Up-09-0.70\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.70079\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.70079\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.70079 to 0.71260, saving model to weights-Up-12-0.71\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.71260\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.71260\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.71260\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.71260\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.71260 to 0.72047, saving model to weights-Up-17-0.72\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.72047\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.72047\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.72047 to 0.72441, saving model to weights-Up-20-0.72\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.72441\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.72441\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.72441\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.72441\n",
      "\n",
      "Epoch 00025: val_acc improved from 0.72441 to 0.72441, saving model to weights-Up-25-0.72\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.72441\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.72441\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.72441\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.72441\n",
      "\n",
      "Epoch 00030: val_acc improved from 0.72441 to 0.72835, saving model to weights-Up-30-0.73\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.72835\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.72835\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.72835\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.72835\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.72835\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.72835\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.72835\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.72835\n",
      "\n",
      "Epoch 00039: val_acc improved from 0.72835 to 0.73622, saving model to weights-Up-39-0.74\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.73622\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.73622\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.73622\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.73622\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.73622\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.73622\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.73622\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.73622\n",
      "\n",
      "Epoch 00048: val_acc improved from 0.73622 to 0.74409, saving model to weights-Up-48-0.74\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.74409\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.74409\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.74409\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.74409\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.74409\n",
      "\n",
      "Epoch 00054: val_acc improved from 0.74409 to 0.74803, saving model to weights-Up-54-0.75\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.74803\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.74803\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.74803\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.74803\n",
      "\n",
      "Epoch 00059: val_acc improved from 0.74803 to 0.74803, saving model to weights-Up-59-0.75\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.74803\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.74803\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.74803\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.74803\n",
      "\n",
      "Epoch 00064: val_acc improved from 0.74803 to 0.75197, saving model to weights-Up-64-0.75\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.75197\n",
      "\n",
      "Epoch 00066: val_acc improved from 0.75197 to 0.76772, saving model to weights-Up-66-0.77\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.76772\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.76772\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.76772\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.76772\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.76772\n",
      "\n",
      "Epoch 00072: val_acc improved from 0.76772 to 0.77165, saving model to weights-Up-72-0.77\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.77165\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.77165\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.77165\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.77165\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.77165\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.77165\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.77165\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.77165\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.77165\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.77165\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.77165\n",
      "\n",
      "Epoch 00084: val_acc improved from 0.77165 to 0.77559, saving model to weights-Up-84-0.78\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.77559\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.77559\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.77559\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.77559\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.77559\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.77559\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.77559\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.77559\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.77559\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.77559\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.77559\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.77559\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.77559\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.77559\n",
      "\n",
      "Epoch 00099: val_acc improved from 0.77559 to 0.78346, saving model to weights-Up-99-0.78\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.78346\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.78346\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.78346\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.78346\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.78346\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.78346\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.78346\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.78346\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.78346\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.78346\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.78346\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.78346\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.78346\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.78346\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.78346\n",
      "\n",
      "Epoch 00115: val_acc improved from 0.78346 to 0.78740, saving model to weights-Up-115-0.79\n",
      "\n",
      "Epoch 00116: val_acc improved from 0.78740 to 0.79134, saving model to weights-Up-116-0.79\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.79134\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.79134\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.79134\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.79134\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.79134\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.79134\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.79134\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.79134\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.79134\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.79134\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.79134\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.79134\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.79134\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.79134\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.79134\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.79134\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.79134\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.79134\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.79134\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.79134\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.79134\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.79134\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.79134\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.79134\n",
      "\n",
      "Epoch 00141: val_acc improved from 0.79134 to 0.79134, saving model to weights-Up-141-0.79\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.79134\n",
      "\n",
      "Epoch 00143: val_acc improved from 0.79134 to 0.79528, saving model to weights-Up-143-0.80\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.79528\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.79528\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.79528\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.79528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00148: val_acc did not improve from 0.79528\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.79528\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.79528\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xe04d7b1cc0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=Sequential()\n",
    "model.add(Dense(12,input_dim=8,kernel_initializer='uniform',activation='relu'))\n",
    "model.add(Dense(8,kernel_initializer='uniform',activation='relu'))\n",
    "model.add(Dense(1,kernel_initializer='uniform',activation='sigmoid'))\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "checkpoint=ModelCheckpoint(filepath='weights-Up-{epoch:02d}-{val_acc:.2f}',monitor='val_acc',\n",
    "                           verbose=1,save_best_only=True,mode='max')\n",
    "callback_list=[checkpoint]\n",
    "\n",
    "model.fit(X,Y,epochs=150,batch_size=10,verbose=0,validation_split=0.33,callbacks=callback_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据 https://keras.io/zh/callbacks/#modelcheckpoint 可以知道，ModelCheckPoint中filepath。如果 filepath 是 weights.{epoch:02d}-{val_loss:.2f}.hdf5， 那么模型被保存的的文件名就会有{epoch:02d}这个模型对于的训练轮数和{val_loss:.2f}验证损失。\n",
    "\n",
    "    filepath: 字符串，保存模型的路径。\n",
    "    monitor: 被监测的数据。\n",
    "    verbose: 详细信息模式，0 或者 1 。\n",
    "   **save_best_only: 如果 save_best_only=True，the latest best model according to the quantity monitored will not be overwritten。从上面代码执行过程可知，依据要检测的指标得出的最新最好的模型将不会被覆盖。也就是之前训练得到的最佳模型不会被当前新出的最佳模型覆盖，都会保存，可以在当前这个.ipynb文件目录下看到保存的.hdf5文件**\n",
    "   \n",
    "    mode: {auto, min, max} 的其中之一。 如果 save_best_only=True，那么是否覆盖保存文件的决定就取决于被监测数据的最大或者最小值。 对于 val_acc，模式就会是 max，而对于 val_loss，模式就需要是 min，等等。 在 auto 模式中，方向会自动从被监测的数据的名字中判断出来。\n",
    "  \n",
    "  **save_weights_only: 如果 True，那么只有模型的权重会被保存 (model.save_weights(filepath))， 否则的话，整个模型会被保存 (model.save(filepath))。默认False，保存整个模型，所以文中写错了，读取保存的文件的时候其实是整个网络，不止是权重**\n",
    "  \n",
    "    period: 每个检查点之间的间隔（训练轮数）。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 只保存最好的模型"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文档里说的是，把之前的文件名改成固定的就行，那无非就是每次都保存，但是由于文件名一样，导致新的覆盖了旧的，应该不是这样的。\n",
    "\n",
    "+ 尝试将save_best_only变成False，结果每轮模型都保存了。。。看来只能改文件名了改成一样的，但是这样就无法获取对应的轮次和准确率了。可以在verbose=1中人眼去看，对应的epochs和val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.77165, saving model to weights-best.hdf5\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.77165 to 0.77559, saving model to weights-best.hdf5\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.77559\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.77559\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.77559\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.77559\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.77559\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.77559\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.77559\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.77559\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.77559\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.77559\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.77559\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.77559\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.77559\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.77559\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.77559\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.77559\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.77559\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.77559\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.77559\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.77559\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.77559\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.77559\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.77559\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.77559\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.77559\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.77559\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.77559\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.77559\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.77559\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.77559\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.77559\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.77559\n",
      "\n",
      "Epoch 00035: val_acc improved from 0.77559 to 0.77953, saving model to weights-best.hdf5\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.77953\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.77953\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xe04dd2b668>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint=ModelCheckpoint(filepath='weights-best.hdf5',monitor='val_acc',\n",
    "                           verbose=1,save_best_only=True,mode='max')\n",
    "model.fit(X,Y,epochs=150,batch_size=10,verbose=0,validation_split=0.33,callbacks=[checkpoint])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入保存的最好的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:81.25%\n"
     ]
    }
   ],
   "source": [
    "modelbest=keras.models.load_model('weights-best.hdf5') \n",
    "score=modelbest.evaluate(X,Y,verbose=0)\n",
    "print('%s:%.2f%%'%(modelbest.metrics_names[1],score[1]*100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第15章 模型训练效果可视化"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "直接用以前保存的模型来进行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "seed=7\n",
    "np.random.seed(seed)\n",
    "\n",
    "filepath='datasets/'\n",
    "dataset=np.loadtxt(filepath+'diabetes.csv',delimiter=',',skiprows=1) \n",
    "X=dataset[:,0:8]\n",
    "Y=dataset[:,8]\n",
    "\n",
    "import keras\n",
    "from  keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "modelPer=keras.models.load_model('modelPer.h5') \n",
    "history=modelPer.fit(X,Y,epochs=150,verbose=0,batch_size=10,validation_split=0.33)#如果这里加了验证集，那么history的keys中就会包含验证集的准确率和损失\n",
    "\n",
    "# model=Sequential()\n",
    "# model.add(Dense(12,input_dim=8,kernel_initializer='uniform',activation='relu'))\n",
    "# model.add(Dense(8,kernel_initializer='uniform',activation='relu'))\n",
    "# model.add(Dense(1,kernel_initializer='uniform',activation='sigmoid'))\n",
    "# model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "# history=model.fit(X,Y,epochs=150,batch_size=10,verbose=0,validation_split=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0xe04bd2d4e0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['acc'],label='acc')\n",
    "plt.plot(history.history['val_acc'],label='val_acc')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，每次训练的结果都不一样，使用之前保存的模型和新建的模型跑出来的结果不同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
