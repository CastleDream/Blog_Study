{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前言"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主要参考：\n",
    "\n",
    "https://github.com/richliao/textClassifier\n",
    "\n",
    "参考其博客：\n",
    "\n",
    "https://richliao.github.io/supervised/classification/2016/12/26/textclassifier-HATN/\n",
    "\n",
    "主要使用数据是IMDB：\n",
    "\n",
    "https://www.kaggle.com/c/word2vec-nlp-tutorial/data  （到这里下载）\n",
    "\n",
    "----\n",
    "**运行前提：**\n",
    "参考 E:\\jupter\\Blog_Study\\深度框架学习\\theano安装 这个文档，Anaconda默认环境是3.6 这不影响，但是3.6默认安装的keras是2.2.x,而这个代码是2.0.8 想要正确运行，就需要先卸载2.2版本的keras 去使用2.0.8的，python版本倒不是很重要。台式机是配置好的， 在HAAT_Attention文件夹下，py3.x的版本正确，以后可以用那个运行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 先根据博客走一遍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 在实现层次注意力机制网络（那个论文的模型）之前，先实现一个层次LSTM网络作为基线。想要实现层次LSTM，需要将数据构建成3D而不是以前的2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys,os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train=pd.read_csv('../labeledTrainData.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  sentiment                                             review\n",
       "0  5814_8          1  With all this stuff going down at the moment w...\n",
       "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">+ 这里要使用句子级别，构建了一个三维数组，文档个数X最大句子数X每句最大长度  本例中= 25000X15X100\n",
    "+ 如果是以前词级别的话，输入的数据应该是 文档个数X文档最大长度，25000X250 TensorFlow教程里是这个维度\n",
    "+ data[i,j,k] i表示review序号，[0,len(reviews)],j表示句子序号，[1,MAX_SENTS],k表示词语序号[0,MAX_NB_WORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_str(string):\n",
    "    string=re.sub(r\"\\\\\",\"\",string)\n",
    "    string=re.sub(r\"\\'\",\"\",string)\n",
    "    string=re.sub(r'\\\"','',string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, (25000, 3))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.review.shape[0],data_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分句"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The  (Salamandra salamandra)?',\n",
       " 'an amphibian found.',\n",
       " 'Its black the head and back, toxins.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stringa='''The  (Salamandra salamandra)? an amphibian found. Its black the head and back, toxins.'''\n",
    "tokenize.sent_tokenize(stringa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看上个cell可以知道，sent_tokenize的标准是 句号,问号，其他标点符号都不分的，  \n",
    "\n",
    "+ 关于英文分句 https://zhuanlan.zhihu.com/p/41804488  手动划分 句号 叹号 问号 貌似暂时知道的就这三种符号比较常见\n",
    "+ 关于中文分句，\n",
    "    + 看一下结巴分词行不行，（查看后知道，不行）\n",
    "    + 更多是使用正则表达式或者string的split，自己固定要分的标点符号，我觉得这样比较好吧。毕竟微博语言并不规整，很多人不怎么好好用标点符号的。\n",
    "    + 参考：\n",
    "        + （采用）https://blog.csdn.net/blmoistawinde/article/details/82379256#commentBox  \n",
    "        + （借鉴理解）https://blog.csdn.net/zhuzuwei/article/details/80487032"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_string='巨无霸锯床，懂不？加工金属的，3米厚的金属板一下下去像裁布似的，莫名喜欢[色]，全淘男友力max排行榜上有名'\n",
    "sentences = re.split('(。|！|\\!|\\.|？|\\?|，)',zh_string) \n",
    "'''\n",
    "只进行split操作会保留那些标点符号，所以需要额外操作来只提取文本\n",
    "'''\n",
    "new_sents = []\n",
    "for i in range(int(len(sentences)/2)):\n",
    "    sent = sentences[2*i] + sentences[2*i+1]\n",
    "    '''\n",
    "    这里的假设是 分句 标点符号 分句 标点符号 有点理想化，但是还可以吧\n",
    "    '''\n",
    "    new_sents.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['巨无霸锯床，', '懂不？', '加工金属的，', '3米厚的金属板一下下去像裁布似的，', '莫名喜欢[色]，']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def cut_sent(para):\n",
    "    para = re.sub('([。！？\\?])([^”’])', r\"\\1\\n\\2\", para)  # 单字符断句符\n",
    "    para = re.sub('(\\.{6})([^”’])', r\"\\1\\n\\2\", para)  # 英文省略号\n",
    "    para = re.sub('(\\…{2})([^”’])', r\"\\1\\n\\2\", para)  # 中文省略号\n",
    "    para = re.sub('([。！？\\?][”’])([^，。！？\\?])', r'\\1\\n\\2', para)\n",
    "    # 如果双引号前有终止符，那么双引号才是句子的终点，把分句符\\n放到双引号后，注意前面的几句都小心保留了双引号\n",
    "    para = para.rstrip()  # 段尾如果有多余的\\n就去掉它\n",
    "    # 很多规则中会考虑分号;，但是这里我把它忽略不计，破折号、英文双引号等同样忽略，需要的再做些简单调整即可。\n",
    "    return para.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['巨无霸锯床，懂不？', '加工金属的，3米厚的金属板一下下去像裁布似的，莫名喜欢[色]，全淘男友力max排行榜上有名']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cut_sent(zh_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 得到句子集 评论集 标签集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ reviews的数据结构是 [['I love you','I love you too;],['I love dog','I love sunshine'],.....]，列表嵌入列表，reviews的每个元素是一个列表sentenceslist，sentenceslist中是每条评论分句后的列表\n",
    "+ texts是经过处理的问题，是[],其中列表的每个元素就是一整条评论\n",
    "+ labels 列表，二分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import  BeautifulSoup\n",
    "reviews=[]\n",
    "texts=[]\n",
    "labels=data_train.sentiment.values\n",
    "for idx in range(data_train.review.shape[0]):\n",
    "    text=BeautifulSoup(data_train.review[idx],'lxml')\n",
    "    text=clean_str(text.get_text())\n",
    "    texts.append(text)\n",
    "    sentences=tokenize.sent_tokenize(text)\n",
    "    '''\n",
    "    把text分成句子,参照上个cell\n",
    "    '''\n",
    "    reviews.append(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews[2],labels[2],texts[2]\n",
    "# 这个数据集文本太长了，稍微看一下就好"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义几个全局变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENT_LENGTH=100\n",
    "MAX_SENTS=10\n",
    "MAX_NB_WORDS=20000  #这边参数改一改，不然我这电脑跑不动\n",
    "EMBEDDING_DIM=300\n",
    "VALIDATION_SPLIT=0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 进行分词，词表构建和向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer,text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Text tokenization utility class.<br>\n",
    "This class allows to vectorize a text corpus, by turning each text into either a sequence of integers (each integer being the index of a token in a dictionary) or into a vector where the coefficient for each token could be binary, based on word count, based on tf-idf...<br>\n",
    "Tokenizer类可以用来向量化一个文本语料，通过将每个文本变成整数序列，或者变成一个每个元素是二进制表示的向量（one-hot编码）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "import os\n",
    "import importlib  #python3 reload模块 reload不再是内建函数 需要显式调用\n",
    "\n",
    "def set_keras_backend(backend):\n",
    "    if K.backend() != backend:\n",
    "        os.environ['KERAS_BACKEND'] = backend\n",
    "        importlib.reload(K)\n",
    "        assert K.backend() == backend\n",
    "\n",
    "set_keras_backend(\"theano\")  #如果后端不是theano 就换成theano\n",
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts) # Updates internal vocabulary based on a list of texts. 要求参数是一个list\n",
    "# tokenizer会构建一个词典 id映射字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "642"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index['happy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'salamandra',\n",
       " 'salamandra',\n",
       " 'an',\n",
       " 'amphibian',\n",
       " 'found',\n",
       " 'its',\n",
       " 'black',\n",
       " 'the',\n",
       " 'head',\n",
       " 'and',\n",
       " 'back',\n",
       " 'toxins']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_word_sequence('The  (Salamandra salamandra)? an amphibian found. Its black the head and back, toxins.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Converts a text to a sequence of words (or tokens).分词 但是对象必须是一个string，不能是列表了，所以只能一个句子一个句子分\n",
    "'''\n",
    "data=np.zeros((len(texts),MAX_SENTS,MAX_SENT_LENGTH),dtype='int32')\n",
    "for i,sentences in enumerate(reviews):  #对每条评论\n",
    "    for j,sent in enumerate(sentences):  #对每条评论中的每个分句\n",
    "        if j < MAX_SENTS:  #直接取前10个句子，不考虑什么情感词\n",
    "            wordTokens=text_to_word_sequence(sent) \n",
    "            k=0 #用来计数分句的分词数\n",
    "            for _,word in enumerate(wordTokens):  #对这个分句的分词\n",
    "                if k<MAX_SENT_LENGTH and tokenizer.word_index[word]<MAX_NB_WORDS:\n",
    "                    # 保证每个分句的词个数 即分句长度不超过100\n",
    "                    #保证这个分句里的词 位于 词频的前2w位\n",
    "                    data[i,j,k]=tokenizer.word_index[word]\n",
    "                    k=k+1       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 81501 unique tokens.\n",
      "Shape of data tensor: (25000, 10, 100)\n",
      "Shape of label tensor: (25000, 2)\n",
      "Number of positive and negative reviews in traing and validation set\n",
      "[10020.  9980.]\n",
      "[2480. 2520.]\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Total %s unique tokens.' % len(word_index))\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]\n",
    "\n",
    "print('Number of positive and negative reviews in traing and validation set')\n",
    "print(y_train.sum(axis=0))\n",
    "print(y_val.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 209513 word vectors.\n"
     ]
    }
   ],
   "source": [
    "GLOVE_DIR='../../TheSecond-Paper/word_embedding/en_model.txt'\n",
    "embedding_index={}\n",
    "f=open(GLOVE_DIR)\n",
    "for  line in f:\n",
    "    values=line.split()\n",
    "    word=values[0]\n",
    "    coefs=np.asarray(values[1:],dtype='float64')\n",
    "    embedding_index[word]=coefs\n",
    "f.close()\n",
    "\n",
    "print('Total %s word vectors.' % len(embedding_index))\n",
    "\n",
    "embedding_matrix=np.random.random((len(tokenizer.word_index)+1,EMBEDDING_DIM)) \n",
    "for word,i in tokenizer.word_index.items():   #词 词id\n",
    "    embedding_vector=embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i]=embedding_vector  #词id是从0开始的 词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Merge, Dropout, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from keras.models import Model\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttLayer(Layer):\n",
    "    def __init__(self, attention_dim):\n",
    "        self.init = initializers.get('normal')\n",
    "        self.supports_masking = True\n",
    "        self.attention_dim = attention_dim\n",
    "        super(AttLayer, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)))\n",
    "        self.b = K.variable(self.init((self.attention_dim, )))\n",
    "        self.u = K.variable(self.init((self.attention_dim, 1)))\n",
    "        self.trainable_weights = [self.W, self.b, self.u]\n",
    "        super(AttLayer, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return mask\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # size of x :[batch_size, sel_len, attention_dim]\n",
    "        # size of u :[batch_size, attention_dim]\n",
    "        # uit = tanh(xW+b)\n",
    "        uit = K.tanh(K.bias_add(K.dot(x, self.W), self.b))\n",
    "        ait = K.dot(uit, self.u)\n",
    "        ait = K.squeeze(ait, -1)\n",
    "\n",
    "        ait = K.exp(ait)\n",
    "\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            ait *= K.cast(mask, K.floatx())\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        ait = K.expand_dims(ait)\n",
    "        weighted_input = x * ait\n",
    "        output = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SENT_LENGTH,\n",
    "                            trainable=True,\n",
    "                            mask_zero=True)\n",
    "sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sentence_input)\n",
    "l_lstm = Bidirectional(GRU(100, return_sequences=True))(embedded_sequences)\n",
    "l_att = AttLayer(100)(l_lstm)\n",
    "sentEncoder = Model(sentence_input, l_att)\n",
    "\n",
    "review_input = Input(shape=(MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
    "review_encoder = TimeDistributed(sentEncoder)(review_input)\n",
    "l_lstm_sent = Bidirectional(GRU(100, return_sequences=True))(review_encoder)\n",
    "l_att_sent = AttLayer(100)(l_lstm_sent)\n",
    "preds = Dense(2, activation='softmax')(l_att_sent)\n",
    "model = Model(review_input, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitting - Hierachical attention network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 400 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "400/400 [==============================] - 197s - loss: 0.7518 - acc: 0.5375 - val_loss: 0.6893 - val_acc: 0.6088\n",
      "Epoch 2/10\n",
      "400/400 [==============================] - 196s - loss: 0.6766 - acc: 0.5825 - val_loss: 0.6951 - val_acc: 0.4960\n",
      "Epoch 3/10\n",
      "400/400 [==============================] - 191s - loss: 0.6673 - acc: 0.5950 - val_loss: 0.6680 - val_acc: 0.6278\n",
      "Epoch 4/10\n",
      "400/400 [==============================] - 189s - loss: 0.5821 - acc: 0.7725 - val_loss: 0.6912 - val_acc: 0.5258\n",
      "Epoch 5/10\n",
      "400/400 [==============================] - 195s - loss: 0.3914 - acc: 0.8575 - val_loss: 0.9110 - val_acc: 0.5464\n",
      "Epoch 6/10\n",
      "400/400 [==============================] - 191s - loss: 0.1370 - acc: 0.9575 - val_loss: 0.7450 - val_acc: 0.7176\n",
      "Epoch 7/10\n",
      "400/400 [==============================] - 189s - loss: 0.0214 - acc: 0.9975 - val_loss: 0.6469 - val_acc: 0.7740\n",
      "Epoch 8/10\n",
      "400/400 [==============================] - 192s - loss: 0.0051 - acc: 1.0000 - val_loss: 0.8999 - val_acc: 0.7584\n",
      "Epoch 9/10\n",
      "400/400 [==============================] - 194s - loss: 9.5540e-04 - acc: 1.0000 - val_loss: 0.9268 - val_acc: 0.7668\n",
      "Epoch 10/10\n",
      "400/400 [==============================] - 204s - loss: 3.9817e-04 - acc: 1.0000 - val_loss: 0.9913 - val_acc: 0.7710\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xf854a7b7b8>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"model fitting - Hierachical attention network\")\n",
    "model.fit(x_train[:400], y_train[:400], validation_data=(x_val, y_val),epochs=10, batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 配置安装theano： https://zhyack.github.io/posts/2016_05_26-Configurate-Theano-On-Windows.html\n",
    "+ WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n",
    "    + https://blog.csdn.net/m0_38058163/article/details/80657447\n",
    "+ http://deeplearning.net/software/theano/library/config.html#envvar-THEANO_FLAGS  <br>\n",
    "    config.blas.ldflags[source]\n",
    "    \n",
    "    Default: '-lblas'\n",
    "    \n",
    "    Link arguments to link against a (Fortran) level-3 blas implementation. The default will test if '-lblas' works. If not, we will disable our C code for BLAS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
