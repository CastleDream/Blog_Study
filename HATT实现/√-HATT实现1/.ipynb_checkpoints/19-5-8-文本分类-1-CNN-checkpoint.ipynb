{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://richliao.github.io/supervised/classification/2016/11/26/textclassifier-convolutional/\n",
    "\n",
    "这个人的博客，文本分类有3个博文，一个一个看吧，一边翻译一边看，数据来IMDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一篇博文中，首先看一下如何使用CNN来构建一个分类器，主要还是为了那篇分层注意力机制论文的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 采用IMDB数据集，训练集2.5w，3列，id，Sentiment以及review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train=pd.read_csv('.../labeledTrainData.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\The Classic War of the Worlds\\\\\" by Timothy Hines is a very entertaining film that obviously goes to great effort and lengths to faithfully recreate H. G. Wells\\' classic book. Mr. Hines succeeds in doing so. I, and those who watched his film with me, appreciated the fact that it was not the standard, predictable Hollywood fare that comes out every year, e.g. the Spielberg version with Tom Cruise that had only the slightest resemblance to the book. Obviously, everyone looks for different things in a movie. Those who envision themselves as amateur \\\\\"critics\\\\\" look only to criticize everything they can. Others rate a movie on more important bases,like being entertained, which is why most people never agree with the \\\\\"critics\\\\\". We enjoyed the effort Mr. Hines put into being faithful to H.G. Wells\\' classic novel, and we found it to be very entertaining. This made it easy to overlook what the \\\\\"critics\\\\\" perceive to be its shortcomings.\"'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.review[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，文本里存在一些杂乱的字符，比如 单引号 双引号 \\ ' \" 对数据进行清洗，同时保证所有词语都是小写 lower case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据清理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 清理\\ ' \"符号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# re.sub(pattern, repl, string, count=0, flags=0)\n",
    "def clean_str(string):\n",
    "    string=re.sub(r\"\\\\\",\"\",string)\n",
    "    string=re.sub(r\"\\'\",\"\",string)\n",
    "    string=re.sub(r'\\\"','',string)\n",
    "    return string.strip().lower()\n",
    "# Python strip() 方法用于移除字符串头尾指定的字符（默认为空格或换行符）或字符序列。\n",
    "# 注意：该方法只能删除开头或是结尾的字符，不能删除中间部分的字符。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shanshan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')  # 下载保存的本地地址 C:\\Users\\shanshan\\AppData\\Roaming\\nltk_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 清理 '<\\br>' 类似的html tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import tokenize\n",
    "texts=data_train.review.apply(clean_str).values\n",
    "#因为数据集稍微有些大，所以这里比较费时，原博文写的博文太low了。。。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the film starts with a manager (nicholas bell) giving welcome investors (robert carradine) to primal park . a secret project mutating a primal animal using fossilized dna, like ¨jurassik park¨, and some scientists resurrect one of natures most fearsome predators, the sabretooth tiger or smilodon . scientific ambition turns deadly, however, and when the high voltage fence is opened the creature escape and begins savagely stalking its prey - the human visitors , tourists and scientific.meanwhile some youngsters enter in the restricted area of the security center and are attacked by a pack of large pre-historical animals which are deadlier and bigger . in addition , a security agent (stacy haiduk) and her mate (brian wimmer) fight hardly against the carnivorous smilodons. the sabretooths, themselves , of course, are the real star stars and they are astounding terrifyingly though not convincing. the giant animals savagely are stalking its prey and the group run afoul and fight against one natures most fearsome predators. furthermore a third sabretooth more dangerous and slow stalks its victims.<br /><br />the movie delivers the goods with lots of blood and gore as beheading, hair-raising chills,full of scares when the sabretooths appear with mediocre special effects.the story provides exciting and stirring entertainment but it results to be quite boring .the giant animals are majority made by computer generator and seem totally lousy .middling performances though the players reacting appropriately to becoming food.actors give vigorously physical performances dodging the beasts ,running,bound and leaps or dangling over walls . and it packs a ridiculous final deadly scene. no for small kids by realistic,gory and violent attack scenes . other films about sabretooths or smilodon are the following : ¨sabretooth(2002)¨by james r hickox with vanessa angel, david keith and john rhys davies and the much better ¨10.000 bc(2006)¨ by roland emmerich with with steven strait, cliff curtis and camilla belle. this motion picture filled with bloody moments is badly directed by george miller and with no originality because takes too many elements from previous films. miller is an australian director usually working for television (tidal wave, journey to the center of the earth, and many others) and occasionally for cinema ( the man from snowy river, zeus and roxanne,robinson crusoe ). rating : below average, bottom of barrel.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encode函数说明\n",
    "\n",
    "encode() 方法以指定的编码格式编码字符串。errors参数可以指定不同的错误处理方案。\n",
    "\n",
    "str.encode(encoding='UTF-8',errors='strict')\n",
    "\n",
    "encoding -- 要使用的编码，如: UTF-8。\n",
    "errors -- 设置不同错误的处理方案。默认为 'strict',意为编码错误引起一个UnicodeError。 其他可能得值有 'ignore', 'replace', 'xmlcharrefreplace', 'backslashreplace' 以及通过 codecs.register_error() 注册的任何值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'the film starts with a manager (nicholas bell) giving welcome investors (robert carradine) to primal park . a secret project mutating a primal animal using fossilized dna, like ?jurassik park?, and some scientists resurrect one of natures most fearsome predators, the sabretooth tiger or smilodon . scientific ambition turns deadly, however, and when the high voltage fence is opened the creature escape and begins savagely stalking its prey - the human visitors , tourists and scientific.meanwhile some youngsters enter in the restricted area of the security center and are attacked by a pack of large pre-historical animals which are deadlier and bigger . in addition , a security agent (stacy haiduk) and her mate (brian wimmer) fight hardly against the carnivorous smilodons. the sabretooths, themselves , of course, are the real star stars and they are astounding terrifyingly though not convincing. the giant animals savagely are stalking its prey and the group run afoul and fight against one natures most fearsome predators. furthermore a third sabretooth more dangerous and slow stalks its victims.the movie delivers the goods with lots of blood and gore as beheading, hair-raising chills,full of scares when the sabretooths appear with mediocre special effects.the story provides exciting and stirring entertainment but it results to be quite boring .the giant animals are majority made by computer generator and seem totally lousy .middling performances though the players reacting appropriately to becoming food.actors give vigorously physical performances dodging the beasts ,running,bound and leaps or dangling over walls . and it packs a ridiculous final deadly scene. no for small kids by realistic,gory and violent attack scenes . other films about sabretooths or smilodon are the following : ?sabretooth(2002)?by james r hickox with vanessa angel, david keith and john rhys davies and the much better ?10.000 bc(2006)? by roland emmerich with with steven strait, cliff curtis and camilla belle. this motion picture filled with bloody moments is badly directed by george miller and with no originality because takes too many elements from previous films. miller is an australian director usually working for television (tidal wave, journey to the center of the earth, and many others) and occasionally for cinema ( the man from snowy river, zeus and roxanne,robinson crusoe ). rating : below average, bottom of barrel.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temtext=BeautifulSoup(texts[2],'lxml')\n",
    "temtext.get_text().encode('ascii','replace') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "很明显，这里存在一些即便是utf-8都无法编码的字符\n",
    "\n",
    "参照：\n",
    "\n",
    "https://github.com/richliao/textClassifier/pull/21/commits/47a6d73570e9f39adeaeb5b0a60835bedbe15fb4\n",
    "\n",
    "这个人的commit，也是用python3改过的，他也加入了lxml参数，也没有去编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=[]\n",
    "for  i in range(data_train.review.shape[0]):\n",
    "    text=BeautifulSoup(data_train.review[i],'lxml')\n",
    "    texts.append(clean_str(text.get_text()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it must be assumed that those who praised this film (the greatest filmed opera ever, didnt i read somewhere?) either dont care for opera, dont care for wagner, or dont care about anything except their desire to appear cultured. either as a representation of wagners swan-song, or as a movie, this strikes me as an unmitigated disaster, with a leaden reading of the score matched to a tricksy, lugubrious realisation of the text.its questionable that people with ideas as to what an opera (or, for that matter, a play, especially one by shakespeare) is about should be allowed anywhere near a theatre or film studio; syberberg, very fashionably, but without the smallest justification from wagners text, decided that parsifal is about bisexual integration, so that the title character, in the latter stages, transmutes into a kind of beatnik babe, though one who continues to sing high tenor -- few if any of the actors in the film are the singers, and we get a double dose of armin jordan, the conductor, who is seen as the face (but not heard as the voice) of amfortas, and also appears monstrously in double exposure as a kind of batonzilla or conductor who ate monsalvat during the playing of the good friday music -- in which, by the way, the transcendant loveliness of nature is represented by a scattering of shopworn and flaccid crocuses stuck in ill-laid turf, an expedient which baffles me. in the theatre we sometimes have to piece out such imperfections with our thoughts, but i cant think why syberberg couldnt splice in, for parsifal and gurnemanz, mountain pasture as lush as was provided for julie andrews in sound of music...the sound is hard to endure, the high voices and the trumpets in particular possessing an aural glare that adds another sort of fatigue to our impatience with the uninspired conducting and paralytic unfolding of the ritual. someone in another review mentioned the 1951 bayreuth recording, and knappertsbusch, though his tempi are often very slow, had what jordan altogether lacks, a sense of pulse, a feeling for the ebb and flow of the music -- and, after half a century, the orchestral sound in that set, in modern pressings, is still superior to this film.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建词表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH=1000  #规定每个review的最大长度\n",
    "MAX_NB_WORDS=20000 #整个语料库取词频最高的前2w词\n",
    "EMBEDDING_DIM=300  #用的是glove的100维的词向量原博  我之前是下载的300d的，用自己之前的\n",
    "VALIDATION_SPLIT=0.2 #验证集比例是0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://keras-cn.readthedocs.io/en/latest/preprocessing/text/ \n",
    "\n",
    "可以在这里看到 分词器的属性和方法，Tokenizer\n",
    "\n",
    "    方法： fit_on_texts(texts)\n",
    "    texts：要用以训练的文本列表\n",
    "    属性：word_index: 字典，将单词（字符串）映射为它们的排名或者索引。仅在调用fit_on_texts之后设置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=keras.preprocessing.text.Tokenizer(num_words=MAX_NB_WORDS) #num_words基于词频选择前num_words个词语，和原博客有差异，版本问题\n",
    "tokenizer.fit_on_texts(texts)  #构建了一个分词器，用texts进行训练（单层列表，不能使用reviews）\n",
    "# Updates internal vocabulary based on a list of texts.  基于给定的一个文本列表更新内部词表\n",
    "sequences=tokenizer.texts_to_sequences(texts)  #Transforms each text in texts to a sequence of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81501"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个语料库中共有大约81k个词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=keras.preprocessing.sequence.pad_sequences(sequences,maxlen=MAX_SENTENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=data_train.sentiment.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    12500\n",
       "0    12500\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.sentiment.value_counts()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这只是一个二分类问题，不需要to_categorical,但是后面构建的网络最后一层是softmax，所以需要进行to_categorical,但是损失写的多分类损失我就不能忍了，还是自己弄吧，用二分类做；毕竟TF的官方教程也是用的二分类，这个垃圾博主"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    labels=keras.utils.np_utils.to_categorical(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000, 1000), (25000,), 1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape,labels.shape,labels[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 划分训练集测试集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "手动划分是为了保证每个类别的数据在训练集和测试集中尽量平衡，比如说，一共25000个数据，12500个积极，12500个消极，1w到训练集，2k5到测试集，保证积极和消极的比例，而不是全部随机\n",
    "\n",
    "参考的博客里也是随机划分的，把label打乱了，所以为什么不用sklearn还要自己敲呢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices=np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "# 把data和label两个顺序同时都打乱\n",
    "data=data[indices]\n",
    "labels=labels[indices]\n",
    "nb_validation_samples=int(VALIDATION_SPLIT*data.shape[0])\n",
    "\n",
    "x_train=data[:-nb_validation_samples]\n",
    "y_train=labels[:-nb_validation_samples]\n",
    "x_val=data[-nb_validation_samples:]\n",
    "y_val=labels[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集测试集的数量：\n",
      "10005\n",
      "2495\n"
     ]
    }
   ],
   "source": [
    "print('训练集测试集的数量：')\n",
    "print(y_train.sum())\n",
    "print(y_val.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_DIR='../../TheSecond-Paper/word_embedding/en_model.txt'\n",
    "embedding_index={}\n",
    "f=open(GLOVE_DIR)\n",
    "for  line in f:\n",
    "    values=line.split()\n",
    "    word=values[0]\n",
    "    coefs=np.asarray(values[1:],dtype='float32')\n",
    "    embedding_index[word]=coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209513"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_index)  #似乎直接读txt文件比使用gensim要快很多"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所使用的glove共有大约21w个词向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建词向量矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "值得学习的一点是，这里初始化词向量矩阵时，没有用全0，而是随机取0-1之间的数进行初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix=np.random.random((len(tokenizer.word_index)+1,EMBEDDING_DIM)) #一般初始化都是用0初始化的，这里用的随机数。。\n",
    "# Return random floats in the half-open interval [0.0, 1.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word,i in tokenizer.word_index.items():   #词 词id\n",
    "    embedding_vector=embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i]=embedding_vector  #词id是从0开始的 词向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_input=keras.layers.Input(shape=(MAX_SENTENCE_LENGTH,),dtype='int32')\n",
    "\n",
    "# https://keras-cn-docs.readthedocs.io/zh_CN/latest/blog/word_embedding/ 根据这个网页\n",
    "# 如果要使用预训练的词向量 trainable是False 原博客错了 写的 True\n",
    "embedded_sequences=keras.layers.Embedding(len(tokenizer.word_index)+1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SENTENCE_LENGTH,\n",
    "                            trainable=False)(sequence_input)\n",
    "l_conv1=keras.layers.Conv1D(128,5,activation='relu')(embedded_sequences)\n",
    "l_pool1=keras.layers.MaxPooling1D(5)(l_conv1)\n",
    "\n",
    "l_conv2=keras.layers.Conv1D(128,5,activation='relu')(l_pool1)\n",
    "l_pool2=keras.layers.MaxPooling1D(5)(l_conv2)\n",
    "\n",
    "l_conv3=keras.layers.Conv1D(128,5,activation='relu')(l_pool2)\n",
    "l_pool3=keras.layers.MaxPooling1D(35)(l_conv3)\n",
    "\n",
    "l_flat=keras.layers.Flatten()(l_pool3)\n",
    "l_dense=keras.layers.Dense(128,activation='relu')(l_flat)\n",
    "preds=keras.layers.Dense(1,activation='sigmoid')(l_dense)\n",
    "\n",
    "# https://keras.io/zh/models/model/\n",
    "# Model(input=,output=)\n",
    "model=keras.models.Model(sequence_input,preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编译训练评分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 1000, 300)         24450600  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 996, 128)          192128    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 199, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 195, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 39, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 35, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 1, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 24,823,465\n",
      "Trainable params: 372,865\n",
      "Non-trainable params: 24,450,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 504s 25ms/step - loss: 0.6989 - acc: 0.4963 - val_loss: 0.6937 - val_acc: 0.4990\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 501s 25ms/step - loss: 0.6935 - acc: 0.4985 - val_loss: 0.6932 - val_acc: 0.5002\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 502s 25ms/step - loss: 0.6936 - acc: 0.5008 - val_loss: 0.6931 - val_acc: 0.5014\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 501s 25ms/step - loss: 0.6933 - acc: 0.4928 - val_loss: 0.6931 - val_acc: 0.5032\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 502s 25ms/step - loss: 0.6933 - acc: 0.4975 - val_loss: 0.6932 - val_acc: 0.5010\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 500s 25ms/step - loss: 0.6934 - acc: 0.5008 - val_loss: 0.6931 - val_acc: 0.5048\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 503s 25ms/step - loss: 0.6933 - acc: 0.4968 - val_loss: 0.6932 - val_acc: 0.5010\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 499s 25ms/step - loss: 0.6932 - acc: 0.4982 - val_loss: 0.6932 - val_acc: 0.4966\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 502s 25ms/step - loss: 0.6930 - acc: 0.5104 - val_loss: 0.6932 - val_acc: 0.4960\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 499s 25ms/step - loss: 0.6928 - acc: 0.5101 - val_loss: 0.6936 - val_acc: 0.5010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xe300131d68>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,validation_data=(x_val,y_val),epochs=10,batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练的比较慢。。。。等着吧，其实写的不是很好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 55s 11ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6935513503074646, 0.501]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_val,y_val,batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原博的准确率是89%。。。。差的有点多，哈哈哈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
