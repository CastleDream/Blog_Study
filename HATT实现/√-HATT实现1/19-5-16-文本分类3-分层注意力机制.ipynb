{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前言"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主要参考：\n",
    "\n",
    "https://github.com/richliao/textClassifier\n",
    "\n",
    "参考其博客：\n",
    "\n",
    "https://richliao.github.io/supervised/classification/2016/12/26/textclassifier-HATN/\n",
    "\n",
    "主要使用数据是IMDB：\n",
    "\n",
    "https://www.kaggle.com/c/word2vec-nlp-tutorial/data  （到这里下载）\n",
    "\n",
    "----\n",
    "**运行前提：**\n",
    "参考 E:\\jupter\\Blog_Study\\深度框架学习\\theano安装 这个文档，Anaconda默认环境是3.6 这不影响，但是3.6默认安装的keras是2.2.x,而这个代码是2.0.8 想要正确运行，就需要先卸载2.2版本的keras 去使用2.0.8的，python版本倒不是很重要。台式机是配置好的， 在HAAT_Attention文件夹下，py3.x的版本正确，以后可以用那个运行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 先根据博客走一遍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 在实现层次注意力机制网络（那个论文的模型）之前，先实现一个层次LSTM网络作为基线。想要实现层次LSTM，需要将数据构建成3D而不是以前的2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys,os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train=pd.read_csv('../labeledTrainData.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  sentiment                                             review\n",
       "0  5814_8          1  With all this stuff going down at the moment w...\n",
       "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi..."
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">+ 这里要使用句子级别，构建了一个三维数组，文档个数X最大句子数X每句最大长度  本例中= 25000X15X100\n",
    "+ 如果是以前词级别的话，输入的数据应该是 文档个数X文档最大长度，25000X250 TensorFlow教程里是这个维度\n",
    "+ data[i,j,k] i表示review序号，[0,len(reviews)],j表示句子序号，[1,MAX_SENTS],k表示词语序号[0,MAX_NB_WORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_str(string):\n",
    "    string=re.sub(r\"\\\\\",\"\",string)\n",
    "    string=re.sub(r\"\\'\",\"\",string)\n",
    "    string=re.sub(r'\\\"','',string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, (25000, 3))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.review.shape[0],data_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分句"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The  (Salamandra salamandra)?',\n",
       " 'an amphibian found.',\n",
       " 'Its black the head and back, toxins.']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stringa='''The  (Salamandra salamandra)? an amphibian found. Its black the head and back, toxins.'''\n",
    "tokenize.sent_tokenize(stringa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看上个cell可以知道，sent_tokenize的标准是 句号,问号，其他标点符号都不分的，  \n",
    "\n",
    "+ 关于英文分句 https://zhuanlan.zhihu.com/p/41804488  手动划分 句号 叹号 问号 貌似暂时知道的就这三种符号比较常见\n",
    "+ 关于中文分句，\n",
    "    + 看一下结巴分词行不行，（查看后知道，不行）\n",
    "    + 更多是使用正则表达式或者string的split，自己固定要分的标点符号，我觉得这样比较好吧。毕竟微博语言并不规整，很多人不怎么好好用标点符号的。\n",
    "    + 参考：\n",
    "        + （采用）https://blog.csdn.net/blmoistawinde/article/details/82379256#commentBox  \n",
    "        + （借鉴理解）https://blog.csdn.net/zhuzuwei/article/details/80487032"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_string='巨无霸锯床，懂不？加工金属的，3米厚的金属板一下下去像裁布似的，莫名喜欢[色]，全淘男友力max排行榜上有名'\n",
    "sentences = re.split('(。|！|\\!|\\.|？|\\?|，)',zh_string) \n",
    "'''\n",
    "只进行split操作会保留那些标点符号，所以需要额外操作来只提取文本\n",
    "'''\n",
    "new_sents = []\n",
    "for i in range(int(len(sentences)/2)):\n",
    "    sent = sentences[2*i] + sentences[2*i+1]\n",
    "    '''\n",
    "    这里的假设是 分句 标点符号 分句 标点符号 有点理想化，但是还可以吧\n",
    "    '''\n",
    "    new_sents.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['巨无霸锯床，', '懂不？', '加工金属的，', '3米厚的金属板一下下去像裁布似的，', '莫名喜欢[色]，']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def cut_sent(para):\n",
    "    para = re.sub('([。！？\\?])([^”’])', r\"\\1\\n\\2\", para)  # 单字符断句符\n",
    "    para = re.sub('(\\.{6})([^”’])', r\"\\1\\n\\2\", para)  # 英文省略号\n",
    "    para = re.sub('(\\…{2})([^”’])', r\"\\1\\n\\2\", para)  # 中文省略号\n",
    "    para = re.sub('([。！？\\?][”’])([^，。！？\\?])', r'\\1\\n\\2', para)\n",
    "    # 如果双引号前有终止符，那么双引号才是句子的终点，把分句符\\n放到双引号后，注意前面的几句都小心保留了双引号\n",
    "    para = para.rstrip()  # 段尾如果有多余的\\n就去掉它\n",
    "    # 很多规则中会考虑分号;，但是这里我把它忽略不计，破折号、英文双引号等同样忽略，需要的再做些简单调整即可。\n",
    "    return para.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['巨无霸锯床，懂不？', '加工金属的，3米厚的金属板一下下去像裁布似的，莫名喜欢[色]，全淘男友力max排行榜上有名']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cut_sent(zh_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 得到句子集 评论集 标签集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ reviews的数据结构是 [['I love you','I love you too;],['I love dog','I love sunshine'],.....]，列表嵌入列表，reviews的每个元素是一个列表sentenceslist，sentenceslist中是每条评论分句后的列表\n",
    "+ texts是经过处理的问题，是[],其中列表的每个元素就是一整条评论\n",
    "+ labels 列表，二分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import  BeautifulSoup\n",
    "reviews=[]\n",
    "texts=[]\n",
    "labels=data_train.sentiment.values\n",
    "for idx in range(data_train.review.shape[0]):\n",
    "    text=BeautifulSoup(data_train.review[idx],'lxml')\n",
    "    text=clean_str(text.get_text())\n",
    "    texts.append(text)\n",
    "    sentences=tokenize.sent_tokenize(text)\n",
    "    '''\n",
    "    把text分成句子,参照上个cell\n",
    "    '''\n",
    "    reviews.append(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews[2],labels[2],texts[2]\n",
    "# 这个数据集文本太长了，稍微看一下就好"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义几个全局变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENT_LENGTH=100\n",
    "MAX_SENTS=10\n",
    "MAX_NB_WORDS=20000  #这边参数改一改，不然我这电脑跑不动\n",
    "EMBEDDING_DIM=300\n",
    "VALIDATION_SPLIT=0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 进行分词，词表构建和向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer,text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Text tokenization utility class.<br>\n",
    "This class allows to vectorize a text corpus, by turning each text into either a sequence of integers (each integer being the index of a token in a dictionary) or into a vector where the coefficient for each token could be binary, based on word count, based on tf-idf...<br>\n",
    "Tokenizer类可以用来向量化一个文本语料，通过将每个文本变成整数序列，或者变成一个每个元素是二进制表示的向量（one-hot编码）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import os\n",
    "import importlib  #python3 reload模块 reload不再是内建函数 需要显式调用\n",
    "\n",
    "def set_keras_backend(backend):\n",
    "    if K.backend() != backend:\n",
    "        os.environ['KERAS_BACKEND'] = backend\n",
    "        importlib.reload(K)\n",
    "        assert K.backend() == backend\n",
    "\n",
    "set_keras_backend(\"theano\")  #如果后端不是theano 就换成theano\n",
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts) # Updates internal vocabulary based on a list of texts. 要求参数是一个list\n",
    "# tokenizer会构建一个词典 id映射字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "642"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index['happy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81501"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'salamandra',\n",
       " 'salamandra',\n",
       " 'an',\n",
       " 'amphibian',\n",
       " 'found',\n",
       " 'its',\n",
       " 'black',\n",
       " 'the',\n",
       " 'head',\n",
       " 'and',\n",
       " 'back',\n",
       " 'toxins']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_word_sequence('The  (Salamandra salamandra)? an amphibian found. Its black the head and back, toxins.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义三维句子向量表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Converts a text to a sequence of words (or tokens).分词 但是对象必须是一个string，不能是列表了，所以只能一个句子一个句子分\n",
    "'''\n",
    "data=np.zeros((len(texts),MAX_SENTS,MAX_SENT_LENGTH),dtype='int32')\n",
    "for i,sentences in enumerate(reviews):  #对每条评论\n",
    "    for j,sent in enumerate(sentences):  #对每条评论中的每个分句\n",
    "        if j < MAX_SENTS:  #直接取前10个句子，不考虑什么情感词\n",
    "            wordTokens=text_to_word_sequence(sent) \n",
    "            k=0 #用来计数分句的分词数\n",
    "            for _,word in enumerate(wordTokens):  #对这个分句的分词\n",
    "                if k<MAX_SENT_LENGTH and tokenizer.word_index[word]<MAX_NB_WORDS:\n",
    "                    # 保证每个分句的词个数 即分句长度不超过100\n",
    "                    #保证这个分句里的词 位于 词频的前2w位\n",
    "                    data[i,j,k]=tokenizer.word_index[word]\n",
    "                    k=k+1       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练集测试集划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 81501 unique tokens.\n",
      "Shape of data tensor: (25000, 10, 100)\n",
      "Shape of label tensor: (25000, 2)\n",
      "Number of positive and negative reviews in traing and validation set\n",
      "[ 9998. 10002.]\n",
      "[2502. 2498.]\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Total %s unique tokens.' % len(word_index))\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]\n",
    "\n",
    "print('Number of positive and negative reviews in traing and validation set')\n",
    "print(y_train.sum(axis=0))\n",
    "print(y_val.sum(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取预训练的词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 209513 word vectors.\n"
     ]
    }
   ],
   "source": [
    "GLOVE_DIR='E:/jupter/TheSecond-Paper/word_embedding/en_model.txt'\n",
    "embedding_index={}\n",
    "f=open(GLOVE_DIR)\n",
    "for  line in f:\n",
    "    values=line.split()\n",
    "    word=values[0]\n",
    "    coefs=np.asarray(values[1:],dtype='float64')\n",
    "    embedding_index[word]=coefs\n",
    "f.close()\n",
    "\n",
    "print('Total %s word vectors.' % len(embedding_index))\n",
    "\n",
    "embedding_matrix=np.random.random((len(tokenizer.word_index)+1,EMBEDDING_DIM)) \n",
    "for word,i in tokenizer.word_index.items():   #词 词id\n",
    "    embedding_vector=embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i]=embedding_vector  #词id是从0开始的 词向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding,Dropout, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from keras.models import Model\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建注意力机制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 理解代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "list1=[1,2]\n",
    "print(list1[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 下面的build函数中 self.W = K.variable(self.init((input_shape[-1], self.attention_dim)))中 input_shape是从上一个层得到的，所以__init__(self, attention_dim) ，所以代码调用中的AttLayer(100)(l_lstm)这个100代表的是attentin_dim。\n",
    "+ build一般用来定义权重，或者是定义自定义类中要使用的变量\n",
    "---\n",
    "```python \n",
    "#对应公式1 \n",
    "uit = K.tanh(K.bias_add(K.dot(x, self.W), self.b))  \n",
    "# 对应公式2\n",
    "ait = K.dot(uit, self.u)  # 这里的self.u应该就是论文中的context vector 是要训练的，不是计算得到的\n",
    "ait = K.squeeze(ait, -1)  #删除多余的维度，因为exp：Returns a variable representing the exponential of a, ie e^a.\n",
    "ait = K.exp(ait)\n",
    "if mask is not None:\n",
    "        # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "        ait *= K.cast(mask, K.floatx())\n",
    "ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "ait = K.expand_dims(ait)\n",
    "#对应公式3\n",
    "weighted_input = x * ait\n",
    "output = K.sum(weighted_input, axis=1)\n",
    "```\n",
    "---\n",
    "很明显这就是注意力机制的公式，是使用了\n",
    "[Hierarchical Attention Networks for document classification](https://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf)这个文章当中的公式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$h_i$就是上面代码中的x，代表的是句子向量或者是词向量，其实词级别和句子级别的注意力机制公式一样，就是放入其中的数据长得不一样而已。**\n",
    "\n",
    "$$u_{i}=tanh(W_sh_{i}+b_s)   \\quad(1)$$\n",
    "$$\\alpha_{i}=\\frac{exp(u_{i}^Tu_s)}{\\sum{exp(u_{i}^Tu_s)}}   \\quad  (2)$$\n",
    "$$v=\\sum{\\alpha_ih_i}  \\quad  (3)$$\n",
    "\n",
    "squeeze函数解释：Remove a 1-dimension from the tensor at index \"axis\".\n",
    "\n",
    "Remove broadcastable dimensions from the shape of an array.\n",
    "\n",
    "It returns the input array, but with the broadcastable dimensions removed. This is always x itself or a view into x.\n",
    "\n",
    "Parameters:\tx – Input data, tensor variable.\n",
    "Returns:\tx without its broadcastable dimensions.\n",
    "Return type:\tobject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttLayer(Layer):\n",
    "    def __init__(self, attention_dim):\n",
    "        self.init = initializers.get('normal')\n",
    "        self.supports_masking = True\n",
    "        self.attention_dim = attention_dim\n",
    "        super(AttLayer, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)))\n",
    "        self.b = K.variable(self.init((self.attention_dim, )))\n",
    "        self.u = K.variable(self.init((self.attention_dim, 1)))\n",
    "        self.trainable_weights = [self.W, self.b, self.u]\n",
    "        super(AttLayer, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return mask\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # size of x :[batch_size, sel_len, attention_dim]\n",
    "        # size of u :[batch_size, attention_dim]\n",
    "        # uit = tanh(xW+b)\n",
    "        uit = K.tanh(K.bias_add(K.dot(x, self.W), self.b))  \n",
    "        ait = K.dot(uit, self.u)\n",
    "        ait = K.squeeze(ait, -1)\n",
    "\n",
    "        ait = K.exp(ait)\n",
    "\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            ait *= K.cast(mask, K.floatx())\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        ait = K.expand_dims(ait)\n",
    "        weighted_input = x * ait\n",
    "        output = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TimeDistributed理解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```keras.layers.TimeDistributed(layer)\n",
    "这个封装器将一个层应用于输入的每个时间片。\n",
    "输入至少为 3D，且第一个维度应该是时间所表示的维度。\n",
    "考虑 32 个样本的一个 batch， 其中每个样本是 10 个 16 维向量的序列。 那么这个 batch 的输入尺寸为 (32, 10, 16)， 而 input_shape 不包含样本数量的维度，为 (10, 16)。\n",
    "你可以使用 TimeDistributed 来将 Dense 层独立地应用到 这 10 个时间步的每一个：\n",
    "\n",
    "# 作为模型第一层\n",
    "model = Sequential()\n",
    "model.add(TimeDistributed(Dense(8), input_shape=(10, 16)))\n",
    "# 现在 model.output_shape == (None, 10, 8)\n",
    "输出的尺寸为 (32, 10, 8)。\n",
    "\n",
    "在后续的层中，将不再需要 input_shape：\n",
    "\n",
    "model.add(TimeDistributed(Dense(32)))\n",
    "# 现在 model.output_shape == (None, 10, 32)\n",
    "输出的尺寸为 (32, 10, 32)。\n",
    "\n",
    "TimeDistributed 可以应用于任意层，不仅仅是 Dense， 例如运用于 Conv2D 层：\n",
    "\n",
    "model = Sequential()\n",
    "model.add(TimeDistributed(Conv2D(64, (3, 3)),input_shape=(10, 299, 299, 3)))\n",
    "参数\n",
    "layer: 一个网络层实例。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型调用理解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "l_lstm = Bidirectional(GRU(100, return_sequences=True))(embedded_sequences)\n",
    "l_att = AttLayer(100)(l_lstm)\n",
    "```\n",
    "|layer|output_shape|\n",
    "|------|------|\n",
    "|input_7 (InputLayer)| (None, 100) |                    \n",
    "|embedding_4 (Embedding) |(None, 100, 300)|          \n",
    "|bidirectional_7 (Bidirection |(None, 100, 200)|            \n",
    "|att_layer_7 (AttLayer)  |(None, 200)|               \n",
    "\n",
    "由此可以知道，经过BiGRU后，传递给下一层的out_put是(None,100,200).其中None应该代表的是输入样本集的个数，也就是类似len(train_data)这样,更准确的说是batch_size的大小，因为一次训练是batch_size个样本。所以传到att_layer的时候，input_shape就是(None,200),即(len(x_train),100，200).这里GRU用了return_sequences，同时还有Bidirectional,所以（None,100,200）中，100是return sequences对应的，200是Bidirectional之后的。\n",
    "\n",
    "att_layer(input_shape=(None,100,200),attention_dim=100)。所以之后的self.W=shape(200,100).最后定义的output_shape=(input_shape[0],input_shape[-1]).在这里也就是batch_size和200。\n",
    "\n",
    "所以这里的attention_dim只是注意力层内部计算的维度大小。并不会传至下一层。就像GRU(100)GRU层的100就表示GRU层的输出是100，但是Attention这里不同，表示注意力层里面是100个计算单元，但是输出也是自己定的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Shape of data tensor: (25000, 10, 100)\n",
    "Shape of label tensor: (25000, 2)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "反正那个注意力机制，上面的Attlayer层，用到词向量输入上就是词级别的了。得到的结果再用一次Attlayer层，就是句子级别的了。\n",
    "相当于一种注意力机制应用了两个层次的对象而已，仅此而已。\n",
    "'''\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SENT_LENGTH,\n",
    "                            trainable=True,\n",
    "                            mask_zero=True)\n",
    "sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sentence_input)\n",
    "l_lstm = Bidirectional(GRU(100, return_sequences=True))(embedded_sequences)\n",
    "l_att = AttLayer(100)(l_lstm)\n",
    "sentEncoder = Model(sentence_input, l_att)\n",
    "\n",
    "review_input = Input(shape=(MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
    "review_encoder = TimeDistributed(sentEncoder)(review_input)\n",
    "'''\n",
    "所以根据上面2.2.2的理解，可以知道 这里的TimeDistributed(sentEncoder)(review_input) \n",
    "这个代码就是说把sentEncoder层应用到review_input中每一个样本中\n",
    "使用 TimeDistributed 来将 sentEncoder层独立地应用到 这(MAX_SENTS, MAX_SENT_LENGTH)个时间步的每一个\n",
    "\n",
    "MAX_SENT_LENGTH = 100\n",
    "MAX_SENTS = 15\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "所以这里和batch没有关系，也就是把sentEncoder这个层独立的应用到15个个时间步中的每一个。\n",
    "所以这样样本先使用sentEncoder层，得到了每一个样本的句子向量，然后再处理\n",
    "'''\n",
    "l_lstm_sent = Bidirectional(GRU(100, return_sequences=True))(review_encoder)\n",
    "l_att_sent = AttLayer(100)(l_lstm_sent)\n",
    "preds = Dense(2, activation='softmax')(l_att_sent)\n",
    "model = Model(review_input, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 100, 300)          24450600  \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 100, 200)          240600    \n",
      "_________________________________________________________________\n",
      "att_layer_7 (AttLayer)       (None, 200)               20200     \n",
      "=================================================================\n",
      "Total params: 24,711,400\n",
      "Trainable params: 24,711,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sentEncoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 10, 100)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 10, 200)           24711400  \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 10, 200)           180600    \n",
      "_________________________________________________________________\n",
      "att_layer_2 (AttLayer)       (None, 200)               20200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 24,912,602\n",
      "Trainable params: 24,912,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 编译训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"model fitting - Hierachical attention network\")\n",
    "model.fit(x_train[:400], y_train[:400], validation_data=(x_val, y_val),epochs=10, batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 配置安装theano： https://zhyack.github.io/posts/2016_05_26-Configurate-Theano-On-Windows.html\n",
    "+ WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n",
    "    + https://blog.csdn.net/m0_38058163/article/details/80657447\n",
    "+ http://deeplearning.net/software/theano/library/config.html#envvar-THEANO_FLAGS  <br>\n",
    "    config.blas.ldflags[source]\n",
    "    \n",
    "    Default: '-lblas'\n",
    "    \n",
    "    Link arguments to link against a (Fortran) level-3 blas implementation. The default will test if '-lblas' works. If not, we will disable our C code for BLAS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
