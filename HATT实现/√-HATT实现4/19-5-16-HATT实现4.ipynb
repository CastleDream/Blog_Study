{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主要参考：\n",
    "\n",
    "+ [1] https://github.com/Ashish-Gupta03/Hierarchical-Attention-Networks-for-Document-Classification-NAACL-2016\n",
    "+ [2] https://github.com/charlesdong1991/interpretable-han-for-document-classification-with-keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现[1]  模型代码在本目录下的model.py中.仔细看了之后发现也是最初那个实现的复现，心疼。。。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sen_len = 100\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.1\n",
    "EMBEDDING_FILE = 'E:/jupter/TheSecond-Paper/word_embedding/en_model.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    string = re.sub(r\"\\\\\", \"\", string)    \n",
    "    string = re.sub(r\"\\'\", \"\", string)    \n",
    "    string = re.sub(r\"\\\"\", \"\", string)    \n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df =pd.read_csv('../labeledTrainData.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, labels = [], []\n",
    "for i in range(train_df['review'].shape[0]):\n",
    "    text = train_df['review'][i]\n",
    "    texts.append(clean_str(text))\n",
    "    labels.append(train_df['sentiment'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 100)\n"
     ]
    }
   ],
   "source": [
    "tok = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tok.fit_on_texts(texts) \n",
    "#tok.word_index 更新内部词典\n",
    "\n",
    "seq = tok.texts_to_sequences(texts) #Transforms each text in texts to a sequence of integers.\n",
    "\n",
    "data = pad_sequences(seq,maxlen=max_sen_len)\n",
    "\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  157,    60,    10,   102,     6,    88,    80,    44,    21,\n",
       "           91,   786,   240,     9,   123,   351,     2,   198,   121,\n",
       "            3,  7681,   746,     2,  3622,  2053,     8,    11,    17,\n",
       "            6,     3,   243,   488,  1877,     6,   367,    28,     4,\n",
       "            1,    88,  1019,    80,   122,     5,  1686,    11,  1206,\n",
       "           18,     6,    26,  2517,    69,    16,    30,     1,   690,\n",
       "          201,   521,    11,   873,  7261,    69,    10,    89,   120,\n",
       "           85,    80,    68,    27,   272,   496,  4584,  3552,    10,\n",
       "          120,    11,    15,     3,   189,    26,     6,   343,    33,\n",
       "          576,   324,    18,   375,   227,    40,    28,     4,     1,\n",
       "           88, 19375, 19376,    10,   440,    26,     6,    21,     1,\n",
       "         1564],\n",
       "       [  171,   287,   961,  1330,     1,  3676,   308,    16,   815,\n",
       "         3787,    12,    67,    62,     1,  3648,  4066,     5,     1,\n",
       "          271,   540,   312,   269,    15,   272,   179,     8,     3,\n",
       "           17,   144,    35,   532,    14,  2355,  1376,   164,    62,\n",
       "            5,  7084,   283,    34,    68,   382,   966,     3,    17,\n",
       "           20,    50,   672, 11309,    38,   108,  2172,    60,     6,\n",
       "          134,    88,    80,   112,  1040,    16,     1,  1376,    72,\n",
       "          510,     1,   781,   437,  7369,   273,    81,   108,  2738,\n",
       "            5,  2005,  1330,  3727,   352,   667,     2,    72,   254,\n",
       "            9,     5,    27,    52,   441,    11,    90,     9,   774,\n",
       "            5,  4857,    48,     1,  1376,  9249,     5,    27,    29,\n",
       "         5691]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.expand_dims(data, axis=1)\n",
    "# Insert a new axis that will appear at the `axis` position in the expanded array shape.把新加的轴放在axis=1的位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 1, 100)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = tok.word_index\n",
    "labels_cat = np.expand_dims(labels,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(labels).ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "labels本来只有一维，axis=0,然后在1的位置扩展了\n",
    "\n",
    "针对np.expand_dims:\n",
    "\n",
    "```note:: Previous to NumPy 1.13.0, neither ``axis < -a.ndim - 1`` nor``axis > a.ndim`` raised errors or put the new axis where documented.\n",
    "Those axis values are now deprecated and will raise an AxisError in the future.```\n",
    "\n",
    "在np1.13.0版本以前，当axis参数< 或者 axis>  的时候会引起错误，或者将axis放在记录的位置，以后这样会报错。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000, 1), '1.16.2')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_cat.shape,np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_val,y_train,y_val = train_test_split(data,labels_cat,test_size=0.12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取预训练词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "with open(EMBEDDING_FILE,encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.rstrip().rsplit(' ')\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建词向量矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = min(MAX_NUM_WORDS, len(idx) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in idx.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\jupter\\Blog_Study\\HATT实现\\HATT实现4\\model.py:154: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "  model = Model(input=docInp,output=docOut)\n"
     ]
    }
   ],
   "source": [
    "from model import *\n",
    "from importlib import reload\n",
    "# reload(model)\n",
    "model = createHierarchicalAttentionModel(max_sen_len, embWeights=embedding_matrix,\n",
    "                                                        embeddingSize = 300, vocabSize = min(MAX_NUM_WORDS, len(idx) + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"weights_base.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=5)\n",
    "callbacks_list = [checkpoint, early]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22000 samples, validate on 3000 samples\n",
      "Epoch 1/4\n",
      "22000/22000 [==============================] - 169s 8ms/step - loss: 0.4315 - acc: 0.8044 - val_loss: 0.3666 - val_acc: 0.8320\n",
      "\n",
      "Epoch 00001: val_acc improved from 0.77633 to 0.83200, saving model to weights_base.best.hdf5\n",
      "Epoch 2/4\n",
      "22000/22000 [==============================] - 167s 8ms/step - loss: 0.3781 - acc: 0.8334 - val_loss: 0.3685 - val_acc: 0.8350\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.83200 to 0.83500, saving model to weights_base.best.hdf5\n",
      "Epoch 3/4\n",
      "22000/22000 [==============================] - 168s 8ms/step - loss: 0.3678 - acc: 0.8387 - val_loss: 0.3436 - val_acc: 0.8507\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.83500 to 0.85067, saving model to weights_base.best.hdf5\n",
      "Epoch 4/4\n",
      "22000/22000 [==============================] - 171s 8ms/step - loss: 0.3461 - acc: 0.8501 - val_loss: 0.3310 - val_acc: 0.8640\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.85067 to 0.86400, saving model to weights_base.best.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x924f14a518>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=64, epochs=4,validation_data=(x_val, y_val),callbacks = callbacks_list,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "唯一的差别就是词向量不一样，参考的代码得到的准确率：\n",
    "\n",
    " loss: 0.0627 - acc: 0.9796 - val_loss: 0.2265 - val_acc: 0.9227\n",
    " \n",
    " 相差12%左右，差距很大，所以词向量很重要。实现了就好，谢谢大佬。\n",
    " \n",
    " 这个用TensorFlow跑的和用Theano跑的，速度上差了很多"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 10s 3ms/step\n",
      "score  0.8640000001589457\n",
      "loss  0.3310120234489441\n"
     ]
    }
   ],
   "source": [
    "loss,score=model.evaluate(x_val,y_val)\n",
    "print('score ',score)\n",
    "print('loss ',loss)\n",
    "model.load_weights(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('../testData.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_test = [] \n",
    "for i in range(test_df['review'].shape[0]):\n",
    "    text = test_df['review'][i]\n",
    "    texts_test.append(clean_str(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seq = tok.texts_to_sequences(texts_test)\n",
    "test_data = pad_sequences(test_seq,maxlen=max_sen_len)\n",
    "test_data = np.expand_dims(test_data, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test_data,batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9816089 ],\n",
       "       [0.04317772],\n",
       "       [0.3155845 ]], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred = []\n",
    "for i in pred:    \n",
    "    if i>0.9:\n",
    "        final_pred.append(1)\n",
    "    else: final_pred.append(0)   #其实不是很理解为什么>0.9的就是积极，剩下的就是消极。。。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm = pd.DataFrame(data=[],columns=['id','sentiment'])#read_csv('sampleSubmission.csv')\n",
    "subm['sentiment'] = final_pred\n",
    "subm['id'] = test_df['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12311_10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8348_2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5828_4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7186_2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12128_7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment\n",
       "0  12311_10          1\n",
       "1    8348_2          0\n",
       "2    5828_4          0\n",
       "3    7186_2          0\n",
       "4   12128_7          0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
